{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import utils\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Let $X$ training input of size $n * p$.  \n",
    "It contains $n$ examples, each with $p$ features.  \n",
    "Let $y$ traiing target of size $n$.  \n",
    "Each input $X_i$, vector of size $p$, is associated with it's target, $y_i$, a real number.  \n",
    "Linear regression tries to fit a linear model to predict the target $y$ of a new input vector $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are denoted $\\hat{y}$.\n",
    "$$\\hat{y_i} = X_i\\beta = \\sum_{j=1}^{p} X_{ij}\\beta_j$$\n",
    "$$\\hat{y} = X\\beta$$\n",
    "\n",
    "Where $\\beta$, vector of size $p$, are the model parameters.  \n",
    "The goal is to find $\\beta$ in order the minimizes the Mean Squared Error.\n",
    "$$MSE(\\beta) = \\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$$\n",
    "$$MSE(\\beta) = \\frac{1}{n} \\sum_{i=1}^n(y_i - X_i\\beta)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 4)\n",
      "(73,)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(73, 4)\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1]**2 + 5 * X[:, 2] - 3.4 * X[:, 3]\n",
    "y += np.random.randn(73) * 0.2\n",
    "w = np.random.randn(4)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(w.shape)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "ty = torch.tensor(y, requires_grad=False)\n",
    "tw = torch.tensor(w, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    n = len(y)\n",
    "    return (1 / n) * np.sum((y - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.37392132995149\n",
      "53.37392132995148\n",
      "1.4210854715202004e-14\n"
     ]
    }
   ],
   "source": [
    "y_pred = X @ w\n",
    "ty_pred = torch.matmul(tX, tw)\n",
    "utils.save_grad(ty_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "tloss = criterion(ty_pred, ty)\n",
    "tloss.backward()\n",
    "\n",
    "err = mse(y, y_pred)\n",
    "err_sol = tloss.data.numpy()\n",
    "print(err)\n",
    "print(err_sol)\n",
    "print(metrics.tdist(err, err_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution\n",
    "\n",
    "It's possible to find the direct solution by solving for the gradient of the MSE relative to the weights equals 0 (gradient details in gradient descent part).\n",
    "\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\beta} = \\frac{2}{n} X^T (\\hat{y} - y)$$\n",
    "$$X^T (y - X\\beta) = 0$$\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "$\\hat{\\beta}$ are the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.86409491 -0.39515774  4.80155806 -3.7966973 ]\n",
      "[ 2.86409491 -0.39515774  4.80155806 -3.7966973 ]\n",
      "9.064649753153016e-15\n"
     ]
    }
   ],
   "source": [
    "def fit_sk_lq(X, y):\n",
    "    m = LinearRegression(fit_intercept=False)\n",
    "    m.fit(X, y)\n",
    "    return m.coef_\n",
    "\n",
    "def fit_cf(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "w1 = fit_sk_lq(X, y)\n",
    "w2 = fit_cf(X, y)\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(metrics.tdist(w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest solution by orthogonal projection\n",
    "\n",
    "Another solution use orthogonal projection.  \n",
    "Compute the $QR$ projection of $X$: $X = QR$.  \n",
    "$$\\hat{\\beta} = R^{-1}Q^Ty$$\n",
    "\n",
    "$\\hat{\\beta}$ can be found by solving the upper triangular system $R\\hat{\\beta}=c$, with $c = Q^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.86409491 -0.39515774  4.80155806 -3.7966973 ]\n",
      "[ 2.86409491 -0.39515774  4.80155806 -3.7966973 ]\n",
      "1.1107918681964452e-14\n"
     ]
    }
   ],
   "source": [
    "def fit_qr(X, y):\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    c = np.dot(Q.T, y)\n",
    "    return scipy.linalg.solve_triangular(R, c)\n",
    "\n",
    "w1 = fit_sk_lq(X, y)\n",
    "w2 = fit_qr(X, y)\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(metrics.tdist(w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "This technique works by computing the gradient of the error relative to the \n",
    "weights, and update the weights to minimize the eror."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\hat{y}_j} = \\frac{2}{n}(\\hat{y_j} - y_j)$$\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\hat{y}} = \\frac{2}{n}(\\hat{y} - y)$$\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial y_j} = \\frac{2}{n}(y_j - \\hat{y_j})$$\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial y} = \\frac{2}{n}(y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_dy_pred(y, y_pred):\n",
    "    n = len(y)\n",
    "    return (2 / n) * (y_pred - y)\n",
    "\n",
    "def mse_dy(y, y_pred):\n",
    "    n = len(y)\n",
    "    return (2 / n) * (y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.484532191350646e-17\n"
     ]
    }
   ],
   "source": [
    "dy_pred = mse_dy_pred(y, y_pred)\n",
    "dy_pred_sol = utils.get_grad(ty_pred).data.numpy()\n",
    "print(metrics.tdist(dy_pred, dy_pred_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule of partial derivatives:\n",
    "$$\\frac{\\partial E}{\\partial u} = \\sum_{x_i \\in preds(E)} \\frac{\\partial E}{\\partial x_i} * \\frac{\\partial x_i}{\\partial u}$$\n",
    "\n",
    "Where $preds(E)$ representents the required variables to compute $E$.\n",
    "\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\beta_i} = \\sum_{j=1}^n \\frac{\\partial MSE(\\beta)}{\\partial \\hat{y}_j} * \\frac{\\partial \\hat{y}_j}{\\partial \\beta_i}$$\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_j}{\\partial \\beta_i} = X_{ji}$$\n",
    "Let $e = \\frac{\\partial MSE(\\beta)}{\\partial \\hat{y}} = \\frac{2}{n}(\\hat{y} - y)$\n",
    "\n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\beta_i} = \\sum_{j=1}^n X^T_{ij} e_j$$ \n",
    "$$\\frac{\\partial MSE(\\beta)}{\\partial \\beta} = X^T e = \\frac{2}{n} X^T (\\hat{y} - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_dw(X, dout):\n",
    "    return X.T @ dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.66205343881779e-15\n"
     ]
    }
   ],
   "source": [
    "dw = matmul_dw(X, dy_pred)\n",
    "dw_sol = tw.grad.data.numpy()\n",
    "print(metrics.tdist(dw, dw_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient descent\n",
    "All the dataset is fit at each epoch.  \n",
    "Let $\\mu$ the learning rate, number close to $0$.  \n",
    "At each epoch, compute the gradient of the whole dataset and perform the following update:\n",
    "$$\\beta = \\beta - \\mu * \\frac{\\partial MSE(\\beta)}{\\partial \\beta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.86409491 -0.39515774  4.80155806 -3.7966973 ]\n",
      "[ 2.86394295 -0.39633776  4.80088141 -3.7964394 ]\n",
      "[ 2.86409476 -0.39515788  4.80155796 -3.79669702]\n",
      "0.0013928060078077804\n",
      "3.649091577064423e-07\n",
      "0.0013925685810385509\n"
     ]
    }
   ],
   "source": [
    "def fit_sk_sgd(X, y, w_init, lr, nepochs):\n",
    "    \n",
    "    m = SGDRegressor(fit_intercept=False, learning_rate='constant',\n",
    "                    eta0=lr)\n",
    "    \n",
    "    m.coef_ = w_init.copy()\n",
    "    m.intercept_ = np.array([0])\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "        m.partial_fit(X, y)\n",
    "    \n",
    "    return m.coef_\n",
    "\n",
    "def fit_sgd(X, y, w_init, lr, nepochs):\n",
    "    \n",
    "    w = w_init.copy()\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "        y_pred = X @ w\n",
    "        dy_pred = mse_dy_pred(y, y_pred)\n",
    "        dw = matmul_dw(X, dy_pred)\n",
    "        w -= lr * dw\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "w_init = np.random.randn(X.shape[1])\n",
    "w1 = fit_sk_lq(X, y)\n",
    "w2 = fit_sk_sgd(X, y, w_init, 0.01 / X.shape[0] * 2, 1000)\n",
    "w3 = fit_sgd(X, y, w_init, 0.01, 1000)\n",
    "\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(w3)\n",
    "print(metrics.tdist(w1, w2))\n",
    "print(metrics.tdist(w1, w3))\n",
    "print(metrics.tdist(w2, w3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regresion with multiple targets\n",
    "\n",
    "Linear regression can be extend to multiple targets for each input, it's the same that solving one linear regression problem for each target.  \n",
    "Let $X$ training input of size $n * p$.  \n",
    "It contains $n$ examples, each with $p$ features.  \n",
    "Let $y$ traiing target, matrix of size $n*m$. Each example has $m$ targets.    \n",
    "Each input $X_i$, vector of size $p$, is associated with it's target, $y_i$, a vector of size $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are denoted $\\hat{y}$.\n",
    "$$\\hat{y_i} = X_i W$$\n",
    "$$\\hat{y} = XW$$\n",
    "\n",
    "Where $W$, matrice of size $p * m$, are the model parameters.  \n",
    "The goal is to find $W$ in order the minimizes the Mean Squared Error.\n",
    "$$MSE(W) = \\frac{1}{n*m} \\sum_{i=1}^n||y_i - \\hat{y}_i||^2$$\n",
    "$$MSE(W) = \\frac{1}{n*m} \\sum_{i=1}^n||y_i - X_iW||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 4)\n",
      "(73, 3)\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(73, 4)\n",
    "y =  3 * X[:, 0] + 2 * X[:, 1] * X[:, 1] + 5 * X[:, 2] - 3.4 * X[:, 3]\n",
    "y += np.random.randn(73) * 0.2\n",
    "\n",
    "y2 =  3.3 * X[:, 0] - 0.4 * X[:, 1] * X[:, 1] + 1 * X[:, 2] - 2.5 * X[:, 3]\n",
    "y2 += np.random.randn(73) * 0.2\n",
    "y3 =  0.2*X[:, 0] - 0.4*X[:, 1]**3 * + 1.9 * X[:, 2] - 2.56 * X[:, 3]\n",
    "y3 += np.random.randn(73) * 0.2\n",
    "\n",
    "y = np.column_stack((y, y2, y3))\n",
    "W = np.random.randn(4, 3)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(W.shape)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "ty = torch.tensor(y, requires_grad=False)\n",
    "tW = torch.tensor(W, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulas to compute the gradient are similar.\n",
    "\n",
    "$$\\frac{\\partial MSE(W)}{\\partial \\hat{y}} = \\frac{2}{n*m}(\\hat{y} - y)$$\n",
    "$$\\frac{\\partial MSE(W)}{\\partial y} = \\frac{2}{n*m}(y - \\hat{y})$$\n",
    "\n",
    "Let $e = \\frac{\\partial MSE(W)}{\\partial \\hat{y}} = \\frac{2}{n*m}(\\hat{y} - y)$\n",
    "\n",
    "$$\\frac{\\partial MSE(W)}{\\partial W_{ij}} = \\sum_{k=1}^n X^T_{ik} e_{kj}$$ \n",
    "$$\\frac{\\partial MSE(W)}{\\partial W} = X^T e = \\frac{2}{n*m} X^T (\\hat{y} - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    m = y.shape[1]\n",
    "    return (1 / (n*m)) * np.sum((y - y_pred)**2)\n",
    "\n",
    "def mse_dy_pred(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    m = y.shape[1]\n",
    "    return (2 / (n*m)) * (y_pred - y)\n",
    "\n",
    "def mse_dy(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    m = y.shape[1]\n",
    "    return (2 / (n*m)) * (y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.36640883620705\n",
      "46.36640883620706\n",
      "7.105427357601002e-15\n"
     ]
    }
   ],
   "source": [
    "y_pred = X @ W\n",
    "ty_pred = torch.matmul(tX, tW)\n",
    "utils.save_grad(ty_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "tloss = criterion(ty_pred, ty)\n",
    "tloss.backward()\n",
    "\n",
    "err = mse(y, y_pred)\n",
    "err_sol = tloss.data.numpy()\n",
    "print(err)\n",
    "print(err_sol)\n",
    "print(metrics.tdist(err, err_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.787324280642369e-17\n"
     ]
    }
   ],
   "source": [
    "dy_pred = mse_dy_pred(y, y_pred)\n",
    "dy_pred_sol = utils.get_grad(ty_pred).data.numpy()\n",
    "print(metrics.tdist(dy_pred, dy_pred_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5059797815742444e-15\n"
     ]
    }
   ],
   "source": [
    "dW = matmul_dw(X, dy_pred)\n",
    "dW_sol = tW.grad.data.numpy()\n",
    "print(metrics.tdist(dW, dW_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gradient descent, the algorithms are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.21515455  3.26578491  0.19849848]\n",
      " [-0.17254224  0.07420195 -0.88803732]\n",
      " [ 4.94163407  0.96301013  0.04748709]\n",
      " [-3.79287317 -2.40189836 -2.77119892]]\n",
      "[[ 3.21515455  3.26578491  0.19849848]\n",
      " [-0.17254224  0.07420195 -0.88803732]\n",
      " [ 4.94163407  0.96301013  0.04748709]\n",
      " [-3.79287317 -2.40189836 -2.77119892]]\n",
      "1.646949566984573e-13\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.randn(X.shape[1], y.shape[1])\n",
    "w1 = fit_sk_lq(X, y).T\n",
    "w2 = fit_sgd(X, y, w_init, 0.01, 10000)\n",
    "\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(metrics.tdist(w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
