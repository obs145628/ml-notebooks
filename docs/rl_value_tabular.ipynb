{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL: Value function (Tabular case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process (MDP) is a tuple <$\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{P}$, $\\mathcal{R}$, $\\gamma$>.  \n",
    "$\\mathcal{S}$ finite set of states.  \n",
    "$\\mathcal{A}$ finite set of actions.  \n",
    "$\\mathcal{P}$ state transition probability matrix.\n",
    "$$\\mathcal{P}_{ss'}^{a} = \\mathbb{P}[S_{t+1} = s' | S_t=s, A_t=a]$$\n",
    "$\\mathcal{R}$ reward function.\n",
    "$$\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$$\n",
    "$\\gamma$ discount factor, $\\gamma \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return $G_t$: discounted reward from time-step $t$\n",
    "$$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t_k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy $\\pi$ is a distribution over actions given states.  \n",
    "It defines the behaviour of the agent, and depend only on the current state. \n",
    "$$\\pi (a|s) = \\mathbb{P}[A_t=a|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following policy $\\pi$ on an MDP produces a Markow reward process <$\\mathcal{S}$, $\\mathcal{P^\\pi}$, $\\mathcal{R^\\pi}$, $\\gamma$>.\n",
    "$$\\mathcal{P}_{ss'}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\mathcal{P}_{ss'}^{a}$$\n",
    "$$\\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\mathcal{R}_{s}^{a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State value function $v_\\pi(s)$: Expected return starting from state $s$, following policy $\\pi$.\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi [G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-actons value function $q_\\pi(s, a)$: Expected return starting from state $s$, taking action $a$, and then following policy $\\pi$.\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi [G_t|S_t=s, A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Exceptation Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi(s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1}) |S_t=s]$$  \n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) |S_t=s, A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)q_\\pi(s, a)$$  \n",
    "$$q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')$$  \n",
    "$$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)(\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s'))$$  \n",
    "$$q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s')q_\\pi(s', a)'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi$$\n",
    "$$v_\\pi = (I - \\gamma \\mathcal{P}^\\pi)^{-1} \\mathcal{R}^\\pi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_*(s)$ optimal statue-value function.\n",
    "$$v_*(s) = \\max_{\\pi} v_\\pi(s)$$  \n",
    "$q_*(s, a)$ optimal action-value function.\n",
    "$$q_*(s, a) = \\max_{\\pi} q_\\pi(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oreding over policies: $\\pi \\geq \\pi'$ if $v_\\pi(s) \\geq v_{\\pi'}(s) \\forall s$.  \n",
    "There always exists an optimal policy $\\pi_*$: $\\pi_* \\geq \\pi, \\forall \\pi$\n",
    "$$v_{\\pi_*}(s) = v_*(s)$$\n",
    "$$q_{\\pi_*}(s. a) = q_*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi_*(a|s) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } a = \\text{arg}\\max_{a \\in \\mathcal{A}} q_*(s, a)\\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimilality Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_*(s) = \\max_{a \\in \\mathcal{A}} q_*(s, a)$$  \n",
    "$$q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')$$  \n",
    "$$v_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s'))$$  \n",
    "$$q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\max_{a' \\in \\mathcal{A}} q_*(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative policy evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)(\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_k(s'))$$  \n",
    "\n",
    "As $k \\rightarrow \\infty$, $v_k \\rightarrow v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Itearation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluates policy $\\pi$\n",
    "- Improve $\\pi$: $\\pi' = \\text{greedy}(v_\\pi)$\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi'(a|s) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } a = \\text{arg}\\max_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')\\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converges to $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{k+1}(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_k(s'))$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $k \\rightarrow \\infty$, $v_k \\rightarrow v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Place: Value iteration store only one value functions, no k\n",
    "- Prioretized sweeping: update state with largest remaining Bellman error\n",
    "- Real-Time: update states visited by a real agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman error:\n",
    "$$|\\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v(s')) - v(s)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Evaluation (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(s) = \\frac{S(s)}{N(s)}$$\n",
    "$S(s)$: sum of all returns $G_t$ from state $S_t=s$.  \n",
    "$N(s)$: number of times state $s$ is visited.  \n",
    "As $N(s) \\rightarrow \\infty$, $V(s) \\rightarrow v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First visit MC: Update $N$ et $S$ only the first time $s$ is visited at each episode\n",
    "- Every visit MC: Update $N$ et $S$ every time $s$ is visited at each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incremeantal updates:  \n",
    "For each state $_t$ with return $G_t$:\n",
    "$$N(S_t) \\leftarrow N(S_t) + 1$$\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t}(G_t - V(S_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates for non-stationary problems:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha(G_t - V(S_t))$$\n",
    "Forget old episodes over time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference-Learning (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each visited state $_t$:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$  \n",
    "\n",
    "TD target: $R_{t+1} + \\gamma V(S_{t+1})$  \n",
    "TD Error: $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC must wait until the end of the episode to update its estimate.  \n",
    "MC only work for terminating environments.  \n",
    "TD can update its estimate after every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC has high variance and no bias\n",
    "TD has low variance and some bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC tries to minizime the mean squared error of the observed returns $G_t$.  \n",
    "TD(0) converges to a MDP that best fits the data.\n",
    "Usually MC is more effective is non-markov enironements than TD, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-step returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t^{(n)} = \\sum_{k = 1}^n \\gamma^{k-1} R_{t+k} + \\gamma^n V(S_t + n)$$\n",
    "$n = 1$: TD(0).  \n",
    "$n = \\infty$: MC.  \n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{(n)} - V(S_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\lambda$-return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t^\\lambda = (1 - \\lambda) \\sum_{k=1}^\\infty \\lambda^{n-1} G_t^{(n)}$$  \n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{\\lambda} - V(S_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can only be computed from complete episodes, very slow to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign credit to most frequent states\n",
    "- Assign credit to most recent states\n",
    "\n",
    "$$E_0(s) = 0$$\n",
    "$$E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbb{1}(S_t = s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every visited state $s$:\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1} - V(S_t)$$\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t(s)$$\n",
    "Updates are equivalent to the $\\lambda$-return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On-Policy Learning: learn about policy $\\pi$ from experience sampled from $\\pi$.  \n",
    "Off-Policy Learning: learn about policy $\\pi$ from experience sampled from $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy evaluation: $Q \\approx q_\\pi$\n",
    "- $\\epsilon$-greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy policy:\n",
    "- choose greedy action with probability 1 - $\\epsilon$\n",
    "- choose random action with probability $\\epsilon$\n",
    "\n",
    "$$\n",
    "\\pi_(a|s) = \n",
    "\\begin{cases}\n",
    "    \\frac{\\epsilon}{m} + 1 - \\epsilon & \\text{if } a = \\text{arg}\\max_{a' \\in \\mathcal{A}} Q(s, a')\\\\\n",
    "    \\frac{\\epsilon}{m} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy in the Limit with Infinite Exploration (GLIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GLIE policy satisfies:\n",
    "\n",
    "$$\\lim_{k \\to \\infty} N_k(s,a) = \\infty$$\n",
    "$$\\lim_{k \\to \\infty} \\pi_k(a|s) = \\mathbb{1}(a = \\text{arg}\\max_{a' \\in \\mathcal{A}} Q(s, a'))$$  \n",
    "$\\epsilon$-greedy policy with $\\epsilon = \\frac{1}{k}$ is GLIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIE MC-Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each episode $k$ sampled following policy $\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)$, with $\\epsilon = \\frac{1}{k}$:  \n",
    "For each state and action $S_t$ and $A_t$ in episode $k$:  \n",
    "\n",
    "$$N(S_t, A_t) \\leftarrow N(S_t, A_t) + 1$$\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))$$  \n",
    "\n",
    "As $k \\to \\infty$, $Q(s, a) \\to q_*(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same technique than for MC control with $\\epsilon$-greedy policy.  \n",
    "Replace MC update by TD(0) update.  \n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) = \\alpha (R_t + \\lambda Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA converges to the optimat policy if:\n",
    "\n",
    "- policy $\\pi_t(a|s)$ is GLIE\n",
    "- step-size $\\alpha_t$ follow a Robbins-Monro sequence:\n",
    "\n",
    "$$\\sum_{t=1}^\\infty \\alpha_t = \\infty$$\n",
    "$$\\sum_{t=1}^\\infty \\alpha_t^2 < \\infty$$  \n",
    "\n",
    "In practice, might work with fixed $\\alpha$ and $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-step SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-step Q-return:\n",
    "\n",
    "$$q_t^{(n)} = \\sum_{k=1}^{n} \\lambda^{k-1}R_{t+k} + \\gamma^n Q(S_{t+n})$$\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) = \\alpha (q_t^{(n)} - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\lambda$-return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1}q_t^{(n)}$$\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) = \\alpha (q_t^\\lambda - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eligibily traces for q-values:  \n",
    "$$E_0(s,a) = 0$$\n",
    "$$E_t(s, a) = \\gamma \\lambda E_{t-1}(s, a) + \\mathbb{1}(S_t = s, A_t = a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every visited state $S_t$ and action $A_t$ visited following $\\epsilon$-greedy policy:  \n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$  \n",
    "\n",
    "For every state-action pair $s,a$:  \n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta_t E_t(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn from target policy $\\pi$ while following behaviour policy $\\mu$. Usefull for:\n",
    "    \n",
    "- Learn from observing other agent\n",
    "- Reuse experience from old policies\n",
    "- Learn about optimal policy while following exploratory policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{X \\sim P}[f(X)] = \\mathbb{E}_{X \\sim Q}[\\frac{P(X)}{Q(X)} f(X)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy Monte-Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t^{\\pi / \\mu} = \\frac{\\sum_{k=t}^T \\pi(A_k|S_k)}{\\sum_{k=t}^T \\mu(A_k|S_k)} G_t$$\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{\\pi / \\mu} - V(St))$$  \n",
    "\n",
    "Extremely high variance, doesn't work in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-POlicy TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (\\frac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)}(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$  \n",
    "\n",
    "Much lower variance than MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off policy control with TD(0):  \n",
    "\n",
    "Next action $A_{t+1}$ chosen from behaviour policy $\\mu$.  \n",
    "Update toward action $A'$ chosen from target policy $\\pi$\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose target policy $\\pi$ greedy with respect to $Q(s, a)$ and behaviour policy $\\mu$ $\\epsilon$-greedy with respect to $Q(s, a)$.  \n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t))$$  \n",
    "\n",
    "$Q(s, a) \\to q_*(s, a)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
