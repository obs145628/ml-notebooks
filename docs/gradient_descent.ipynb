{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model for training\n",
    "\n",
    "IN_SIZE = 28 * 28\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 256\n",
    "OUT_SIZE = 10\n",
    "\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader, test_loader = utils.load_mnist(BATCH_SIZE)\n",
    "train_loader_01, test_loader_01 = utils.load_mnist_01(BATCH_SIZE)\n",
    "\n",
    "def compute_accuracy(y_preds, y):\n",
    "    total = len(y_preds)\n",
    "    correct = np.equal(y_preds, y).sum()\n",
    "    return correct, total\n",
    "\n",
    "def run_tests(net, test_loader):\n",
    "    test_y_preds, test_y_true = utils.get_class_output(net, test_loader)\n",
    "    correct, total = compute_accuracy(test_y_preds, test_y_true)\n",
    "    acc = float(correct) / total\n",
    "    print('Test accuracy = {} ({}/{})'.format(acc, correct, total))\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = torch.nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = torch.nn.Linear(HIDDEN2_SIZE, OUT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IN_SIZE)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        y_logits = self.l3(x)\n",
    "        return y_logits\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go over the whole training set for several passes, each pass is called an epoch\n",
    "- For each epoch, divide the training set into x random grousps with a fixed bath size, and iterative over all the batchs of data\n",
    "- For each batch, compute the gradient of the weights, and update them accordingly to the learning rate $\\epsilon$:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\epsilon \\nabla_\\theta J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.0416259765625\n",
      "1[301] Loss = 33.40361404418945\n",
      "1[601] Loss = 28.80887794494629\n",
      "1[901] Loss = 21.72443389892578\n",
      "Test accuracy = 0.9229 (9229/10000)\n",
      "2[1] Loss = 15.224136352539062\n",
      "2[301] Loss = 13.496562004089355\n",
      "2[601] Loss = 16.52216339111328\n",
      "2[901] Loss = 7.9787516593933105\n",
      "Test accuracy = 0.9389 (9389/10000)\n",
      "3[1] Loss = 12.116883277893066\n",
      "3[301] Loss = 11.854864120483398\n",
      "3[601] Loss = 9.45172119140625\n",
      "3[901] Loss = 16.465801239013672\n",
      "Test accuracy = 0.9564 (9564/10000)\n",
      "4[1] Loss = 12.34631061553955\n",
      "4[301] Loss = 7.600798606872559\n",
      "4[601] Loss = 7.513490200042725\n",
      "4[901] Loss = 7.896974086761475\n",
      "Test accuracy = 0.9603 (9603/10000)\n",
      "5[1] Loss = 2.8604655265808105\n",
      "5[301] Loss = 12.922957420349121\n",
      "5[601] Loss = 12.881921768188477\n",
      "5[901] Loss = 6.0581512451171875\n",
      "Test accuracy = 0.965 (9650/10000)\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "net = Net()\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y)\n",
    "    \n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        for w in net.parameters():\n",
    "            lr_k = LEARNING_RATE\n",
    "            w.data.sub_(w.grad.data * lr_k)\n",
    "    \n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with linearly decaying learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate $\\epsilon$ linearly decrease over the whole training.  \n",
    "$\\epsilon_0$: initial learning rate.  \n",
    "$\\epsilon_\\tau$: final learning rate.  \n",
    "$\\tau$: number of iterations to reach $\\epsilon_\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha = \\frac{k}{\\tau}$$\n",
    "\n",
    "Learning rate at iteration $k$:\n",
    "\n",
    "$$\n",
    "\\epsilon_k = \n",
    "\\begin{cases}\n",
    "    (1 - \\alpha)\\epsilon_0 + \\alpha \\epsilon_\\tau & \\text{if } k \\leq \\tau\\\\\n",
    "    \\epsilon_\\tau & \\text{if } k > \\tau\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.7835693359375\n",
      "1[301] Loss = 19.255510330200195\n",
      "1[601] Loss = 15.425236701965332\n",
      "1[901] Loss = 14.171222686767578\n",
      "Test accuracy = 0.9362 (9362/10000)\n",
      "2[1] Loss = 6.965445041656494\n",
      "2[301] Loss = 10.846342086791992\n",
      "2[601] Loss = 7.963724613189697\n",
      "2[901] Loss = 9.435616493225098\n",
      "Test accuracy = 0.9576 (9576/10000)\n",
      "3[1] Loss = 3.6737990379333496\n",
      "3[301] Loss = 6.052556991577148\n",
      "3[601] Loss = 1.3734350204467773\n",
      "3[901] Loss = 16.566186904907227\n",
      "Test accuracy = 0.9685 (9685/10000)\n",
      "4[1] Loss = 3.8570051193237305\n",
      "4[301] Loss = 4.3883771896362305\n",
      "4[601] Loss = 5.736208438873291\n",
      "4[901] Loss = 9.423709869384766\n",
      "Test accuracy = 0.9733 (9733/10000)\n",
      "5[1] Loss = 5.546458721160889\n",
      "5[301] Loss = 9.521862030029297\n",
      "5[601] Loss = 0.7484488487243652\n",
      "5[901] Loss = 2.3971590995788574\n",
      "Test accuracy = 0.9759 (9759/10000)\n"
     ]
    }
   ],
   "source": [
    "LR_0 = 0.005\n",
    "LR_T = 0.0001\n",
    "T = len(train_loader) * NEPOCHS\n",
    "\n",
    "net = Net()\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y)\n",
    "    \n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        for w in net.parameters():\n",
    "            lr_k = max(LR_T, (1 - k/T) * LR_0 + k/T * LR_T)\n",
    "            w.data.sub_(w.grad.data * lr_k)\n",
    "    \n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momemtum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep tracks of an exponentially decaying moving average of all gradients $v$.  \n",
    "Update the wieghts in the direction of v.  \n",
    "\n",
    "$$v \\leftarrow \\alpha v - \\epsilon \\nabla_\\theta J$$\n",
    "$$\\theta \\leftarrow \\theta + v$$  \n",
    "\n",
    "$\\epsilon$: learning rate.  \n",
    "$\\alpha$: momemtum coefficient.  \n",
    "The initial value of $v$ influence the converge, it usually starts at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commom values for $\\alpha$ are $0.5$, $0.9$ and $0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.8729248046875\n",
      "1[301] Loss = 14.689855575561523\n",
      "1[601] Loss = 5.42473030090332\n",
      "1[901] Loss = 20.71593475341797\n",
      "Test accuracy = 0.9366 (9366/10000)\n",
      "2[1] Loss = 7.896501541137695\n",
      "2[301] Loss = 2.8708600997924805\n",
      "2[601] Loss = 12.732126235961914\n",
      "2[901] Loss = 5.810671806335449\n",
      "Test accuracy = 0.9523 (9523/10000)\n",
      "3[1] Loss = 13.077378273010254\n",
      "3[301] Loss = 8.24414348602295\n",
      "3[601] Loss = 6.599118709564209\n",
      "3[901] Loss = 4.978887557983398\n",
      "Test accuracy = 0.9643 (9643/10000)\n",
      "4[1] Loss = 8.29452133178711\n",
      "4[301] Loss = 3.3276140689849854\n",
      "4[601] Loss = 5.042981147766113\n",
      "4[901] Loss = 9.86808967590332\n",
      "Test accuracy = 0.9615 (9615/10000)\n",
      "5[1] Loss = 12.975824356079102\n",
      "5[301] Loss = 1.485975980758667\n",
      "5[601] Loss = 5.177299976348877\n",
      "5[901] Loss = 2.8250112533569336\n",
      "Test accuracy = 0.9689 (9689/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "ALPHA = 0.9\n",
    "\n",
    "net = Net()\n",
    "\n",
    "vs = list()\n",
    "for w in net.parameters():\n",
    "    v = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    vs.append(v)\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y)\n",
    "\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            vs[i].data = ALPHA * vs[i].data - LR * w.grad.data\n",
    "            w.data.add_(vs[i].data)\n",
    "    \n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Nestverov momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates gradient after the current velocity is applied.  \n",
    "\n",
    "$$v \\leftarrow \\alpha v - \\epsilon \\nabla_\\theta J(\\theta + \\alpha v)$$\n",
    "$$\\theta \\leftarrow \\theta + v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 148.1569366455078\n",
      "1[301] Loss = 22.189186096191406\n",
      "1[601] Loss = 25.577085494995117\n",
      "1[901] Loss = 17.550888061523438\n",
      "Test accuracy = 0.9499 (9499/10000)\n",
      "2[1] Loss = 11.429716110229492\n",
      "2[301] Loss = 11.429800033569336\n",
      "2[601] Loss = 14.804403305053711\n",
      "2[901] Loss = 12.393882751464844\n",
      "Test accuracy = 0.9615 (9615/10000)\n",
      "3[1] Loss = 9.213675498962402\n",
      "3[301] Loss = 3.9947948455810547\n",
      "3[601] Loss = 9.342105865478516\n",
      "3[901] Loss = 2.3476529121398926\n",
      "Test accuracy = 0.9697 (9697/10000)\n",
      "4[1] Loss = 0.6105036735534668\n",
      "4[301] Loss = 1.2539119720458984\n",
      "4[601] Loss = 3.8320446014404297\n",
      "4[901] Loss = 3.2328386306762695\n",
      "Test accuracy = 0.968 (9680/10000)\n",
      "5[1] Loss = 3.1399521827697754\n",
      "5[301] Loss = 2.3626508712768555\n",
      "5[601] Loss = 12.396028518676758\n",
      "5[901] Loss = 3.1604554653167725\n",
      "Test accuracy = 0.9726 (9726/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "ALPHA = 0.9\n",
    "\n",
    "net = Net()\n",
    "\n",
    "vs = list()\n",
    "for w in net.parameters():\n",
    "    v = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    vs.append(v)\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            params[i].data.add_(ALPHA * vs[i])\n",
    "        \n",
    "        y_logits = net(X)       \n",
    "        loss = criterion(y_logits, y)\n",
    "\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            w.data.sub_(ALPHA * vs[i])\n",
    "            vs[i].data = ALPHA * vs[i].data - LR * w.grad.data\n",
    "            w.data.add_(vs[i].data)\n",
    "    \n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale gradients by the inverse of the sum of all squared previous gradients.  \n",
    "\n",
    "$$r\\leftarrow r + \\nabla_\\theta J \\odot \\nabla_\\theta J$$\n",
    "$$\\theta \\leftarrow \\theta - \\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot \\nabla_\\theta J$$  \n",
    "\n",
    "$\\epsilon$: learning rate.  \n",
    "$\\delta$: small constant ($10^{-7}$) to avoid division by $0$.  \n",
    "$r$ is initialized at $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large gradient results in small learning rate.  \n",
    "small gradients results in large learning rate. \n",
    "It makes progress on more gently slopes directions of the parameters space.  \n",
    "Fast for convex optimization, but might also work for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.806884765625\n",
      "1[301] Loss = 38.82220458984375\n",
      "1[601] Loss = 30.04961395263672\n",
      "1[901] Loss = 24.438278198242188\n",
      "Test accuracy = 0.8982 (8982/10000)\n",
      "2[1] Loss = 23.91636085510254\n",
      "2[301] Loss = 33.47201919555664\n",
      "2[601] Loss = 17.666114807128906\n",
      "2[901] Loss = 20.431621551513672\n",
      "Test accuracy = 0.9089 (9089/10000)\n",
      "3[1] Loss = 11.867258071899414\n",
      "3[301] Loss = 26.43012237548828\n",
      "3[601] Loss = 15.245670318603516\n",
      "3[901] Loss = 14.666797637939453\n",
      "Test accuracy = 0.9136 (9136/10000)\n",
      "4[1] Loss = 16.747650146484375\n",
      "4[301] Loss = 16.740951538085938\n",
      "4[601] Loss = 17.106903076171875\n",
      "4[901] Loss = 21.785045623779297\n",
      "Test accuracy = 0.9181 (9181/10000)\n",
      "5[1] Loss = 29.080963134765625\n",
      "5[301] Loss = 12.126216888427734\n",
      "5[601] Loss = 12.868086814880371\n",
      "5[901] Loss = 9.228414535522461\n",
      "Test accuracy = 0.9211 (9211/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "DELTA = 1e-7\n",
    "\n",
    "NEPOCHS = 5\n",
    "\n",
    "net = Net()\n",
    "\n",
    "rs = list()\n",
    "for w in net.parameters():\n",
    "    r = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    rs.append(r)\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):        \n",
    "        y_logits = net(X)\n",
    "        \n",
    "        loss = criterion(y_logits, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            rs[i].data.add_(w.grad.data ** 2)\n",
    "            w.data.sub_(LR / torch.sqrt(DELTA+rs[i].data) * w.grad.data)\n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a modified version of Adagrad.  \n",
    "$r$ is now an exponentially dacying average.  \n",
    "\n",
    "$$r\\leftarrow \\rho r + (1 - \\rho) \\nabla_\\theta J \\odot \\nabla_\\theta J$$\n",
    "$$\\theta \\leftarrow \\theta - \\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot \\nabla_\\theta J$$  \n",
    "\n",
    "$\\rho$: day rate.  \n",
    "$\\epsilon$: learning rate.  \n",
    "$\\delta$: small constant ($10^{-6}$) to avoid division by $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives better results than Adagrad for deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.15213012695312\n",
      "1[301] Loss = 16.530723571777344\n",
      "1[601] Loss = 13.429508209228516\n",
      "1[901] Loss = 7.354430675506592\n",
      "Test accuracy = 0.9501 (9501/10000)\n",
      "2[1] Loss = 11.361360549926758\n",
      "2[301] Loss = 13.25605297088623\n",
      "2[601] Loss = 8.930535316467285\n",
      "2[901] Loss = 2.5876593589782715\n",
      "Test accuracy = 0.9653 (9653/10000)\n",
      "3[1] Loss = 1.5092804431915283\n",
      "3[301] Loss = 2.7342429161071777\n",
      "3[601] Loss = 1.9135382175445557\n",
      "3[901] Loss = 3.1017608642578125\n",
      "Test accuracy = 0.9663 (9663/10000)\n",
      "4[1] Loss = 2.0715930461883545\n",
      "4[301] Loss = 2.3610613346099854\n",
      "4[601] Loss = 0.3909754753112793\n",
      "4[901] Loss = 5.248087406158447\n",
      "Test accuracy = 0.9693 (9693/10000)\n",
      "5[1] Loss = 3.4261670112609863\n",
      "5[301] Loss = 1.4230625629425049\n",
      "5[601] Loss = 3.818699836730957\n",
      "5[901] Loss = 8.033636093139648\n",
      "Test accuracy = 0.9736 (9736/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "DELTA = 1e-6\n",
    "DECAY = 0.9\n",
    "\n",
    "NEPOCHS = 5\n",
    "\n",
    "net = Net()\n",
    "\n",
    "rs = list()\n",
    "for w in net.parameters():\n",
    "    r = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    rs.append(r)\n",
    "\n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        \n",
    "        loss = criterion(y_logits, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            rs[i].data = DECAY * rs[i].data + (1-DECAY) * w.grad.data**2\n",
    "            w.data.sub_(LR / torch.sqrt(DELTA+rs[i].data) * w.grad.data)\n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp with Neskerov momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add neskerov momemtum to RMSProp: \n",
    "\n",
    "$$r\\leftarrow \\rho r + (1 - \\rho) \\nabla_\\theta J \\odot \\nabla_\\theta J$$\n",
    "$$v \\leftarrow v - \\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot \\nabla_\\theta J$$  \n",
    "$$\\theta \\leftarrow \\theta + v$$  \n",
    "\n",
    "$v$ is a moving average of the scaled gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.29151916503906\n",
      "1[301] Loss = 9.260927200317383\n",
      "1[601] Loss = 19.946502685546875\n",
      "1[901] Loss = 25.767475128173828\n",
      "Test accuracy = 0.927 (9270/10000)\n",
      "2[1] Loss = 13.981639862060547\n",
      "2[301] Loss = 19.20871925354004\n",
      "2[601] Loss = 8.406815528869629\n",
      "2[901] Loss = 8.474459648132324\n",
      "Test accuracy = 0.9257 (9257/10000)\n",
      "3[1] Loss = 9.32218074798584\n",
      "3[301] Loss = 11.439027786254883\n",
      "3[601] Loss = 3.137418031692505\n",
      "3[901] Loss = 32.93860626220703\n",
      "Test accuracy = 0.9354 (9354/10000)\n",
      "4[1] Loss = 11.794574737548828\n",
      "4[301] Loss = 9.430360794067383\n",
      "4[601] Loss = 12.386016845703125\n",
      "4[901] Loss = 19.25663948059082\n",
      "Test accuracy = 0.9275 (9275/10000)\n",
      "5[1] Loss = 1.8568296432495117\n",
      "5[301] Loss = 32.5167121887207\n",
      "5[601] Loss = 8.295333862304688\n",
      "5[901] Loss = 26.667999267578125\n",
      "Test accuracy = 0.9375 (9375/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "DELTA = 1e-6\n",
    "DECAY = 0.9\n",
    "ALPHA = 0.9\n",
    "\n",
    "NEPOCHS = 5\n",
    "\n",
    "net = Net()\n",
    "\n",
    "rs = list()\n",
    "vs = list()\n",
    "for w in net.parameters():\n",
    "    r = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    v = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    rs.append(r)\n",
    "    vs.append(v)\n",
    "    \n",
    "k = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            params[i].data.add_(ALPHA * vs[i])\n",
    "        \n",
    "        y_logits = net(X)\n",
    "        \n",
    "        loss = criterion(y_logits, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            w.data.sub_(ALPHA * vs[i])\n",
    "            rs[i].data = DECAY * rs[i].data + (1-DECAY) * w.grad.data**2\n",
    "            vs[i].data = ALPHA * vs[i].data - LR / torch.sqrt(DELTA+rs[i].data) * w.grad.data\n",
    "            w.data.add_(vs[i].data)\n",
    "    \n",
    "        k += 1\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an estimate of the first-order and second-order moments.  \n",
    "Then correct the bias on these estimates.  \n",
    "These corrected estimates are then used to compute the weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s \\leftarrow \\rho_1 s + (1 - \\rho_1) \\nabla_\\theta J$$\n",
    "$$r \\leftarrow \\rho_2 r + (1 - \\rho_2) \\nabla_\\theta J \\odot \\nabla_\\theta J$$\n",
    "$$\\hat{s} \\leftarrow \\frac{s}{1 - \\rho_1^t}$$\n",
    "$$\\hat{r} \\leftarrow \\frac{r}{1 - \\rho_2^t}$$\n",
    "$$\\theta \\leftarrow \\theta - \\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}$$  \n",
    "\n",
    "$t$: iteration counter.  \n",
    "$s$ et $r$: respectively first et second-order moments.  \n",
    "$\\hat{s}$ et $\\hat{r}$: respectively corrected first et second-order moments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$: learning rate (default: $0.001$)  \n",
    "$\\rho_1$: decay rate for first-order moments estimates (default: $0.9$)   \n",
    "$\\rho_2$: decay rate for second-order moments estimates (default: $0.999$)   \n",
    "$\\delta$: small contant to avoid division by zero (default: $10^{-8}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 146.7119140625\n",
      "1[301] Loss = 9.362897872924805\n",
      "1[601] Loss = 18.847803115844727\n",
      "1[901] Loss = 9.547917366027832\n",
      "Test accuracy = 0.9514 (9514/10000)\n",
      "2[1] Loss = 11.213214874267578\n",
      "2[301] Loss = 5.74330472946167\n",
      "2[601] Loss = 13.582456588745117\n",
      "2[901] Loss = 19.507999420166016\n",
      "Test accuracy = 0.9652 (9652/10000)\n",
      "3[1] Loss = 1.296182632446289\n",
      "3[301] Loss = 2.2929320335388184\n",
      "3[601] Loss = 7.244189262390137\n",
      "3[901] Loss = 3.0191147327423096\n",
      "Test accuracy = 0.9683 (9683/10000)\n",
      "4[1] Loss = 3.5898241996765137\n",
      "4[301] Loss = 0.941504716873169\n",
      "4[601] Loss = 7.777242660522461\n",
      "4[901] Loss = 3.063556671142578\n",
      "Test accuracy = 0.9743 (9743/10000)\n",
      "5[1] Loss = 1.0956976413726807\n",
      "5[301] Loss = 3.640176773071289\n",
      "5[601] Loss = 0.3000774383544922\n",
      "5[901] Loss = 3.1564254760742188\n",
      "Test accuracy = 0.9765 (9765/10000)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "DELTA = 1e-8\n",
    "DEC1 = 0.9\n",
    "DEC2 = 0.999\n",
    "\n",
    "NEPOCHS = 5\n",
    "\n",
    "net = Net()\n",
    "\n",
    "s = list()\n",
    "r = list()\n",
    "for w in net.parameters():\n",
    "    v = torch.zeros(w.shape, dtype=w.dtype)\n",
    "    s.append(torch.zeros(w.shape, dtype=w.dtype))\n",
    "    r.append(torch.zeros(w.shape, dtype=w.dtype))\n",
    "    \n",
    "t = 1\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        params = list(net.parameters())\n",
    "        for i in range(len(params)):\n",
    "            w = params[i]\n",
    "            s[i].data = DEC1 * s[i].data + (1-DEC1) * w.grad.data\n",
    "            r[i].data = DEC2 * r[i].data + (1-DEC2) * w.grad.data**2\n",
    "            sc = s[i].data / (1 - DEC1**t)\n",
    "            rc = r[i].data / (1 - DEC2**t)\n",
    "\n",
    "            vs[i].data = ALPHA * vs[i].data - LR / torch.sqrt(DELTA+rs[i].data) * w.grad.data\n",
    "            w.data.sub_(LR * sc.data / (torch.sqrt(rc.data) + DELTA))\n",
    "            t += 1\n",
    "            \n",
    "            \n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    run_tests(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
