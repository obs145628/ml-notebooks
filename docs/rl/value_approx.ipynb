{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cp = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.193\n"
     ]
    }
   ],
   "source": [
    "def eval_episode(env, params):\n",
    "    \n",
    "    s = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while True:\n",
    "        a = int(np.dot(s, params) > 0)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        score += r\n",
    "        if done: break\n",
    "            \n",
    "    return score\n",
    "\n",
    "def eval_episodes(env, params, n):\n",
    "    avg = 0\n",
    "    for _ in range(n):\n",
    "        avg += eval_episode(env, params)\n",
    "    return avg / n\n",
    "\n",
    "def random_search(env, nevals, niters):\n",
    "    space_len = len(env_cp.observation_space.low)\n",
    "    best_params = None\n",
    "    best_val = float('-inf')\n",
    "    \n",
    "    for _ in range(niters):\n",
    "        params = np.random.randn(space_len)\n",
    "        val = eval_episodes(env, params, nevals)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params\n",
    "    \n",
    "\n",
    "rand_params = random_search(env_cp, 100, 20)\n",
    "print(eval_episodes(env_cp, rand_params, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAgent:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return int(np.dot(state, self.params) > 0)\n",
    "\n",
    "rand_agent = LinearAgent(rand_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2581.1303337730683\n",
      "2979.6153222180064\n",
      "2916.907838353903\n",
      "3168.62935977055\n",
      "2544.119967973882\n",
      "2497.628668954892\n",
      "3007.1863764520526\n",
      "3041.5115940508626\n",
      "2617.0269577897407\n",
      "2326.8572142882504\n",
      "2339.346790498743\n",
      "2980.76642087632\n",
      "3293.7857722817575\n",
      "2776.696410838806\n",
      "2768.510575879815\n",
      "2390.3746318024228\n",
      "2746.7058138191533\n",
      "2440.213038679841\n",
      "2707.3482849840343\n",
      "2448.0002729960943\n",
      "2346.0587353579126\n",
      "2573.4307564497103\n",
      "3128.2397105527402\n",
      "2590.458034702129\n",
      "3262.5552125326562\n",
      "2853.164975976686\n",
      "2686.217303295113\n",
      "2704.4355294467377\n",
      "2731.4641719718857\n",
      "2511.3752339445114\n",
      "2692.1491297501207\n",
      "2767.523700423666\n",
      "2836.8820328576653\n",
      "2865.798935943203\n",
      "3069.0103344934237\n",
      "2825.15269942996\n",
      "2872.3911674859187\n",
      "2919.9872568055694\n",
      "2985.4696217113724\n",
      "2864.439122448238\n",
      "2310.3512011114694\n",
      "2651.274735135973\n",
      "2635.570201959676\n",
      "2485.0916654924386\n",
      "2687.7401538396684\n",
      "2718.6698954374547\n",
      "2991.136290135467\n",
      "2624.196095880515\n",
      "2644.431129169888\n",
      "2309.087876197769\n",
      "2834.1214207452285\n",
      "2756.4271485843983\n",
      "2094.3853093429984\n",
      "2897.0662621176607\n",
      "3179.2697312008972\n",
      "2678.403608811511\n",
      "2540.627123932295\n",
      "2841.1152295330576\n",
      "2419.800609024616\n",
      "2629.336338818429\n",
      "2833.3548480430973\n",
      "2390.255853672332\n",
      "2992.6273591629065\n",
      "3224.6336121979384\n",
      "2606.923869718309\n",
      "2090.7152352957814\n",
      "2983.200474075605\n",
      "2556.3035278669327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mc_policy_eval(env, agent, ngames):\n",
    "    \n",
    "    space_len = len(env_cp.observation_space.low)\n",
    "    w = np.random.randn(space_len)\n",
    "    \n",
    "    model = SGDRegressor()\n",
    "    \n",
    "    errs = []\n",
    "    t = 0\n",
    "    \n",
    "    for _ in range(ngames):\n",
    "        hist = []\n",
    "        s = env.reset()\n",
    "    \n",
    "        while True:\n",
    "            a = agent.get_action(s)\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            hist.append((s, r))\n",
    "            if done: break\n",
    "            s = s2\n",
    "        \n",
    "        gt = 0\n",
    "        for h in reversed(hist):\n",
    "            t += 1\n",
    "            gt += h[1]\n",
    "            X = h[0].reshape(1, -1)\n",
    "            y = np.array(gt).reshape(1)\n",
    "\n",
    "            #print(model.coef_)\n",
    "            \n",
    "            if hasattr(model, 'coef_'):\n",
    "                err = (model.predict(X) - y)**2\n",
    "                \n",
    "                errs.insert(0, err)\n",
    "                if len(errs) > 1000:\n",
    "                    errs = errs[:-1]\n",
    "                    \n",
    "                if t % 2500 == 0:\n",
    "                    avg_err = np.average(errs)\n",
    "                    print(avg_err)\n",
    "            \n",
    "            model.partial_fit(X, y)\n",
    "            \n",
    "    return model\n",
    "    \n",
    "mc_policy_eval(env_cp, rand_agent, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
