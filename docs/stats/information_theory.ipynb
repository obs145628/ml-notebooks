{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We would like to quantify the information conveyed by the realisation of an event, following some properties:\n",
    "- likely events should have low information content, and events that are guaranteed to happen should have no information at all.\n",
    "- less likely events should have higher information content.\n",
    "- independents event should have additive information.  \n",
    "\n",
    "These 3 properties are satisfied by the self-information of an event $x$:  \n",
    "$$I(x) = - \\log P(x)$$\n",
    "\n",
    "$I(x)$ is written in units of nats. One nat is the amount of observation gained by observing an event of probability $1/e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanon Entropy\n",
    "\n",
    "We can quantify the amount of uncertainty in an entiere probability distribution $P(x)$ using the Shannon entropy $H(P)$:\n",
    "$$H(P) = E_{x \\sim P}[I(x)] = - E_{x \\sim P}[\\log P(x)]$$\n",
    "\n",
    "Distributions that are nealy deterministic have low entropy, and distributions that are closer to uniform have high entropy.  \n",
    "\n",
    "If $x$ is a discrete random variable, we get:\n",
    "$$H(P) = - \\sum_{x_i} P(x_i) \\log P(x_i)$$\n",
    "\n",
    "If $x$ is a continuous random variable with density $f(x)$. We call $H(x)$ the differential entropy:\n",
    "$$H(P) = - \\int f(x) \\log f(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence\n",
    "\n",
    "The KL divergence mesure how different are two probability distributions $P(X)$ and $Q(x)$ over the same probability space:\n",
    "\n",
    "$$D_{KL}(P||Q) = E_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right] = E_{x \\sim P} [\\log P(x) - \\log Q(x)]$$\n",
    "\n",
    "The KL divergence has many properties:\n",
    "- it is $\\geq 0$\n",
    "- for discrete variable, it is $0$ if and only if $P$ and $Q$ are the same distribution.\n",
    "- for continuous variable, it is $0$ if and only if $P$ and $Q$ are equal almost everywhere.  \n",
    "\n",
    "It is can consired as some sort of distance between 2 distributions, but it's not a true distance metric because it's not symmetric: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$.\n",
    "\n",
    "For a discrete probability space, the KL divergence is:\n",
    "$$D_{KL}(P||Q) = \\sum_{x_i} P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}$$\n",
    "\n",
    "For a continous probability space with densities $p(x)$ and $q(x)$, the KL divergence is:\n",
    "$$D_{KL}(P||Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "The cross entropy for two propability distributions $P(X)$ and $Q(X)$ over the same probability space is defined as:\n",
    "\n",
    "$$H(P,Q) = - E_{X \\sim P} \\log Q(X)$$\n",
    "\n",
    "The cross entropy is closely related to the KL divergence:\n",
    "$$H(P,Q) = H(P) + D_{KL}(P||Q)$$\n",
    "Minimizing the cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence.  \n",
    "\n",
    "For a discrete probability space, the cross-entropy is:\n",
    "$$H(P,Q) = - \\sum_{x_i} P(x_i) \\log Q(x_i)$$\n",
    "\n",
    "For a continuous probabiliyt space with densities $p(x)$ and $q(x)$, the cross-entropy is:\n",
    "$$H(P,Q) = - \\int P(x) \\log Q(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "The mutual information between two random variables is a measure of the mutual dependence between them. It mesures how much knowing one of these variables reduces uncertainty about the other.  \n",
    "If $X$ and $Y$ are indepandant, their mutual information is $0$. On the other hand, if $X$ is a deterministic function of $Y$ and vice-versa, the mutual information is at it's maximum.  \n",
    "\n",
    "It is defined as:\n",
    "$$I(X;Y) = E_{x,y \\sim P(x,y)} \\left[ \\log \\frac{P(x,y)}{P(x)P(y)} \\right]$$\n",
    "\n",
    "Mutual information can be expressed using entropy:\n",
    "$$I(X;Y) = H(Y) - H(Y|X)$$\n",
    "\n",
    "Mutual information is also related to the KL divergence:\n",
    "$$I(X;Y) = D_{KL} (P(x,y)||P(x)P(y))$$\n",
    "\n",
    "Mutual information can be extended to the multivariate case:\n",
    "$$I(X_1; \\text{...}, X_{n+1}) = I(X_1;\\text{...};X_n) - I(X_1;\\text{...};X_n|X_{n+1})$$\n",
    "$$\\text{where } I(X_1;\\text{...};X_n|X_{n+1}) = E_{X_{n+1}}[I(X_1;\\text{...};X_n)|X_{n+1}]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
