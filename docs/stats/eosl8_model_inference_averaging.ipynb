{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bootstrap and Maximum Likelihood Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spline(X, inters, M):\n",
    "\n",
    "    Xe = np.empty((len(X), M + len(inters)))\n",
    "    \n",
    "    for i in range(M):\n",
    "        Xe[:,i] = X[:,0]**i\n",
    "    \n",
    "    for i in range(len(inters)):\n",
    "        Xe[:,M+i] = np.maximum(X[:, 0] - inters[i], 0)**(M-1)\n",
    "    return Xe\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N,1) * 4\n",
    "y = 8.4*X[:,0] + 0.8*X[:,0]**2 +0.4*X[:,0]**4 - 3.5+0.3*np.random.randn(N)\n",
    "\n",
    "q1 = np.percentile(X, 25)\n",
    "q2 = np.percentile(X, 50)\n",
    "q3 = np.percentile(X, 75)\n",
    "H = spline(X, np.array([q1,q2,q3]), 4)\n",
    "\n",
    "beta = np.linalg.inv(H.T @ H) @ H.T @ y\n",
    "preds = H @ beta\n",
    "\n",
    "utils.plot_reg(X, y, preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Inference\n",
    "\n",
    "Let's define $Z$ a continious random variable that represents the data. $z_i = (x_i,y_i)$.  \n",
    "We define a probabality density function for $Z$:\n",
    "$$z_i \\sim g_\\theta(z)$$\n",
    "\n",
    "$\\theta$ represents the parameters governing the distribution of $Z$. For example, if $Z$ is normal we will have:\n",
    "$$\\theta = (\\mu, \\sigma^2)$$\n",
    "$$g_\\theta(z) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{(z - \\mu)^2}{2\\sigma^2}) $$\n",
    "\n",
    "We define the likelihood function as:\n",
    "$$L(\\theta;z) = g_\\theta(z)$$\n",
    "This is the probability of the observed data under the model $g_\\theta$\n",
    "$$L(\\theta;Z) = \\prod_{i=1}^N g_\\theta(z_i)$$\n",
    "$L(\\theta;Z)$ is a function of $\\theta$, with the observed data $Z$ fixed.  \n",
    "\n",
    "We define the log-likelihood:\n",
    "$$l(\\theta;z) = \\log L(\\theta;z)$$\n",
    "$$l(\\theta;Z) = \\sum_{i=1}^N \\log g_\\theta(z_i)$$  \n",
    "\n",
    "The method of maximum likelihood choose the value $\\hat{\\theta}$ to maximize $l(\\theta;Z)$  \n",
    "\n",
    "We define sore function:\n",
    "$$\\dot{l}(\\theta;Z) = \\frac{\\partial l(\\theta;Z)}{\\partial \\theta}$$\n",
    "$$\\dot{l}(\\theta;Z) = \\sum_{i=1}^N \\dot{l}(\\theta;z_i)$$\n",
    "$$\\text{with } \\dot{l}(\\theta;z_i) = \\frac{\\partial l(\\theta;z_i)}{\\partial \\theta}$$\n",
    "\n",
    "We have $\\dot{l}(\\hat{\\theta};Z)=0$.  \n",
    "\n",
    "We define the information matrix as:\n",
    "$$I(\\theta) = -\\sum_{i=1}^N \\frac{\\partial^2 l(\\theta;z_i)}{\\partial \\theta \\partial \\theta^T}$$\n",
    "\n",
    "$I(\\hat{\\theta})$ is called the obsvered information.  \n",
    "The Fisher information is defined:\n",
    "$$i(\\theta) = E_\\theta[I(\\theta)]$$  \n",
    "\n",
    "We denote $\\theta_0$ as the true value of $\\theta$.  \n",
    "As $N \\to \\infty$, the sampling distribution of the matrix likelihood estimator has a limitting normal distribution:\n",
    "$$\\hat{\\theta} \\to \\mathcal{N}(\\theta_0, i(\\theta_0)^{-1})$$  \n",
    "\n",
    "The sampling distribution of $\\hat{\\theta}$ may be approximated by:\n",
    "$$\\mathcal{N}(\\hat{\\theta}, I(\\hat{\\theta})^{-1})$$\n",
    "\n",
    "We can estimate the standard error of $\\hat{\\theta_j}$:\n",
    "$$\\text{se}[\\hat{\\theta_j}] = \\sqrt{I(\\hat{\\theta}_{jj}^{-1}}$$\n",
    "\n",
    "We can have condifence bounds for $\\theta_j$:\n",
    "$$\\hat{\\theta}_j \\pm z^{(1-\\alpha)} \\sqrt{I(\\hat{\\theta}_{jj}^{-1}}$$\n",
    "\n",
    "with $z^{(1 - \\alpha)}$ the $1 - \\alpha$ percentile of the standard normal distribution. For a condifence interval of 95\\% for example, $z^{(1-\\alpha)} = 1.96$ \n",
    "\n",
    "Let's apply the log-likelihood for a gaussian of parameters $\\theta = (\\beta, \\sigma^2)$.  The log-likelihood is:\n",
    "\n",
    "$$l(\\theta) = -\\frac{N}{2} \\log \\sigma^2 2\\pi - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - h(x_i)^T\\beta)^2$$\n",
    "\n",
    "We get the maximum likelihood estimate by solving:\n",
    "\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\beta} = 0$$\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\sigma^2} = 0$$\n",
    "\n",
    "We get the results:\n",
    "\n",
    "$$\\hat{\\beta} = (H^TH)^{-1}X^Ty$$\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{\\mu}(x_i))^2$$\n",
    "\n",
    "They are identical to the least squares estimate.  \n",
    "The part of the information matrix about $\\beta$ is:\n",
    "$$I(\\beta) = (H^TH) \\frac{1}{\\sigma^2}$$\n",
    "\n",
    "So the estimated variance $(H^TH)^{-1}\\hat{\\sigma}^2$ aggrees with least sqares estimate.  \n",
    "\n",
    "So least squares and maximum likelihood with a Gaussian give the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap versus Maximum Likelihood\n",
    "\n",
    "Boostrap can be seen as a computer implementation of the maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Bayesian approach, we define a sampling model $P(Z|\\theta)$ for our data given the parameters, and a prior distribution $P(\\theta)$ for the parameters, refecting our knowledge of $\\theta$ before seeing $Z$.  \n",
    "We then compute $P(\\theta|Z)$, the updated knowledge of $\\theta$ after seeing the data.\n",
    "\n",
    "$$P(\\theta|Z) = \\frac{P(Z|\\theta P(\\theta))}{\\int P(Z|\\theta) P(\\theta) d\\theta}$$\n",
    "\n",
    "The posterior distribution can also be used to predict values of fucture observations, via the predictive distribution:\n",
    "\n",
    "$$P(z^\\text{new}|Z) = \\int P(z^\\text{new}|\\theta) P(\\theta|Z)d\\theta$$  \n",
    "\n",
    "We can use the Bayesian approach to solve the example.  \n",
    "Let's assume $\\sigma^2$ kown, and $x_1,\\text{...},x_N$ constant, the randomnes comes from $y$ varying around $\\mu(x)$.  \n",
    "\n",
    "We choose a Gaussian prior centered at zero:\n",
    "$$\\beta \\sim \\mathcal{N}(0, \\tau \\Sigma)$$\n",
    "\n",
    "with prior correlation matrix $\\Sigma$ and variance $\\tau$\n",
    "\n",
    "Using Bayesian approach, the posterior distribution is also Gaussian, with mean and coravriance:\n",
    "\n",
    "$$E(\\beta|Z) = \\left(H^TH + \\frac{\\sigma^2}{\\tau} \\Sigma^{-1}\\right)^{-1}H^Ty$$\n",
    "$$\\text{cov}(\\beta|Z) = \\left(H^TH + \\frac{\\sigma^2}{\\tau} \\Sigma^{-1}\\right)^{-1}\\sigma^2$$\n",
    "\n",
    "For our problem, $\\mu(x)$ should be smooths, we can make it bo choosing as a prior $\\Sigma=I$.  \n",
    "As $\\tau \\to \\infty$, the posterior distribution and the bootstrap distribution coincide.  \n",
    "When $\\tau \\to \\infty$, the prior is called a noninformative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Components mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH49JREFUeJzt3Xl8XHW9//HXJ0v3pnubNmmaUkoXuhMKCrLZy1LRFkGvyk+4CFav+09UlF4VvfATRS7IVblUEao/FP2JtIhFaAEFWYQu6UJa6N4mbdN0S9u0zTaf3x9zUiZt0qSZSc5M5v18PPKYc75nmY+Hcd4933Pme8zdERERaZARdgEiIpJcFAwiItKIgkFERBpRMIiISCMKBhERaUTBICIijSgYRESkEQWDiIg0omAQEZFGssIuoC0GDhzohYWFYZchIpJSli1btsfdB7W0XkoGQ2FhIUuXLg27DBGRlGJmW1uznrqSRESkEQWDiIg0omAQEZFGFAwiItKIgkFERBpRMIiISCMKBhERaSQlf8cgIuDu1EWc2voItfVOXfBaWx+JaY9QVx+zTiTaFolAvTvuTsQh4k59xPFgOuIQifi70zHrujsOuBO8elAPOB7T/u58U7W/Ox3T3midE7bh5B01ue9WH8EECOHRyDe+t5ABvbq263soGERC4O5UHq1l18Fj7Ko8xv4jNVRV11NVXUdVTT1HquuoqqmLaQuma+o4EtMW0SPbQ2fWse/3oSl5CgaRVFNTF2H3oWOUHzzGrspqdh1smD7WaLq6LtLsPnp2yaRH1yx6dsmkZ9csenbJYmCvLhR07XG8rUeXTLplZZKdlUFWhpGdmUF2ZgZZmUZ2ZnQ+KyPj3elMo0tmBlmZGWSaYQaZGUaGGRkGZhbMQ0awPCNoMwPj3fUMjrdhDdMnL2v40oz98oyucWJbzHTMghO/c5v6EraO/mZOA3EFg5n9JzALiAC7gX9z9x1m1g/4FTAKOAZ8yt3XNLH9SOBxYACwDPiku9fEU5NIR3F3dlYeY1XpAYq3V7Kq9ADvlB9mz+Hqk9btkpVBbk43cnO6MSm/L5eP78qQnG7k9om29e/ZhV5ds+jZNYvu2ZlkZOjLTsJjHkcfmZnluPvBYPpLwHh3/6yZ3QMcdvfvmdlY4Gfu/v4mtv8D8Cd3f9zM/gdY6e4PtvS+RUVFrrGSpKMdOFLDqtJKVm4/wMrSSlaWHqDiUDQEsjONsbk5jBvam6F9ujO0TzeGBF/6uTnd6NsjW/+yldCZ2TJ3L2ppvbjOGBpCIdCTd6/7jAfuDtZZZ2aFZjbE3ctjCjTgMuATQdN84A6gxWAQaW/Haut5a0clxdujQbCq9ABb9h45vvyMQT1535kDmTy8L5Py+zBuaA7dsjNDrFgkceK+xmBmdwE3AJXApUHzSuDDwMtmNh0YAeQD5TGbDgAOuHtdMF8K5MVbj0hbHa2p57mSXSws3sFL71RQF1zZzc3pxuThffhI0XCmDO/LhLw+9OmeHXK1Iu2nxWAwsyVAbhOL5rr7QnefC8w1s28BXwC+S/Rs4SdmVgysBlYA9fEUamZzgDkABQUF8exK5Li6+givbtzLguIynl2zi6qaeob26cZNFxRybmF/Jg/vy5CcbmGXKdKh4rrG0GhHZgXAInefcEK7AZuBSbFdT0F7BZDr7nVm9h7gDne/oqX30jUGiYe7s6bsIE+uKOPPq3ZQcaia3t2y+MDEocyemsf0wv66+CudUodcYzCz0e6+PpidBawL2vsCR4I7jG4BXjrhegTu7mb2InAd0TuTbgQWxlOPyKls33eEBSvKWFBcxsaKKrIzjcvGDuaaqXlcMmawrhGIBOK9xnC3mY0hervqVuCzQfs4YL6ZOfAWcHPDBma2CLjF3XcAtwGPm9mdRLubHo6zHpFG9lfV8PTqnSxYUcayrfsBmD6yPzdfeAYzJ+bSt0eXkCsUST7x3pV0bTPtrwFnNbNsZsz0JmB6PDWINKXySC0/fXE981/dSk19hNGDe/H1K8Ywa8ow8vv1CLs8kaSmXz5Lp1JdV89vXtvKf7+wgYPHarl2Wj43XVDI+KE5+h2BSCspGKRTcHeeXrWTHz27ju37jnLRWYP41lVjGTc0J+zSRFKOgkFS3hub93HXorWs3H6Asbm9+fWnpnPRWYPCLkskZSkYJGVtrDjM3c+sY3FJObk53bjnukl8eFo+mbrVVCQuCgZJOXsOV/OTJev57Rvb6J6dydevGMOnLhhJ9y663VQkERQMkjKO1tTzq1c28+DfNnK0tp5PTC/gyzNGM7Cdx6YXSTcKBkkJC1aUcfcz69h18BiXjx/CbVeNZdSgXmGXJdIpKRgk6T3w/Hr+a/E7TM7vwwMfn8r0kf3DLkmkU1MwSFK7f8k73L9kPR+elsc9103WhWWRDqBgkKTk7ty3ZD0PPL+e687J54fXTlIoiHQQBYMkHXfn3ufe4acvbuCjRfnc/eFJGu1UpAMpGCSpuDv3PPs2P//bRj527nD+zzUTFQoiHUzBIEnD3bn7r+t46O+b+MR5Bdw5a4JCQSQECgZJCu7OD55Zx7yXNvG/zi/g+x9SKIiERcEgoXN37vzLWh7+x2ZueM8IvvehszUSqkiIFAwSKnfn+0+X8MgrW/i39xby3Q+OVyiIhEzBIKFxd7735xIefXULn7pgJN++epxCQSQJKBgkFO7Odxa+xW9e38otF45k7gcUCiLJQsEgHS4Scb69cA2P/XMbcy46g29dNVahIJJEFAzSoSIRZ+6CNfzujW189uJR3HblGIWCSJJRMEiH+v7TJfzujW187pJRfP0KhYJIMsoIuwBJHxsrDjP/tS188vwRCgWRJKZgkA7zy5c3kZ2ZwZfeP1qhIJLE4goGM/tPM1tlZsVm9pyZDQva+5nZk8GyN8xsQjPbP2pmm4Pti81sSjz1SPKqOFTNE8vLuHZaPoN664lrIsks3jOGe9x9krtPAZ4GvhO03w4Uu/sk4AbgJ6fYx9fdfUrwVxxnPZKk5r+6hdr6CJ9+38iwSxGRFsQVDO5+MGa2J+DB9HjghWCddUChmQ2J570kdVVV1/Gb17dy+fghnKHHcYokvbivMZjZXWa2Hbied88YVgIfDpZPB0YA+c3s4q6gy+k+M1MfQyf0+ze3U3m0ljkXjQq7FBFphRaDwcyWmNmaJv5mAbj7XHcfDjwGfCHY7G6gr5kVA18EVgD1Tez+W8BY4FygP3DbKeqYY2ZLzWxpRUXF6fxvlBDV1kd4+B+bObewH+eM6Bd2OSLSCi3+jsHdZ7RyX48Bi4DvBl1MNwFY9PaTzcCmJva9M5isNrNHgK+doo55wDyAoqIib249SS6LVu+k7MBR7vjQ2WGXIiKtFO9dSaNjZmcB64L2vmbWJWi/BXjphOsRDdsPDV4NmA2siaceSS7uzkN/38SoQT15/9jBYZcjIq0U7y+f7zazMUAE2Ap8NmgfB8w3MwfeAm5u2MDMFgG3uPsO4DEzGwQYUByzvXQCr2zYS8nOg/zwWj2eUySVxBUM7n5tM+2vAWc1s2xmzPRl8by/JLeHXtrIoN5dmT01L+xSROQ06JfP0i7e2lHJy+v3cNMFhXTNygy7HBE5DQoGaRe/eGkTPbtkcv15I8IuRUROk4JBEq50/xH+vGonH5teQJ/u2WGXIyKnScEgCferf2zBgE9dqOEvRFKRgkESqvJILY+/uY0PTh5GXt/uYZcjIm2gYJCE+r//3MqRmno+/b4zwi5FRNpIwSAJc6y2nkde2cJFZw1i/LCcsMsRkTZSMEjCLFhRxp7D1XzmIp0tiKQyBYMkRCTizHt5E2cPy+G9owaEXY6IxEHBIAmxZG05myqq+MzFo/TYTpEUp2CQhJj30iby+3Vn5oTcsEsRkTgpGCRuy7buY+nW/dxy4UiyMvWREkl1+n+xxO2hv2+ib49sPnru8LBLEZEEUDBIXDZWHGbx2nI+ef4IenSJdxR3EUkGCgaJyy9f3kR2ZgY3vrcw7FJEJEEUDNJmFYeqeWJ5Gdedk8/AXl3DLkdEEkTBIG02/9Ut1NZHNPyFSCejYJA2qaqu4zevb+WK8bmMHNgz7HJEJIEUDNImv39zO5VHa5lzsc4WRDobBYO0yf9bVsqU4X2ZVtAv7FJEJMEUDHLatu87wtqdB/nAxKFhlyIi7UDBIKdtydpyAP5l/JCQKxGR9qBgkNO2uKSc0YN7UaiLziKdUsKCwcxuNTM3s4HBvJnZA2a2wcxWmdm0ZrY7x8xWB+s9YBqaM6lVHqnln5v36WxBpBNLSDCY2XDgcmBbTPNVwOjgbw7wYDObPwh8OmbdKxNRk7SPF9/eTX3EFQwinViizhjuA74BeEzbLODXHvU60NfMGl2tDOZz3P11d3fg18DsBNUk7WBxSTmDendlcn7fsEsRkXYSdzCY2SygzN1XnrAoD9geM18atJ24TmkL60iSqK6r529v72bGuMFkZKjHT6SzatVwmGa2BGjqCSxzgduJdiO1KzObQ7RLioKCgvZ+O2nCaxv3UlVTr24kkU6uVcHg7jOaajezicBIYGVwzTgfWG5m04EyIHaA/vygLVZZ0H6qdRpqmAfMAygqKvKm1pH2tbiknB5dMnnvqIFhlyIi7SiuriR3X+3ug9290N0LiXYFTXP3XcBTwA3B3UnnA5XuvvOE7XcCB83s/OBupBuAhfHUJO0jEnGWrC3notGD6JadGXY5ItKO2vPJKouAmcAG4AhwU8MCMyt29ynB7OeAR4HuwDPBnySZ1WWVlB+sVjeSSBpIaDAEZw0N0w58vpn1psRMLwUmJLIOSbzFJeVkZhiXjR0cdiki0s70y2dplcUl5RSN6Ee/nl3CLkVE2pmCQVq0be8R3i4/pG4kkTShYJAWPVeyC4DLxzd1x7KIdDYKBmnR4pJyxgzpTcGAHmGXIiIdQMEgp7S/qoY3t2jQPJF0omCQU3ph3W4irmcviKQTBYOc0uKScobkdGViXp+wSxGRDqJgkGYdq63npfUVzBg3RIPmiaQRBYM069WNeziiQfNE0o6CQZq1uKScXl2zeM+oAWGXIiIdSMEgTYoOmrebi88aRNcsDZonkk4UDNKk4tIDVBzSoHki6UjBIE1qGDTv0jEaNE8k3SgYpEmLS8o5b2R/+vTIDrsUEelgCgY5yeY9VWzYfVjdSCJpSsEgJ1lSUg7AjHEKBpF0pGCQkywuKWdsbm+G99egeSLpSMEgjeyrqmHp1n1crm4kkbSlYJBGnl9bHgyap2cviKQrBYM0sriknKF9ujEhLyfsUkQkJAoGOe5YbT0vr9/DjHFDMNOgeSLpSsEgx/1j/R6O1mrQPJF0p2CQ4xaXlNO7axbnn6FB80TSmYJBAKiPOM+vK+fiMYPokqWPhUg6S8g3gJndamZuZgODeTOzB8xsg5mtMrNpzWz3NzN728yKgz8NzBOS4u372XO4Rt1IIkJWvDsws+HA5cC2mOargNHB33nAg8FrU65396Xx1iHxea6knKwM4xINmieS9hJxxnAf8A3AY9pmAb/2qNeBvmY2NAHvJe1kcUk5558xgD7dNWieSLqLKxjMbBZQ5u4rT1iUB2yPmS8N2prySNCN9G07xT2SZjbHzJaa2dKKiop4ypYTbKw4zKaKKnUjiQjQiq4kM1sCNPUz2LnA7US7kdrqencvM7PewBPAJ4FfN7Wiu88D5gEUFRV5U+tI2yxuGDRPwSAitCIY3H1GU+1mNhEYCawM/qGfDyw3s+lAGTA8ZvX8oO3EfZcFr4fM7LfAdJoJBmk/i0vKOXtYDnl9u4ddiogkgTZ3Jbn7ancf7O6F7l5ItLtomrvvAp4CbgjuTjofqHT3nbHbm1lWzF1M2cDVwJq21iNtU3GomuXb9qsbSUSOi/uupGYsAmYCG4AjwE0NC8ys2N2nAF2BZ4NQyASWAL9op3qkGS+sK8cdBYOIHJewYAjOGhqmHfh8M+tNCV6rgHMS9f7SNk+v2snw/t0ZP1SD5olIlH7imsZ2HzzGKxv2MGtyngbNE5HjFAxp7KmVO4g4zJ46LOxSRCSJKBjS2MLiHUzIy+HMwb3DLkVEkoiCIU1t2H2Y1WWVzJ7S3O8ORSRdKRjS1MLiMjIMPjRZ3Ugi0piCIQ25OwuKy7jgzIEMzukWdjkikmQUDGlo+bb9bN93lFnqRhKRJigY0tCCFTvolp3BFWfrR20icjIFQ5qprY/w9KodzBg3hN7dNMS2iJxMwZBmXnqngv1HarlmqrqRRKRpCoY08+SKMvr1yOaiswaFXYqIJCkFQxo5dKyWxSXlXD1pGNmZ+k8vIk3Tt0MaefatcqrrIhoCQ0ROScGQRhYWlzG8f3emFfQLuxQRSWIKhjTRMJLq7CkaSVVETk3BkCYaRlLVj9pEpCUKhjSxoLiMiXl9OHNwr7BLEZEkp2BIAxt2H2JN2UFmTdFFZxFpmYIhDSxYsUMjqYpIqykYOjmNpCoip0vB0Mkt27qf0v1H9UAeEWk1BUMnt6C4LDqS6oTcsEsRkRShYOjEauoiPL1qJ/8yPpdeXbPCLkdEUkRCgsHMbjUzN7OBwfxYM3vNzKrN7Gun2G6kmf3TzDaY2e/NrEsi6pGol96p4MCRWmbrbiQROQ1xB4OZDQcuB7bFNO8DvgT8uIXNfwjc5+5nAvuBm+OtR971ZLFGUhWR05eIM4b7gG8A3tDg7rvd/U2gtrmNLDouw2XAH4Om+cDsBNQjREdSXaKRVEWkDeL6xjCzWUCZu69sw+YDgAPuXhfMlwLN3jpjZnPMbKmZLa2oqGjD26WXd0dS1d1IInJ6WrwiaWZLgKZuaZkL3E60G6ndufs8YB5AUVGRt7B62luwooyC/j2YVtA37FJEJMW0GAzuPqOpdjObCIwEVgajdeYDy81survvasV77wX6mllWcNaQD5S1unJpVvnBY7y6cQ+fv/RMjaQqIqetzV1J7r7a3Qe7e6G7FxLtCprWylDA3R14EbguaLoRWNjWeuRdf9ZIqiISh3a5KmlmuWZWCnwV+A8zKzWznGDZIjNruH/yNuCrZraB6DWHh9ujnnSjkVRFJB4J+9VTcNbQML2LaNdQU+vNjJneBExPVA3y7kiq3756fNiliEiK0n2MnUzDSKofnDw07FJEJEUpGDqRRiOp9tZIqiLSNgqGTkQjqYpIIigYOpEnV2gkVRGJn4Khk6ipi/CX1RpJVUTip2DoJJasLefAkVqumaqRVEUkPgqGTqC2PsKPn32bMwb15H2jNZKqiMRHwdAJPPb6VjbtqWLuzHEaSVVE4qZvkRRXeaSW+59fzwVnDuCysYPDLkdEOgEFQ4r77xfWU3m0lrkzx2vAPBFJCAVDCtu6t4r5r23ho+cMZ/ywnLDLEZFOQsGQwu5+Zh3ZmRncevlZYZciIp2IgiFFvbF5H8+s2cVnLx7F4BwNfyEiiaNgSEGRiHPnX0rIzenGp993RtjliEgno2BIQQtXlrGqtJJvXDmG7l0ywy5HRDoZBUOKOVpTz4/++jYT8/posDwRaRcKhhTzy5c3sbPyGP/xgXFkZOj2VBFJPAVDCtl98BgP/n0jV56dy3lnDAi7HBHppBQMKeTe596htj7CN68aG3YpItKJKRhSRMmOg/xh2XZufE8hhQN7hl2OiHRiCoYU4B69PbVP92y+eNnosMsRkU5OwZACXli3m1c37uUr7x9Nnx7ZYZcjIp2cgiHJ1dZHuGvRWs4Y2JPrzx8RdjkikgYSEgxmdquZuZkNDObHmtlrZlZtZl87xXaPmtlmMysO/qYkop7O5HdvbGNTRRW361kLItJB4n44sJkNBy4HtsU07wO+BMxuxS6+7u5/jLeOzqjyaC33LX6H944awPvH6VkLItIxEvFP0PuAbwDe0ODuu939TaA2AftPWz97cQMHjtYy9wPj9KwFEekwcQWDmc0Cytx9ZRy7ucvMVpnZfWbW9RTvNcfMlprZ0oqKijjeLjVs3VvFo69s4bpp+Zw9rE/Y5YhIGmkxGMxsiZmtaeJvFnA78J043v9bwFjgXKA/cFtzK7r7PHcvcveiQYM6/wPvf/jXdWRmGF+7YkzYpYhImmnxGoO7z2iq3cwmAiOBlUE3Rz6w3Mymu/uu1ry5u+8MJqvN7BGg2QvV6eTNLftYtHoX/3vGWQzRsxZEpIO1+eKzu68Gjl8RNbMtQJG772ntPsxsqLvvtGiyzAbWtLWeziISce58OnjWwkUjwy5HRNJQ3HclNcXMcoGlQA4QMbOvAOPd/aCZLQJucfcdwGNmNggwoBj4bHvUkyrcne8/XcLK0kru/chkenRpl/88IiKnlLBvHncvjJneRbRrqan1ZsZMX5ao90917s5df1nLo69u4ZYLR/LhaXrWgoiEQ7+YSgLuzo+efZtf/mMzN75nhG5PFZFQKRiSwP1L1vPg3zbyifMKuONDZysURCRUCoaQ/fSF9fzk+fV85Jx87pw1QaEgIqFTMIToob9v5MfPvcM1U/O4+9pJelSniCQFBUNIfvWPzfzgmXVcPWko91w3iUyFgogkCQVDCH7z2ha+/3QJV56dy33/OoUsjZoqIklE30gd7PE3tvHthW8xY9xgHvj4VA2lLSJJR99KHeiPy0r51pOrufisQfzs+ml0ydLhF5Hko2+mDrKwuIyv/3ElF4wayEOfPIeuWZlhlyQi0iQFQwf4y6qdfPUPKzlvZH9+cUMR3bIVCiKSvBQM7ezZt3bx5cdXMHV4Xx6+8Vy6d1EoiEhyUzC0oxfWlfOF3y5nQl4fHrnpXHp21aB4IpL89E3VDg4eq+X+xeuZ/9oWxg/NYf6nptO7W3bYZYmItIqCIYEiEeeJ5aX88K/r2FtVw8enF3DblWPp012hICKpQ8GQIKtKD/Ddp95ixbYDTCvoy6M3TWdCnp7VLCKpR8EQp31VNdzz7Doef3M7A3p25d6PTOaaqXka90hEUpaCoY3q6iP89o1t3PvcO1RV13HzBSP50ozR5OhagoikOAVDG7yxeR/ffeot1u48yAVnDuCOD57N6CG9wy5LRCQhFAynofzgMX6waC0LincwrE83fn79NK6akKtnKIhIp6JgaIWaugiPvLKZB55fT23E+eJlZ/K5S87Uj9VEpFNSMJzClj1V/Gl5KU8sL6PswFFmjBvMt68ez4gBPcMuTUSk3SgYTnDwWC1/WbWTJ5aVsnTrfszgwjMHcuc1E7h0zOCwyxMRaXcKBqJ3GL28YQ9PLCtlcUk51XURzhzci9uuHMs1U/PI7dMt7BJFRDpMQoLBzG4FfgwMcvc9ZnY9cBtgwCHg3919ZRPbjQQeBwYAy4BPuntNImpqjbd3HeKJ5aUsWFHG7kPV9O2Rzb+eO5xrp+UzKb+PLiqLSFqKOxjMbDhwObAtpnkzcLG77zezq4B5wHlNbP5D4D53f9zM/ge4GXgw3ppOZe/hap5auYMnlpeypuwgWRnGJWMGc905eVw6drCekyAiaS8RZwz3Ad8AFjY0uPurMctfB/JP3Mii/xy/DPhE0DQfuIN2DIbbn1zNH97cTl3EOXtYDt+5ejyzpgxjQK+u7fWWIiIpJ65gMLNZQJm7rzxFt8vNwDNNtA8ADrh7XTBfCuTFU09LhvfrwU0XFHLtOfmMzc1pz7cSEUlZLQaDmS0BcptYNBe4nWg3UnPbXko0GC5sa4Ex+5oDzAEoKCho0z7+/ZJR8ZYhItLptRgM7j6jqXYzmwiMBBrOFvKB5WY23d13mdkk4JfAVe6+t4ld7AX6mllWcNaQD5Sdoo55RK9VUFRU5C3VLSIibdPmJ7i5+2p3H+zuhe5eSLQraFoQCgXAn4jeZfROM9s78CJwXdB0IzHXKUREJBzt9WjP7xC9hvBzMys2s6UNC8xskZkNC2ZvA75qZhuC9R9up3pERKSVEvYDt+CsoWH6FuCWZtabGTO9CZieqBpERCR+7XXGICIiKUrBICIijSgYRESkEQWDiIg0YtG7RlOLmVUAVcCesGtJMgPRMWmKjsvJdExOlg7HZIS7D2pppZQMBgAzW+ruRWHXkUx0TJqm43IyHZOT6Zi8S11JIiLSiIJBREQaSeVgmBd2AUlIx6RpOi4n0zE5mY5JIGWvMYiISPtI5TMGERFpBykZDGZ2pZm9bWYbzOybYdeTDMxsi5mtPnHQwnRiZr8ys91mtiamrb+ZLTaz9cFrvzBr7GjNHJM7zKws+KwUm9nMU+2jszGz4Wb2opmVmNlbZvbloD2tPyuxUi4YzCwT+BlwFTAe+LiZjQ+3qqRxqbtPSeNb7h4Frjyh7ZvA8+4+Gng+mE8nj3LyMYHos9anBH+LOrimsNUBt7r7eOB84PPBd0i6f1aOS7lgIDoa6wZ33+TuNcDjwKyQa5Ik4O4vAftOaJ5F9HniBK+zO7SokDVzTNKau+909+XB9CFgLdHHCqf1ZyVWKgZDHrA9Zr7dnxWdIhx4zsyWBY9Blagh7r4zmN4FDAmzmCTyBTNbFXQ1pW2XiZkVAlOBf6LPynGpGAzStAvdfRrRLrbPm9lFYReUbIKnBuo2PHgQGAVMAXYC94ZbTjjMrBfwBPAVdz8YuyzdPyupGAxlwPCY+VM+KzpduHtZ8LobeBI9AKlBuZkNBQhed4dcT+jcvdzd6909AvyCNPysmFk20VB4zN3/FDTrsxJIxWB4ExhtZiPNrAvwMeCpkGsKlZn1NLPeDdPA5cCaU2+VNp4i+jxx0HPFgeNfeg2uIc0+K2ZmRB8jvNbd/ytmkT4rgZT8gVtwe939QCbwK3e/K+SSQmVmZxA9S4Do41p/m47HxMx+B1xCdJTMcuC7wALgD0ABsBX4qLunzcXYZo7JJUS7kRzYAnwmpm+90zOzC4GXgdVAJGi+neh1hrT9rMRKyWAQEZH2k4pdSSIi0o4UDCIi0oiCQUREGlEwiIhIIwoGERFpRMEgIiKNKBhERKQRBYOIiDTy/wGTwGVMWwMJbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.44557035014334134\n",
      "u1 = 1.0826249375972088\n",
      "u2 = 4.655293230922111\n",
      "s1 = 0.8105011324221852\n",
      "s2 = 0.8196989381005306\n"
     ]
    }
   ],
   "source": [
    "def gauss_pdf(mu, s2, x):\n",
    "    num = np.exp(-((x - mu)**2)/(2*s2))\n",
    "    den = np.sqrt(2 * np.pi * s2)\n",
    "    return num/den\n",
    "\n",
    "def em(y, max_iters = 10000):\n",
    "    \n",
    "    #inititalization\n",
    "    u1, u2 = np.random.choice(y, 2, replace=False)\n",
    "    p = 0.5\n",
    "    s1 = s2 = np.mean((y - np.mean(y))**2)\n",
    "    lls = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        ## Expectation Step\n",
    "        res = np.empty(len(y))\n",
    "        for i in range(len(y)):\n",
    "            res[i] = (p*gauss_pdf(u2,s2,y[i])) / (\n",
    "                (1-p)*gauss_pdf(u1,s1,y[i]) + p*gauss_pdf(u2,s2,y[i])\n",
    "            )                                           \n",
    "        \n",
    "        ##maximization step\n",
    "        u1 = np.sum((1 - res) * y) / np.sum(1 - res)\n",
    "        u2 = np.sum(res * y) / np.sum(res)\n",
    "        s1 = np.sum((1 - res) * (y - u1)**2) / np.sum(1 - res)\n",
    "        s2 = np.sum(res * (y - u2)**2) / np.sum(res)\n",
    "        p = np.mean(res)\n",
    "        \n",
    "        ##compute log-likelihood\n",
    "        ll = 0\n",
    "        for i in range(len(y)):\n",
    "            ll += np.log((1-p)*gauss_pdf(u1,s1,y[i])\n",
    "                       + p*gauss_pdf(u2,s2,y[i]))\n",
    "        lls.append(ll)\n",
    "        if len(lls) > 1:\n",
    "            diff = (lls[-1] - lls[-2])**2\n",
    "            if diff < 1e-10:\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return p, u1, u2, s1, s2, lls\n",
    "\n",
    "y = np.array([\n",
    "    -0.39, 0.12, 0.94, 1.67, 1.76, 2.44, 3.72, 4.28, 4.92, 5.53,\n",
    "     0.06, 0.48, 1.01, 1.68, 1.80, 3.25, 4.12, 4.60, 5.28, 6.22\n",
    "])\n",
    "\n",
    "p, u1, u2, s1, s2, lls = em(y)\n",
    "\n",
    "plt.plot(np.arange(1,len(lls)+1), lls)\n",
    "plt.show()\n",
    "\n",
    "print('p =', p)\n",
    "print('u1 =', u1)\n",
    "print('u2 =', u2)\n",
    "print('s1 =', s1)\n",
    "print('s2 =', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation\n",
    "\n",
    "We have a dataset of points $x_1,\\text{...},x_N$, with $x_i \\in \\mathbb{R}^p$.  \n",
    "Density estimation build a model to estimate and fit the density $P(x)$.  \n",
    "\n",
    "It can be used for anomaly detection. We have a dataset of normal events $x_1,\\text{...},x_N$, and we build a model $P(x)$.  \n",
    "For a new event $x$, we compute the probability that $x$ comes from $P(x)$. If this probability is too low, the example is detected as an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "We have the random variable for our dataset $x$.  \n",
    "Let's define a lattent (hidden / unobserved) latent variable $z$.  \n",
    "$(x,z)$ have a joint distribution:\n",
    "$$P(x,z) = P(x|z) P(z)$$\n",
    "\n",
    "with\n",
    "$$Z \\sim \\text{Multinomial}(\\phi)$$\n",
    "$$X|Z=j \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)$$  \n",
    "\n",
    "We need to find $\\phi \\in \\mathbb{R}^{K}$ ($K$ number of gaussians), $\\mu_j \\in \\mathbb{R}^p$, and $\\Sigma_j \\in \\mathbb{R}^{p*p}$ that fit best the dataset $X$.  \n",
    "\n",
    "If the $z_i$ are known, the problem can be easily solved with maximum likelihood estimation:\n",
    "$$l(\\phi,\\mu,\\Sigma) = \\sum_{i=1}^N \\log p(x_i,z_i;\\phi,\\mu,\\Sigma)$$\n",
    "\n",
    "We get the folowing estimators:\n",
    "\n",
    "$$\\phi_j = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}(z_i = j)$$\n",
    "\n",
    "$$\\mu_j = \\frac{\\sum_{i=1}^N \\mathbf{1}(z_i = j)x_i}{\\sum_{i=1}^N \\mathbf{1}(z_i = j)}$$\n",
    "\n",
    "$$\\Sigma_j = \\frac{\\sum_{i=1}^N \\mathbf{1}(z_i = j)(x_i - \\mu_j)(x_i - \\mu_j)^T}{\\sum_{i=1}^N \\mathbf{1}(z_i = j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi[0] = 0.2\n",
      "mu[0] = [ 1.4  6.7 -2.4]\n",
      "sig[0] =\n",
      "[[ 62.96  34.8  -18.38]\n",
      " [ 34.8   20.45 -16.47]\n",
      " [-18.38 -16.47 176.34]]\n",
      "phi[1] = 0.5\n",
      "mu[1] = [ 0.2 -5.2  4.5]\n",
      "sig[1] =\n",
      "[[ 95.89   1.9   -7.27]\n",
      " [  1.9   40.56 -27.96]\n",
      " [ -7.27 -27.96  40.38]]\n",
      "phi[2] = 0.25\n",
      "mu[2] = [-5.7 -5.3 -1.4]\n",
      "sig[2] =\n",
      "[[ 97.02    62.78    73.985 ]\n",
      " [ 62.78    56.24    87.35  ]\n",
      " [ 73.985   87.35   168.8825]]\n",
      "phi[3] = 0.05\n",
      "mu[3] = [3.5 4.6 2.8]\n",
      "sig[3] =\n",
      "[[ 89.94  40.77 -72.09]\n",
      " [ 40.77  50.66 -64.39]\n",
      " [-72.09 -64.39 103.7 ]]\n"
     ]
    }
   ],
   "source": [
    "N = 50#0000\n",
    "p = 3\n",
    "K = 4\n",
    "\n",
    "rphi = [0.2, 0.5, 0.25, 0.05]\n",
    "\n",
    "rmu = [\n",
    "    np.array([1.4, 6.7, -2.4]),\n",
    "    np.array([0.2, -5.2, 4.5]),\n",
    "    np.array([-5.7, -5.3, -1.4]),\n",
    "    np.array([3.5, 4.6, 2.8])\n",
    "]\n",
    "\n",
    "rsig = [\n",
    "    np.array([[1.4,1.6,2.8],[0.4,-0.5,12.5],[7.8,4.2,-3.5]]),\n",
    "    np.array([[1.8,-2.8,5.8],[8.4,3.4,-2.5],[-4.7,4.6,-0.7]]),\n",
    "    np.array([[2.3,3.6,4.2],[7.3,2.2,-1.55],[6.2,6.2,12.2]]),\n",
    "    np.array([[1.7,-4.8,3.9],[2.8,1.9,-6.5],[8.9,4.9,-6.8]])\n",
    "]\n",
    "for i in range(K):\n",
    "    rsig[i] = rsig[i].T @ rsig[i] #generate PSD matrices\n",
    "\n",
    "\n",
    "Zc = np.random.multinomial(N, rphi)\n",
    "Z = []\n",
    "for i in range(K):\n",
    "    Z += [i] * Zc[i]\n",
    "Z = np.array(Z).astype(np.int)\n",
    "np.random.shuffle(Z)\n",
    "\n",
    "X = np.empty((N, p))\n",
    "for i in range(K):\n",
    "    X[Z==i] = np.random.multivariate_normal(rmu[i], rsig[i], \n",
    "                                            size=len(X[Z==i]),\n",
    "                                            check_valid='raise')\n",
    "    \n",
    "for i in range(K):\n",
    "    print('phi[{}] = {}'.format(i, rphi[i]))\n",
    "    print('mu[{}] = {}'.format(i, rmu[i]))\n",
    "    print('sig[{}] ='.format(i))\n",
    "    print(rsig[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi[0] = 0.16\n",
      "mu[0] = [ 6.83030077  9.81523784 -6.89434724]\n",
      "sig[0] =\n",
      "[[ 58.75850249  37.25204025 -20.92176941]\n",
      " [ 37.25204025  24.2950051  -20.43804884]\n",
      " [-20.92176941 -20.43804884 220.5537518 ]]\n",
      "phi[1] = 0.6\n",
      "mu[1] = [ 1.15535283 -4.95950239  2.896043  ]\n",
      "sig[1] =\n",
      "[[ 52.2629606    1.19635621   3.14975878]\n",
      " [  1.19635621  16.38472988 -15.68584243]\n",
      " [  3.14975878 -15.68584243  33.42905723]]\n",
      "phi[2] = 0.2\n",
      "mu[2] = [-4.99199098 -4.01332495  2.04466062]\n",
      "sig[2] =\n",
      "[[ 74.39138459  46.76362125  58.90115346]\n",
      " [ 46.76362125  42.1791728   68.25336861]\n",
      " [ 58.90115346  68.25336861 130.52718975]]\n",
      "phi[3] = 0.04\n",
      "mu[3] = [3.56327207 5.37717032 0.72195474]\n",
      "sig[3] =\n",
      "[[  0.13387665   2.89867135  -3.25366302]\n",
      " [  2.89867135  62.76146971 -70.4476804 ]\n",
      " [ -3.25366302 -70.4476804   79.07519847]]\n"
     ]
    }
   ],
   "source": [
    "def mle_mixture(X, Z, K):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    phi = np.zeros(K)\n",
    "    mu = np.zeros((K, p))\n",
    "    sig = np.zeros((K,p,p))\n",
    "    \n",
    "    #count number of samples from each mixture\n",
    "    cs = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        cs[k] = np.sum(Z == k)\n",
    "    #print(cs)\n",
    "    \n",
    "    #compute phi\n",
    "    phi = cs / N\n",
    "        \n",
    "    #compute mu\n",
    "    for i in range(N):\n",
    "        mu[Z[i]] += X[i]\n",
    "    mu /= cs.reshape(K,1)\n",
    "    \n",
    "    #compute sig\n",
    "    for i in range(N):\n",
    "        sig[Z[i]] += np.outer(X[i] - mu[Z[i]], X[i] - mu[Z[i]])\n",
    "    sig /= cs.reshape(K,1,1)\n",
    "    \n",
    "    return phi, mu, sig\n",
    "    \n",
    "    \n",
    "    \n",
    "phi, mu, sig = mle_mixture(X, Z, K)\n",
    "\n",
    "for i in range(K):\n",
    "    print('phi[{}] = {}'.format(i, phi[i]))\n",
    "    print('mu[{}] = {}'.format(i, mu[i]))\n",
    "    print('sig[{}] ='.format(i))\n",
    "    print(sig[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm for mixture of Gaussians\n",
    "\n",
    "Repeat E-Step and M-Step until convergence\n",
    "\n",
    "### E-Step\n",
    "\n",
    "Define $w_{ij} = P(z_i=j|x_i,\\phi,\\mu,\\sigma)$\n",
    "$$w_{ij} = \\frac{P(x_i|z_i=j) P(z_i=j)}{\\sum_{l=1}^K P(x_i|z_i=l) P(z_i=l)}$$\n",
    "\n",
    "### M-Step\n",
    "\n",
    "Apply MLE:\n",
    "\n",
    "$$\\phi_j = \\frac{1}{N} \\sum_{i=1}^N w_{ij}$$\n",
    "\n",
    "$$\\mu_j = \\frac{ \\sum_{i=1}^N w_{ij} x_i}{ \\sum_{i=1}^N w_{ij}} $$\n",
    "\n",
    "$$\\sigma_j = \\frac{ \\sum_{i=1}^N w_{ij} (x_i - \\mu_j)(x_i - \\mu_j)^T}{ \\sum_{i=1}^N w_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47494733 0.1042707  0.02203099 0.39875098]\n"
     ]
    }
   ],
   "source": [
    "class MultivariateNormal:\n",
    "    \n",
    "    def __init__(self, mu, cov):\n",
    "        self.mu = mu\n",
    "        self.cov = cov\n",
    "        self.icov = np.linalg.inv(cov)\n",
    "        #self.den = 1/np.sqrt(max(np.linalg.det(self.cov), 1e-8))\n",
    "        p = len(self.mu)\n",
    "        \n",
    "        cdet = np.linalg.det(self.cov)\n",
    "        if (cdet < 0):\n",
    "            print(self.cov)\n",
    "            print(self.icov)\n",
    "            print(cdet)\n",
    "            djdjdjhfh\n",
    "        \n",
    "        self.den = 1/np.sqrt((2*np.pi)**p * cdet)\n",
    "        \n",
    "    def pdf(self, x):\n",
    "        num = np.exp(-.5  * (x-self.mu)@self.icov@(x-self.mu))\n",
    "        return num/self.den\n",
    "        \n",
    "\n",
    "def em_gauss_mixture(X,k):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    phi = np.ones(K)*1/K\n",
    "    \n",
    "    mup = np.random.choice(len(X), K, replace=False)\n",
    "    mu =  X[mup]\n",
    "    \n",
    "    Xc = np.mean(X,axis=0, keepdims=True)\n",
    "    sigi = 1/N * (Xc - X).T @ (Xc - X)\n",
    "    sig = np.repeat(sigi.reshape(1, p, p), repeats=K, axis=0)\n",
    "    \n",
    "    dens = [None] * K\n",
    "    for k in range(K):\n",
    "        dens[k] = MultivariateNormal(mu[k], sig[k])\n",
    "    \n",
    "    \n",
    "    for ite in range(1):\n",
    "    \n",
    "        \n",
    "        #Expectation step\n",
    "        w = np.empty((N, K))\n",
    "        for i in range(N):\n",
    "            \n",
    "            prs = np.array([\n",
    "               dens[j].pdf(X[i]) * phi[j] for j in range(K) \n",
    "            ])\n",
    "            prs_sum = np.sum(prs)\n",
    "            \n",
    "            for j in range(K):\n",
    "                w[i,j] = prs[j] / prs_sum\n",
    "        \n",
    "        \n",
    "        #Maximization step\n",
    "        phi = 1/N * np.sum(w, axis=0)\n",
    "        \n",
    "        for j in range(K):\n",
    "            sig[j] = 0\n",
    "            for i in range(N):\n",
    "                sig[j] += w[i,j]*np.outer(X[i] - mu[j], X[i] - mu[j])\n",
    "            sig[j] /= np.sum(w[:,j])\n",
    "        \n",
    "        \n",
    "        for j in range(K):\n",
    "            mu[j] = np.sum(w[:,j].reshape(N,1)*X, axis=0) / np.sum(w[:,j])\n",
    "        \n",
    "        for j in range(K):\n",
    "            dens[j] = MultivariateNormal(mu[j], sig[j])\n",
    "            \n",
    "\n",
    "        #print(phi)\n",
    "        \n",
    "            \n",
    "            \n",
    "    return phi, mu, sig \n",
    "    \n",
    "phi, mu, sig = em_gauss_mixture(X,K)\n",
    "\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen Inequality\n",
    "\n",
    "Let $f$ a convex function ($f''(x) \\geq 0$) and $X$ a random variable.  \n",
    "$$f(E[X]) \\leq E[f(X)]$$\n",
    "\n",
    "If $f$ is strictly convex ($f''(x) > 0$):\n",
    "$$f(E[X]) = E[f(X)] \\text{ iff } X=E[X]$$\n",
    "\n",
    "The opposite inequality holds for concave functions ($f''(x) \\leq 0$):\n",
    "$$f(E[X]) \\geq E[f(X)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm\n",
    "\n",
    "the log-likelihood of our model is:\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^N \\log \\sum_{z_i} P(x_i,z_i,\\theta)$$\n",
    "\n",
    "But the $z_i$ are not observed, and solving this problem with MLE is quite hard.  \n",
    "Another strategy is to build a lower-bound for $l(\\theta)$ (E-step) and optimize for that lower-bound (M-step).  \n",
    "\n",
    "Let's define for each $i$  some distribution $Q_i$ over the $z$'s ($\\sum_z Q_i(z) = 1, Q_i(z) \\geq 0$).\n",
    "\n",
    "Using Jensen Inequality, we find a lower bound for $l(\\theta)$:\n",
    "$$l(\\theta) \\geq \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$\n",
    "\n",
    "To get a tight lower bound, we need the Jensen step to hold for equality:\n",
    "$$\\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)} = \\text{constant}$$\n",
    "\n",
    "Solving this equality, we get:\n",
    "$$Q_i(z_i) = P(z_i|x_i;\\theta)$$\n",
    "\n",
    "EM algorithm:\n",
    "\n",
    "- E-step:\n",
    "$$\\text{Set } Q_i(z_i) = P(z_i|x_i;\\theta)$$\n",
    "\n",
    "- M-Step:\n",
    "$$\\theta \\leftarrow \\arg \\max_\\theta \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm and coordinate ascent\n",
    "\n",
    "Let's define:\n",
    "$$J(Q,\\theta) = \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$\n",
    "Then we know that:\n",
    "$$l(\\theta) \\geq J(Q,\\theta)$$\n",
    "\n",
    "We can prove that the EM algorithm is equivalent to applying Coordinate ascent on $J(Q,\\theta)$:\n",
    "- The E-Step maximixes $J$ with respect to $Q$\n",
    "- The M-Step maximixes $J$ with respect to $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC for Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.99973432 4.99983867]\n",
      "[[1.00113034 0.90072088]\n",
      " [0.90072088 1.00043999]]\n"
     ]
    }
   ],
   "source": [
    "mus = np.array([5, 5])\n",
    "sigmas = np.array([[1, .9], [.9, 1]])\n",
    "\n",
    "def get_exp_cov(X):\n",
    "    e = np.mean(X, axis=0)\n",
    "    cov = (X - e.reshape(1, -1)).T @ (X - e.reshape(1, -1)) / len(X)\n",
    "    return e, cov\n",
    "\n",
    "X = np.random.multivariate_normal(mus, sigmas, size=5000000)\n",
    "e, cov = get_exp_cov(X)\n",
    "print(e)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19800, 2)\n",
      "[5.00056319 5.002481  ]\n",
      "[[0.18802328 0.16894376]\n",
      " [0.16894376 0.1879098 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CondGauss:\n",
    "    \n",
    "    def __init__(self, mus, sigmas):\n",
    "        self.mus = mus\n",
    "        self.sigmas = sigmas\n",
    "        \n",
    "        self.sigma_xy = sigmas[0,0]-sigmas[1,0]/sigmas[1,1]*sigmas[1,0]\n",
    "        self.sigma_yx = sigmas[1,1]-sigmas[0,1]/sigmas[0,0]*sigmas[0,1]\n",
    "\n",
    "    def p_x_given_y(self, y):\n",
    "        mu = self.mus[0]+self.sigmas[1,0]/self.sigmas[0,0]*(y-self.mus[1])\n",
    "        return np.random.normal(mu, self.sigma_xy)\n",
    "\n",
    "    def p_y_given_x(self, x):\n",
    "        mu = self.mus[1]+self.sigmas[0,1]/self.sigmas[1,1]*(x-self.mus[0])\n",
    "        return np.random.normal(mu, self.sigma_yx)\n",
    "\n",
    "\n",
    "def gibbs_sampling(mus, sigmas, N=1000000):\n",
    "    samples = np.zeros((N, 2))\n",
    "    cond = CondGauss(mus, sigmas)\n",
    "    y = np.random.rand() * 10\n",
    "\n",
    "    for i in range(N):\n",
    "        x = cond.p_x_given_y(y)\n",
    "        y = cond.p_y_given_x(x)\n",
    "        samples[i] = [x,y]\n",
    "\n",
    "    return samples\n",
    "\n",
    "samples = gibbs_sampling(mus, sigmas)\n",
    "samples = samples[10000:]\n",
    "samples = samples[::50]\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "\n",
    "e, cov = get_exp_cov(samples)\n",
    "print(e)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling and LDA\n",
    "\n",
    "LDA in as algorithm to compute a distribution of topics for each documents, and a distribution of words for each topic.  \n",
    "It takes as input several documents as bag of words (number of instances of every word of the vocabulary for each document).  \n",
    "The number of a topics in an hyperparameter.  \n",
    "The algorithm can be used to distinguish topic between all documents. It can describes which words are more likely to appear for each topic.  \n",
    "Finally, given a new document, it can computes it's topic distribution. This can be helpful to categorize the text, and to find other texts of similar topics (recommander system).  \n",
    "\n",
    "Usually NLP preprocessing (stop words, lemming, stemming, filtering) is usually applied before building a bag of words from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aiw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "download dataset from kaggle:\n",
    "https://www.kaggle.com/therohk/million-headlines/data\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def load_data(max_docs = None):\n",
    "\n",
    "\n",
    "    # Loading data\n",
    "    print('Loading data...')\n",
    "\n",
    "\n",
    "    data = pd.read_csv('../../data/abcnews-date-text.csv',\n",
    "                       error_bad_lines=False)\n",
    "    data_text = data[['headline_text']]\n",
    "    data_text['index'] = data_text.index\n",
    "    docs = data_text\n",
    "    if max_docs is not None:\n",
    "        docs = docs[:max_docs]\n",
    "\n",
    "\n",
    "    # Preprocesing\n",
    "    print('Preprocessing data...')\n",
    "\n",
    "\n",
    "\n",
    "    docs = docs['headline_text'].map(preprocess)\n",
    "\n",
    "\n",
    "    # Building dictionary and bag of words\n",
    "    print('Building dictionarry and bag of words...')\n",
    "    dic = gensim.corpora.Dictionary(docs)\n",
    "    dic.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    bow = [dic.doc2bow(doc) for doc in docs]\n",
    "\n",
    "\n",
    "    # Build tf-idf mdel\n",
    "    print('Building tf-idf model...')\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    docs_tfidf = tfidf[bow]\n",
    "\n",
    "\n",
    "    print('Get {} documents'.format(len(docs)))\n",
    "    print('Get {} words'.format(len(dic)))\n",
    "    \n",
    "    return dic, bow, docs_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Building dictionarry and bag of words...\n",
      "Building tf-idf model...\n",
      "Get 1103663 documents\n",
      "Get 14173 words\n",
      "Training LDA with bag of words\n",
      "LDA results with bag of words:\n",
      "Topic: 0 \n",
      "Words: 0.024*\"australia\" + 0.022*\"test\" + 0.018*\"hospit\" + 0.017*\"hour\" + 0.013*\"driver\" + 0.011*\"prison\" + 0.011*\"releas\" + 0.011*\"china\" + 0.010*\"young\" + 0.010*\"work\"\n",
      "Topic: 1 \n",
      "Words: 0.025*\"govern\" + 0.018*\"say\" + 0.016*\"chang\" + 0.015*\"nation\" + 0.014*\"countri\" + 0.013*\"live\" + 0.013*\"interview\" + 0.011*\"council\" + 0.011*\"school\" + 0.011*\"call\"\n",
      "Topic: 2 \n",
      "Words: 0.020*\"final\" + 0.018*\"win\" + 0.015*\"leagu\" + 0.015*\"lose\" + 0.014*\"citi\" + 0.012*\"game\" + 0.012*\"john\" + 0.011*\"announc\" + 0.009*\"unit\" + 0.009*\"celebr\"\n",
      "Topic: 3 \n",
      "Words: 0.017*\"donald\" + 0.016*\"adelaid\" + 0.015*\"turnbul\" + 0.013*\"feder\" + 0.011*\"centr\" + 0.011*\"vote\" + 0.010*\"port\" + 0.009*\"refuge\" + 0.009*\"save\" + 0.009*\"deal\"\n",
      "Topic: 4 \n",
      "Words: 0.027*\"report\" + 0.021*\"famili\" + 0.017*\"water\" + 0.016*\"time\" + 0.015*\"concern\" + 0.014*\"abus\" + 0.014*\"busi\" + 0.013*\"guilti\" + 0.012*\"farm\" + 0.011*\"find\"\n",
      "Topic: 5 \n",
      "Words: 0.045*\"polic\" + 0.026*\"death\" + 0.025*\"attack\" + 0.022*\"kill\" + 0.021*\"crash\" + 0.018*\"die\" + 0.016*\"perth\" + 0.016*\"shoot\" + 0.015*\"rise\" + 0.014*\"woman\"\n",
      "Topic: 6 \n",
      "Words: 0.034*\"court\" + 0.026*\"murder\" + 0.025*\"charg\" + 0.022*\"face\" + 0.018*\"miss\" + 0.017*\"accus\" + 0.016*\"child\" + 0.015*\"alleg\" + 0.014*\"peopl\" + 0.014*\"trial\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"world\" + 0.020*\"market\" + 0.019*\"women\" + 0.017*\"tasmania\" + 0.015*\"open\" + 0.014*\"share\" + 0.014*\"break\" + 0.014*\"life\" + 0.013*\"take\" + 0.013*\"student\"\n",
      "Topic: 8 \n",
      "Words: 0.042*\"australian\" + 0.026*\"queensland\" + 0.022*\"south\" + 0.021*\"north\" + 0.016*\"elect\" + 0.015*\"australia\" + 0.014*\"west\" + 0.013*\"help\" + 0.011*\"victoria\" + 0.010*\"flood\"\n",
      "Topic: 9 \n",
      "Words: 0.037*\"trump\" + 0.037*\"year\" + 0.022*\"hous\" + 0.021*\"canberra\" + 0.012*\"say\" + 0.011*\"bank\" + 0.011*\"polit\" + 0.011*\"elect\" + 0.011*\"hobart\" + 0.010*\"parti\"\n",
      "Training LDA with TF-IDF\n",
      "LDA results with TF-IDF\n",
      "Topic: 0\n",
      "Words: 0.010*\"kill\" + 0.009*\"marriag\" + 0.008*\"septemb\" + 0.007*\"asylum\" + 0.007*\"attack\" + 0.007*\"bomb\" + 0.006*\"syria\" + 0.006*\"seeker\" + 0.006*\"islam\" + 0.006*\"jam\"\n",
      "Topic: 1\n",
      "Words: 0.010*\"turnbul\" + 0.009*\"michael\" + 0.008*\"thursday\" + 0.007*\"malcolm\" + 0.007*\"june\" + 0.006*\"mother\" + 0.006*\"social\" + 0.006*\"spring\" + 0.005*\"media\" + 0.005*\"inquest\"\n",
      "Topic: 2\n",
      "Words: 0.020*\"charg\" + 0.018*\"polic\" + 0.016*\"murder\" + 0.013*\"court\" + 0.011*\"jail\" + 0.010*\"woman\" + 0.009*\"accus\" + 0.009*\"assault\" + 0.009*\"death\" + 0.009*\"alleg\"\n",
      "Topic: 3\n",
      "Words: 0.014*\"crash\" + 0.009*\"queensland\" + 0.008*\"royal\" + 0.008*\"miss\" + 0.007*\"search\" + 0.007*\"street\" + 0.007*\"die\" + 0.007*\"david\" + 0.006*\"truck\" + 0.006*\"wednesday\"\n",
      "Topic: 4\n",
      "Words: 0.016*\"market\" + 0.010*\"share\" + 0.008*\"rise\" + 0.007*\"christma\" + 0.007*\"australian\" + 0.006*\"bank\" + 0.006*\"novemb\" + 0.006*\"rat\" + 0.006*\"price\" + 0.006*\"murray\"\n",
      "Topic: 5\n",
      "Words: 0.008*\"leagu\" + 0.007*\"domest\" + 0.006*\"violenc\" + 0.006*\"scott\" + 0.005*\"rugbi\" + 0.005*\"wrap\" + 0.005*\"anim\" + 0.005*\"player\" + 0.005*\"dog\" + 0.005*\"hors\"\n",
      "Topic: 6\n",
      "Words: 0.012*\"live\" + 0.011*\"drum\" + 0.011*\"elect\" + 0.007*\"octob\" + 0.007*\"friday\" + 0.007*\"monday\" + 0.007*\"tuesday\" + 0.006*\"dairi\" + 0.006*\"polit\" + 0.006*\"liber\"\n",
      "Topic: 7\n",
      "Words: 0.010*\"govern\" + 0.008*\"fund\" + 0.006*\"budget\" + 0.006*\"grandstand\" + 0.006*\"say\" + 0.006*\"abbott\" + 0.006*\"chang\" + 0.005*\"minist\" + 0.005*\"plan\" + 0.005*\"energi\"\n",
      "Topic: 8\n",
      "Words: 0.023*\"rural\" + 0.014*\"news\" + 0.009*\"sport\" + 0.008*\"health\" + 0.008*\"nation\" + 0.007*\"busi\" + 0.007*\"program\" + 0.007*\"juli\" + 0.006*\"mental\" + 0.006*\"centr\"\n",
      "Topic: 9\n",
      "Words: 0.020*\"countri\" + 0.018*\"hour\" + 0.017*\"trump\" + 0.013*\"interview\" + 0.010*\"podcast\" + 0.009*\"final\" + 0.008*\"australia\" + 0.008*\"donald\" + 0.007*\"gold\" + 0.007*\"world\"\n"
     ]
    }
   ],
   "source": [
    "dic, bow, docs_tfidf = load_data()\n",
    "\n",
    "print('Training LDA with bag of words')\n",
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=10, id2word=dic, passes=2, workers=2)\n",
    "\n",
    "print('LDA results with bag of words:')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "\n",
    "print('Training LDA with TF-IDF')\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(docs_tfidf, num_topics=10, id2word=dic, passes=2, workers=4)\n",
    "\n",
    "print('LDA results with TF-IDF')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}\\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Building dictionarry and bag of words...\n",
      "Building tf-idf model...\n",
      "Get 1000 documents\n",
      "Get 18 words\n",
      "Training LDA with bag of words\n",
      "LDA results with bag of words:\n",
      "Topic: 0 \n",
      "Words: 0.296*\"say\" + 0.240*\"govt\" + 0.172*\"iraq\" + 0.053*\"death\" + 0.053*\"record\" + 0.043*\"plan\" + 0.043*\"investig\" + 0.023*\"claim\" + 0.023*\"continu\" + 0.014*\"rain\"\n",
      "Topic: 1 \n",
      "Words: 0.594*\"record\" + 0.122*\"say\" + 0.094*\"plan\" + 0.094*\"warn\" + 0.021*\"iraq\" + 0.021*\"polic\" + 0.004*\"council\" + 0.004*\"rain\" + 0.004*\"govt\" + 0.004*\"court\"\n",
      "Topic: 2 \n",
      "Words: 0.234*\"death\" + 0.218*\"charg\" + 0.204*\"continu\" + 0.198*\"investig\" + 0.092*\"polic\" + 0.026*\"murder\" + 0.008*\"claim\" + 0.002*\"plan\" + 0.002*\"iraq\" + 0.002*\"court\"\n",
      "Topic: 3 \n",
      "Words: 0.632*\"council\" + 0.111*\"plan\" + 0.072*\"warn\" + 0.046*\"iraq\" + 0.030*\"rain\" + 0.026*\"continu\" + 0.026*\"investig\" + 0.015*\"polic\" + 0.015*\"charg\" + 0.003*\"court\"\n",
      "Topic: 4 \n",
      "Words: 0.482*\"claim\" + 0.405*\"govt\" + 0.044*\"plan\" + 0.016*\"polic\" + 0.014*\"death\" + 0.003*\"iraq\" + 0.003*\"rain\" + 0.003*\"council\" + 0.003*\"court\" + 0.003*\"charg\"\n",
      "Topic: 5 \n",
      "Words: 0.507*\"polic\" + 0.300*\"murder\" + 0.052*\"charg\" + 0.040*\"court\" + 0.038*\"say\" + 0.012*\"iraq\" + 0.012*\"council\" + 0.012*\"investig\" + 0.003*\"plan\" + 0.003*\"rain\"\n",
      "Topic: 6 \n",
      "Words: 0.460*\"plan\" + 0.411*\"warn\" + 0.026*\"polic\" + 0.026*\"continu\" + 0.026*\"murder\" + 0.014*\"water\" + 0.003*\"iraq\" + 0.003*\"council\" + 0.003*\"rain\" + 0.003*\"govt\"\n",
      "Topic: 7 \n",
      "Words: 0.523*\"fund\" + 0.135*\"claim\" + 0.087*\"plan\" + 0.043*\"council\" + 0.027*\"court\" + 0.027*\"warn\" + 0.027*\"continu\" + 0.027*\"investig\" + 0.015*\"iraq\" + 0.015*\"polic\"\n",
      "Topic: 8 \n",
      "Words: 0.334*\"rain\" + 0.267*\"court\" + 0.262*\"iraq\" + 0.037*\"warn\" + 0.030*\"water\" + 0.019*\"murder\" + 0.013*\"claim\" + 0.007*\"govt\" + 0.007*\"say\" + 0.007*\"council\"\n",
      "Topic: 9 \n",
      "Words: 0.472*\"water\" + 0.147*\"plan\" + 0.072*\"court\" + 0.064*\"charg\" + 0.064*\"council\" + 0.052*\"say\" + 0.040*\"murder\" + 0.033*\"rain\" + 0.015*\"govt\" + 0.015*\"continu\"\n",
      "Training LDA with TF-IDF\n",
      "LDA results with TF-IDF\n",
      "Topic: 0 Word: 0.332*\"iraq\" + 0.318*\"council\" + 0.188*\"water\" + 0.078*\"rain\" + 0.033*\"say\" + 0.012*\"claim\" + 0.008*\"continu\" + 0.008*\"govt\" + 0.006*\"murder\" + 0.002*\"plan\"\n",
      "Topic: 1 Word: 0.484*\"record\" + 0.300*\"murder\" + 0.051*\"iraq\" + 0.046*\"warn\" + 0.031*\"say\" + 0.021*\"plan\" + 0.020*\"fund\" + 0.004*\"council\" + 0.004*\"court\" + 0.004*\"govt\"\n",
      "Topic: 2 Word: 0.410*\"warn\" + 0.360*\"claim\" + 0.068*\"council\" + 0.065*\"iraq\" + 0.018*\"fund\" + 0.017*\"govt\" + 0.010*\"say\" + 0.009*\"investig\" + 0.009*\"murder\" + 0.009*\"plan\"\n",
      "Topic: 3 Word: 0.701*\"plan\" + 0.099*\"polic\" + 0.096*\"water\" + 0.017*\"continu\" + 0.014*\"court\" + 0.011*\"record\" + 0.010*\"murder\" + 0.010*\"charg\" + 0.010*\"rain\" + 0.009*\"investig\"\n",
      "Topic: 4 Word: 0.407*\"investig\" + 0.342*\"rain\" + 0.061*\"court\" + 0.049*\"iraq\" + 0.037*\"murder\" + 0.023*\"death\" + 0.022*\"continu\" + 0.016*\"warn\" + 0.012*\"council\" + 0.003*\"govt\"\n",
      "Topic: 5 Word: 0.497*\"polic\" + 0.295*\"continu\" + 0.070*\"record\" + 0.043*\"say\" + 0.033*\"fund\" + 0.013*\"rain\" + 0.011*\"claim\" + 0.011*\"death\" + 0.003*\"iraq\" + 0.003*\"govt\"\n",
      "Topic: 6 Word: 0.375*\"court\" + 0.299*\"fund\" + 0.225*\"say\" + 0.057*\"govt\" + 0.012*\"warn\" + 0.007*\"rain\" + 0.002*\"polic\" + 0.002*\"plan\" + 0.002*\"iraq\" + 0.002*\"council\"\n",
      "Topic: 7 Word: 0.521*\"govt\" + 0.229*\"rain\" + 0.053*\"continu\" + 0.046*\"court\" + 0.032*\"water\" + 0.032*\"warn\" + 0.016*\"council\" + 0.012*\"murder\" + 0.012*\"charg\" + 0.012*\"iraq\"\n",
      "Topic: 8 Word: 0.398*\"charg\" + 0.246*\"death\" + 0.129*\"claim\" + 0.109*\"murder\" + 0.032*\"rain\" + 0.026*\"court\" + 0.013*\"iraq\" + 0.013*\"council\" + 0.010*\"polic\" + 0.003*\"plan\"\n",
      "Topic: 9 Word: 0.335*\"death\" + 0.165*\"govt\" + 0.126*\"polic\" + 0.118*\"plan\" + 0.075*\"investig\" + 0.034*\"record\" + 0.031*\"murder\" + 0.027*\"say\" + 0.026*\"council\" + 0.007*\"iraq\"\n"
     ]
    }
   ],
   "source": [
    "dic, bow, docs_tfidf = load_data(max_docs=1000)\n",
    "\n",
    "print('Training LDA with bag of words')\n",
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=10, id2word=dic, passes=2, workers=2)\n",
    "\n",
    "print('LDA results with bag of words:')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "\n",
    "print('Training LDA with TF-IDF')\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(docs_tfidf, num_topics=10, id2word=dic, passes=2, workers=4)\n",
    "\n",
    "print('LDA results with TF-IDF')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define:\n",
    "- $D$ the corpus of documents\n",
    "- $K$ the number of topics\n",
    "- $V$ the vocabulary size\n",
    "- $N$ the number of documents\n",
    "\n",
    "We define $\\phi_k$ the distribution of words for the topic $k$:\n",
    "$$\\phi_k \\sim \\text{Dir}(\\beta)$$\n",
    "It's follows a dirichlet distribution with hyperparameter $\\beta$.\n",
    "\n",
    "We define $\\theta_d$ the distribution of topics for the document $d$:\n",
    "$$\\theta_d \\sim \\text{Dir}(\\alpha)$$\n",
    "It's follows a dirichlet distribution with hyperparameter $\\alpha$.\n",
    "\n",
    "For each word $w_i$ in document $d$:\n",
    "$$z_i \\sim \\text{Categorical}(\\theta_d)$$\n",
    "$$w_i \\sim \\text{Categorical}(\\phi_{z_i})$$\n",
    "\n",
    "Let's define the matrix $\\phi \\in \\mathbb{R}^{K*V}$, with $\\phi_{ij}$ the probability of word $j$ for the distribution of the topic $i$.  \n",
    "Let's define the matrix $\\theta \\in \\mathbb{R}^{N*K}$, with $\\theta_{ij}$ the probability of topic $j$ for the distribution of the document $i$.  \n",
    "\n",
    "The model gives us the following joint distribution:\n",
    "$$p(w, z, \\phi, \\theta | \\alpha, \\beta) = p(\\phi|\\beta) p(\\theta|\\beta) p(z|\\theta) p(w|\\phi_z)$$\n",
    "\n",
    "We are trying to learn the posterior distribution:\n",
    "$$p(z, \\phi, \\theta | w, \\alpha, \\beta) = \\frac{p(w, z, \\phi, \\theta | \\alpha, \\beta)}{p(w|\\alpha,\\beta)}$$\n",
    "Unfortunatately this problem is intractable, the normalization factor cannot be computed.  \n",
    "We are using Gibbs sampling to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define :\n",
    "- $TA$, the topics assignment list, where $TA_{ij}$ is the topic assigned to the word $j$ in document $i$.\n",
    "- $WT \\in \\mathbb{N}^{K*V}$, where $WT_{ij}$ is the number of times the word $j$ is assigned to the topic $i$.\n",
    "- $DT \\in \\mathbb{N}^{N*K}$, where $DT_{ij}$ is the number of words assigned to the topic $j$ in the document $i$.\n",
    "- $T_k = \\sum_{j=1}^V WT_{kj}$ : total number of words assigned to topic $k$\n",
    "- $W_k = \\sum_{j=1}^K DT_{kj}$ : total number of words in document k\n",
    "\n",
    "The goal of the algorithm is to determine $TA$, that is the $z_i$ for every words in every documents. Both matrices $WT$ and $DT$ are directly computed from $TA$.  \n",
    "\n",
    "From $TA$, we can obtain $\\phi$ and $\\theta$:\n",
    "\n",
    "$$\\phi_{ij} = \\frac{WT_{ij} + \\beta}{T_i + V\\beta}$$\n",
    "$$\\theta_{ij} = \\frac{DT_{ij} + \\alpha}{W_i + K\\alpha}$$\n",
    "\n",
    "We can sample from the conditional distribution of $z$:\n",
    "\n",
    "$$p(z_i=k|z^{-i},w_i) \\propto (DT^{-i}_{dk} + \\alpha) \\frac{WT^{-i}_{kw_i} + \\beta}{T_k + V\\beta}$$\n",
    "\n",
    "with $z_i$ topic of word $w_i$ in document $d$. $z^{-i}$ refers to the topic when leaving word $w_i$ out of all calculations.  \n",
    "\n",
    "The algorithm iterate and sample over all $p(z_i=k|z^{-i},w_i)$, updating each time the current value of $z_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 1, 3, 4]\n",
      "[5, 6, 7, 0, 8, 2, 4]\n",
      "[1, 9, 10, 2, 11, 4]\n",
      "[12, 10, 13, 14]\n",
      "[15, 16, 17, 10]\n",
      "[18, 2, 11]\n",
      "[18, 19, 20, 21, 17, 22, 23, 24, 18]\n",
      "[25, 18, 26]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"eat turkey on turkey day holiday\",\n",
    "    \"i like to eat cake on holiday\",\n",
    "    \"turkey trot race on thanksgiving holiday\",\n",
    "    \"snail race the turtle\",\n",
    "    \"time travel space race\",\n",
    "    \"movie on thanksgiving\",\n",
    "    \"movie at air and space museum is cool movie\",\n",
    "    \"aspiring movie star\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def prepr(docs):\n",
    "    docs = [x.split(' ') for x in docs]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_dic(docs):\n",
    "    docs = [list(dict.fromkeys(x)) for  x in docs]\n",
    "    dic = {}\n",
    "    for d in docs:\n",
    "        for w in d:\n",
    "            if w not in dic:\n",
    "                dic[w] = len(dic)\n",
    "\n",
    "    return dic\n",
    "\n",
    "def build_bow(docs, dic):\n",
    "    res = []\n",
    "    for d in docs:\n",
    "        res.append([dic[w] for w in d])\n",
    "    return res\n",
    "    \n",
    "\n",
    "docs = prepr(docs)\n",
    "dic = build_dic(docs)\n",
    "bow = build_bow(docs, dic)\n",
    "for l in bow: print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Words: 0.046512*\"race\"; 0.046512*\"holiday\"; 0.046512*\"turtle\"; 0.023256*\"turkey\"; 0.046512*\"to\"; \n",
      "Topic 1\n",
      "Words: 0.046512*\"on\"; 0.046512*\"space\"; 0.046512*\"movie\"; 0.023256*\"trot\"; 0.046512*\"time\"; \n",
      "Topic 2\n",
      "Words: 0.046512*\"turkey\"; 0.046512*\"thanksgiving\"; 0.046512*\"travel\"; 0.023256*\"star\"; 0.046512*\"on\"; \n"
     ]
    }
   ],
   "source": [
    "def LDA(bow, dic, K, alpha, beta, niters=10000):\n",
    "    N = len(bow)\n",
    "    V = len(dic)\n",
    "    tk = np.zeros(K, dtype=np.int)\n",
    "    wk = np.zeros(N, dtype=np.int)\n",
    "    WT = np.zeros((K, V), dtype=np.int)\n",
    "    DT = np.zeros((N, K), dtype=np.int)\n",
    "    \n",
    "    # Initialization: random assignment of topic to words of docs\n",
    "    TA = list()\n",
    "    for i in range(N):\n",
    "        wk[i] = len(bow[i])\n",
    "        TA.append(np.random.randint(0,K, size=wk[i]))\n",
    "        \n",
    "        for j in range(wk[i]):\n",
    "            word = bow[i][j]\n",
    "            topic = TA[i][j]\n",
    "            WT[topic,word] += 1\n",
    "            DT[i, topic] += 1\n",
    "            tk[topic] += 1\n",
    "\n",
    "    \n",
    "    for it in range(niters):\n",
    "        \n",
    "        for d in range(N):\n",
    "            for i in range(wk[d]):\n",
    "                \n",
    "                wi = bow[d][i]\n",
    "                \n",
    "                zi = TA[d][i]\n",
    "                DT[d,zi]-=1\n",
    "                WT[zi,wi]-=1\n",
    "                tk[zi]-=1\n",
    "                \n",
    "                zips = np.empty(K)\n",
    "                for k in range(K):\n",
    "                    zips[k] = ((WT[k,wi] + beta) / (tk[k] + V * beta)) * (\n",
    "                        (DT[d,k] + alpha) / 1# (wk[d] + K * alpha)\n",
    "                    )\n",
    "\n",
    "                zips = zips/np.sum(zips)\n",
    "                zi = np.random.choice(K, p=zips)\n",
    "                TA[d][i] = zi\n",
    "                DT[d,zi]+=1\n",
    "                WT[zi,wi]+=1\n",
    "                tk[zi]+=1\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute probas    \n",
    "    phi = (WT + beta) / (tk.reshape(K,1) + V * beta)\n",
    "    theta = (DT + alpha) / (wk.reshape(N,1) + K * alpha)\n",
    "    return phi, theta\n",
    "    \n",
    "   \n",
    "phi, theta = LDA(bow, dic, 3, 1, 1)\n",
    "\n",
    "for k in range(len(phi)):\n",
    "    msg = 'Topic {}\\nWords: '.format(k)\n",
    "    words = list(dic.keys())\n",
    "    props = phi[k].tolist()\n",
    "    \n",
    "    props, words = zip(*sorted(zip(props, words), reverse=True))\n",
    "    for i in range(5):\n",
    "        msg += '{:.6f}*\"{}\"; '.format(proprs[i], words[i])\n",
    "    print(msg)\n",
    "    \n",
    "#print(phi)\n",
    "#print(theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
