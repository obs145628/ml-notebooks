{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor $G(X)$ takes values in a discrete set $\\mathbb{G}$. The input space is divided into a collection regions labeled according to their clasification.  \n",
    "The boundaries of these regions are called decisions boundaries, and they are linear for linear methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "Example of $K$ classes with discriminant function:\n",
    "\n",
    "$$\\delta_k(x) = \\beta_{k0} + \\beta_k^Tx$$\n",
    "The assigned class is the one with the biggest value for $\\delta_k$.  \n",
    "\n",
    "The decision boundary between class $k$ and $l$ is the set of points for wich $\\delta_k(x) = \\delta_l(x)$.  \n",
    "That is the hyperplane defined by $\\{x: (\\beta_{k0} - \\beta_{l0}) + (\\beta_k - \\beta_l)^Tx = 0 \\}$.\n",
    "\n",
    "Methods that model the posterior probabilities $P(G = k | X = x)$ are also in this class.  \n",
    "If $\\delta_k(x)$ or $P(G = k | X = x)$ is linear in $x$, then the decision boundaries will be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Example of 2 classes with posterior probabilities:\n",
    "    \n",
    "$$P(G = 1 | X = x) = \\frac{\\exp(\\beta_0 + \\beta^Tx)}{1 + \\exp(\\beta_0 + \\beta^Tx)}$$\n",
    "$$P(G = 2 | X = x) = \\frac{1}{1 + \\exp(\\beta_0 + \\beta^Tx)}$$  \n",
    "\n",
    "The decision boundary is the set of points for which the log-odds are zero:\n",
    "\n",
    "$$\\log(\\frac{p}{1-p}) = 0$$\n",
    "$$\\log \\frac{P(G = 1 | X = x)}{P(G = 2 | X = x)} = \\beta_0 + \\beta^Tx$$\n",
    "\n",
    "Thus the decision boundary is an hyperplane defined by $\\{x | \\beta_0 + \\beta^Tx = 0\\}$.  \n",
    "\n",
    "Linear logistic regression and linear discriminant analysis have linear log-odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to explicitely model linear boundaries between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression of an Indicator Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a vector $y \\in \\mathbb{R}^{K}$, with $K$ the number of classes.\n",
    "If the example belong to classs $k$, $y_j = 1_{j=k}$.  \n",
    "For a training set of size $N$, the output matrix is $Y \\in \\mathbb{R}^{N*K}$.  \n",
    "\n",
    "The parameters are fitted using any multiple outputs linear classification methods for $X$ and $Y$, eg normal equations:\n",
    "$$\\hat{B} = (X^TX)^{-1} X^TY$$.  \n",
    "\n",
    "Classify an example:\n",
    "$$\\hat{y} = x^T \\hat{B}$$\n",
    "$$\\hat{G}(x) = \\arg \\max_{k \\in \\mathbb{G}} \\hat{y}_k$$\n",
    "\n",
    "Regression gives an expectation of conditional expectation.\n",
    "$$y_k = \\mathbb{E}(y_k | X = x) = P(G = k | X = x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_toy_class(N, noise=0.001):\n",
    "    X = 2.8 * np.random.randn(N, 4)**1 + 4.67\n",
    "    v1 = 1.5*X[:, 0] + 2.3*X[:, 1] - 0.3*X[:, 2] + 4.5 + noise*np.random.randn(len(X)) \n",
    "    v2 = 1.7*X[:, 0] + 0.4*X[:, 1] + 2.3*X[:, 2] - 3.7 + noise*np.random.randn(len(X))\n",
    "    v3 = -0.6*X[:, 0] + 5.8*X[:, 1] - 1.3*X[:, 2] + 0.1 + noise*np.random.randn(len(X))\n",
    "    V = np.vstack((v1, v2, v3)).T\n",
    "    g = np.argmax(V, axis=1)\n",
    "    return X, g\n",
    "\n",
    "def label2onehot(g, nclasses):\n",
    "    Y = np.zeros((len(g), nclasses))\n",
    "    Y[np.arange(len(g)), g] = 1\n",
    "    return Y\n",
    "\n",
    "def onehot2labels(Y):\n",
    "    return np.argmax(Y, axis=1)\n",
    "\n",
    "def add_col1(X):\n",
    "    return np.append(np.ones((len(X),1)), X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.11236653073061927\n",
      "acc: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#Example with 3 classes\n",
    "\n",
    "X, g = gen_toy_class(117, noise=1e-3)\n",
    "Y = label2onehot(g, 3)\n",
    "\n",
    "X2 = add_col1(X)\n",
    "B = np.linalg.inv(X2.T @ X2) @ X2.T @ Y\n",
    "\n",
    "Y_preds = X2 @ B\n",
    "preds = onehot2labels(Y_preds)\n",
    "print('error:', np.mean((Y - Y_preds)**2))\n",
    "print('acc:', np.mean(g == preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_toy_bin(N, noise=0.001):\n",
    "    X = 2.8 * np.random.randn(100000, 4)**2 + 4.67\n",
    "    v = 1.5*X[:, 0] + 2.3*X[:, 1] - 4.7*X[:, 2] + 4.5 + noise*np.random.randn(len(X))\n",
    "    m = v.mean()\n",
    "    X = X[:N]\n",
    "    g = (v[:N] > m).astype(np.int)\n",
    "    return X, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.12269558980684189\n",
      "acc: 0.85423\n"
     ]
    }
   ],
   "source": [
    "#Binary example\n",
    "\n",
    "X, g = gen_toy_bin(117000, noise=0)\n",
    "Y = label2onehot(g, 2)\n",
    "\n",
    "X2 = add_col1(X)\n",
    "B = np.linalg.inv(X2.T @ X2) @ X2.T @ Y\n",
    "\n",
    "Y_preds = X2 @ B\n",
    "preds = onehot2labels(Y_preds)\n",
    "print('error:', np.mean((Y - Y_preds)**2))\n",
    "print('acc:', np.mean(g == preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_k$ doesn't belong like a probability. Even though $\\sum_{k \\in \\mathbb{G}} y_k = 1$, $y_k$ might be negative or greater than $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approch is to construct a target $t_k$ for each class, with $t_k$ $k$-th columns of $I_K$.\n",
    "Obervations are $y_i = t_k$ if $g_i = k$.\n",
    "We fit the least-squares criterion:\n",
    "$$\\hat{B} = \\arg \\min_{B} \\sum_{i=1}^N ||y_i - x_i^TB||^2$$\n",
    "\n",
    "Classify an example:\n",
    "$$\\hat{G}(x) = \\arg \\min_{k} ||x_i^T\\hat{B} - t_k||^2$$  \n",
    "\n",
    "Actually, this model yields exactly the same results than the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't work well when $K \\geq 3$. Because of the rigid nature of regression, classes can be masked by others.  \n",
    "A general rule is that with $K \\geq 3$ classes, polynomials terms up to degree $K - 1$ might be needed to solve them.  \n",
    "Masking usually occurs for large $K$ and small $p$.  \n",
    "Other methods like logistic regression and linear distriminant analysis doesn't suffer from masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Bayes theorem:\n",
    "    \n",
    "$$P(G = k | X = x) = \\frac{P(X = x | G = k) P(G = k)}{P(X)}$$\n",
    "$$P(G = k | X = x) = \\frac{P(X = x | G = k) P(G = k)}{\\sum_{l=1}^K P(X = x | G = l) P(G = l)}$$\n",
    "\n",
    "Let $\\pi_k$ the prior probability of class $k$: $\\pi_k = P(G = k)$.  \n",
    "Let $f_k(x)$ the class-condisional density of $X$ in class $G = k$: $P(X \\in T | G = k) = \\int_T f_k(x)dx$.\n",
    "\n",
    "Thus, the posterior probability is:\n",
    "\n",
    "$$P(G = k | X = x) = \\frac{f_k(x) \\pi_k}{\\sum_{l=1}^K f_l(x) \\pi_l}$$\n",
    "\n",
    "Each density class is represented as a multivariate Gaussian:\n",
    "\n",
    "$$f_k(x) = \\frac{\\exp(-\\frac{1}{2} (x-\\mu_k)^T \\Sigma^{-1} (x - \\mu_k) )}{\\sqrt{(2\\pi)^p |\\Sigma|}}$$\n",
    "\n",
    "with:\n",
    "- $\\Sigma \\in \\mathbb{R}^{p*p}$ covariance matrix shared by all class densities.\n",
    "- $\\mu_k \\in \\mathbb{R}^p$ the mean vector for class density $k$.\n",
    "- $|\\Sigma| = \\det(\\Sigma)$  \n",
    "\n",
    "$$\\log \\frac{P(G = k | X = x)}{P(G = k | X = x)} = \\log \\frac{\\pi_k}{\\pi_l} - \\frac{1}{2}(\\mu_k + \\mu_l)^T\\Sigma^{-1}(\\mu_k - \\mu_l) + x^T \\Sigma^{-1}(\\mu_k - \\mu_l)$$.\n",
    "\n",
    "The log-odds function is linear in $x$, so the decision boundaries are linear.\n",
    "\n",
    "The decision rule can be described with linear descriminant functions:\n",
    "\n",
    "$$\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}\\mu_kt^T\\Sigma^{-1}\\mu_k + \\log \\pi_k$$\n",
    "$$G(x) = \\arg \\max_k \\delta_k(x)$$  \n",
    "\n",
    "The parameters are estimated from the training data:\n",
    "$$\\hat{\\pi}_k = \\frac{N_k}{N}$$\n",
    "$$\\hat{\\mu}_k = \\frac{\\sum_{g_i = k} x_i}{N_k}$$\n",
    "$$\\hat{\\Sigma} = \\frac{\\sum_{k=1}^K \\sum_{g_i = k} (x_i - \\mu_k)(x_i - \\mu_k)^T}{N - K}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9889743589743589\n"
     ]
    }
   ],
   "source": [
    "#Example with 3 classes\n",
    "\n",
    "def lda(X, g, K):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    pis = []\n",
    "    mus = []\n",
    "    cov = np.zeros((p, p))\n",
    "    \n",
    "    for k in range(K):\n",
    "        nk = np.sum(g == k)        \n",
    "        pi = nk / N\n",
    "        mu = np.zeros((p,))\n",
    "        for i in range(N):\n",
    "            if g[i] == k:\n",
    "                mu += X[i]\n",
    "        mu /= nk\n",
    "        \n",
    "        pis.append(pi)\n",
    "        mus.append(mu)\n",
    "        \n",
    "    \n",
    "    for i in range(N):\n",
    "        cov += np.outer(X[i] - mus[g[i]], X[i] - mus[g[i]])\n",
    "    cov /= (N - K)\n",
    "    \n",
    "    \n",
    "    icov = np.linalg.inv(cov)\n",
    "    B = np.empty((p, K))\n",
    "    intercept = np.empty((K,))\n",
    "    for k in range(K):\n",
    "        B[:, k] = icov @ mus[k]\n",
    "        intercept[k] = - 1/2 * (mus[k] @ icov @ mus[k]) + np.log(pis[k])\n",
    "    \n",
    "    return B, intercept\n",
    "\n",
    "\n",
    "X, g = gen_toy_class(11700, noise=1e-5)\n",
    "B, intercept = lda(X, g, 3)\n",
    "\n",
    "Y_preds = X @ B + intercept\n",
    "preds = np.argmax(Y_preds, axis=1)\n",
    "\n",
    "print('acc:', np.mean(g == preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "If each $f_k(x)$ as it's own covariance marix $\\Sigma_k$, the logs-odd function and the distriminant functions became quadratic:\n",
    "\n",
    "$$\\delta_l(x) = - \\frac{1}{2} \\log | \\Sigma_k| - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k$$  \n",
    "\n",
    "When $p$ large, QDA causes a dramatic increase in the number of parameters.  \n",
    "There is little difference between LDA applied to dataset augmented with polynomials of degree $2$, and QDA.  \n",
    "\n",
    "For QDA, the estimates of $\\pi_k$ and $u_k$ stays the same, and the estimate of $\\Sigma_k$ is:\n",
    "\n",
    "$$\\hat{\\Sigma}_k = \\frac{\\sum_{g_i = k} (x_i - \\mu_k)(x_i - \\mu_k)^T}{N_k - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9684615384615385\n"
     ]
    }
   ],
   "source": [
    "#Example with 3 classes\n",
    "\n",
    "def qda(X, g, K):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    pis = []\n",
    "    mus = []\n",
    "    dcovs = []\n",
    "    icovs = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        nk = np.sum(g == k)        \n",
    "        pi = nk / N\n",
    "        \n",
    "        mu = np.zeros((p,))\n",
    "        for i in range(N):\n",
    "            if g[i] == k:\n",
    "                mu += X[i]\n",
    "        mu /= nk\n",
    "        \n",
    "        cov = np.zeros((p, p))\n",
    "        for i in range(N):\n",
    "            if g[i] == k:\n",
    "                cov += np.outer(X[i] - mu, X[i] - mu)\n",
    "        cov /= (nk - 1)\n",
    "        \n",
    "        pis.append(pi)\n",
    "        mus.append(mu)\n",
    "        dcovs.append(-1/2 * np.log(np.linalg.det(cov)))\n",
    "        icovs.append(np.linalg.inv(cov))\n",
    "\n",
    "    \n",
    "    return pis, mus, dcovs, icovs\n",
    "\n",
    "def qda_pred(x, pis, mus, dcovs, icovs):\n",
    "    K = len(pis)\n",
    "    y = np.empty((K,))\n",
    "    \n",
    "    for k in range(K):\n",
    "        qt = -1/2 * (x - mus[k]) @ icovs[k] @ (x - mus[k])\n",
    "        y[k] = dcovs[k] + qt + np.log(pis[k])\n",
    "    \n",
    "    return np.argmax(y)\n",
    "\n",
    "\n",
    "X, g = gen_toy_class(11700, noise=1e-5)\n",
    "pis, mus, dcovs, icovs = qda(X, g, 3)\n",
    "preds = np.empty((len(X),))\n",
    "for i in range(len(X)):\n",
    "    preds[i] = qda_pred(X[i], pis, mus, dcovs, icovs)\n",
    "\n",
    "print('acc:', np.mean(g == preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Discriminant Analysis\n",
    "\n",
    "RDA is a oompromise between LDA and QDA, allowing to shrink the separate covariances of QDA toward a common covariance as in LDA.\n",
    "\n",
    "$$\\hat{\\Sigma}_k(\\alpha) = \\alpha \\hat{\\Sigma}_k + (1 - \\alpha) \\hat{\\Sigma}$$\n",
    "\n",
    "with $\\alpha$ hyperparameter that allows a continuum of models between LDA and QDA.  \n",
    "\n",
    "Another modificatio allows $\\hat{\\Sigma}$ to be shunk toward a scalar covariance:\n",
    "\n",
    "$$\\hat{\\Sigma}(\\gamma) = \\gamma \\hat{\\Sigma} + (1 - \\gamma) \\hat{\\sigma}^2I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations for LDA\n",
    "\n",
    "Computations can be simplified by diagonalization of $\\Sigma$.  \n",
    "For QDA:\n",
    "$$\\hat{\\Sigma}_k = U_k D_k U^T_k$$\n",
    "$$(x - \\hat{\\mu}_k)^T\\Sigma^{-1}_k (x - \\hat{\\mu}_k) = [U_k^T(x - \\hat{\\mu}_k)]^T D^{-1}_k [U_k^T(x - \\hat{\\mu}_k)]$$\n",
    "$$log |\\Sigma_k| = \\sum_l log d_{kl}$$  \n",
    "\n",
    "For LDA, we can project the data into a space where the common covariance estimate is $I$:\n",
    "$$\\hat{\\Sigma} = UDU^T$$\n",
    "$$X^* \\leftarrow X D^{-\\frac{1}{2}}U^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced-Rank LDA\n",
    "\n",
    "The $K$ centroids in $p$-dimensional input space lie in an affine subspace of dimension $\\leq K - 1$. We might just as well project $X*$ onto this centroid-spanning subpace $H_{K-1}$.  \n",
    "\n",
    "We can also project $X*$ into a subspace $H_L$ for $L \\leq K$, where the projected centroids were spread out as much as possible in terms of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "- Compute matrix of class centroids $M \\in \\mathbb{R}^{K*p}$\n",
    "$$M_k = \\frac{1}{N_k} \\sum_{g_i = k} x_i$$\n",
    "\n",
    "- Compute within class covariance matrix $W \\in \\mathbb{R}^{p*p}$\n",
    "\n",
    "$$W = \\sum_{k=1}^K \\sum_{g_i = k} (x_i - M_k) (x_i - M_k)^T$$\n",
    "\n",
    "- Compute $M^* = MW^{-\\frac{1}{2}}$, $M^* \\in \\mathbb{R}^{K*p}$\n",
    "\n",
    "$$P_W^T W P_W = D_W$$\n",
    "$$W^{-\\frac{1}{2}} = P_W D^{-\\frac{1}{2}}P_W^T$$\n",
    "\n",
    "- Compute between class covariance matrix $B^*$ of $M^*$, $B^* \\in \\mathbb{R}^{p*p}$\n",
    "$$\\mu = \\frac{1}{K} \\sum_{k=1}^K M_k^*$$\n",
    "\n",
    "$$B^* = \\sum_{k=1}^K N_k (M^*_k - \\mu) (M^*_k - \\mu)^T$$\n",
    "\n",
    "- Diagionalize $B^*$: $B^* = V^* D_B V^{*T}$\n",
    "\n",
    "- Project the data:\n",
    "\n",
    "$$v_l = W^{-\\frac{1}{2}}v_l^*, \\space v_l^* \\in \\mathbb{R}^p$$\n",
    "$$z_l = Xv_l, \\space z_l \\in \\mathbb{R}^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Method\n",
    "\n",
    "This is a different method, that gives the same results.\n",
    "\n",
    "Fisher LDA looks for a projection $Z = a^TX$ such that the between-class variance is maximized relative to the within-class variance.  \n",
    "\n",
    "Let $B$ and $W$ respectively the between-class and the within-class variance of $X$.  Note than $T = B + W$ with $T$ the covariance matrix of $X$, ingoring class information.  \n",
    "The between-class and within-class variance of $Z$ are respectively $a^TBa$ and $a^TWa$.  \n",
    "The objective is:\n",
    "$$\\max_a \\frac{a^TBa}{a^tWa}$$\n",
    "\n",
    "$a$ is the eigeinvector corresponding to the largest eigeinvalue of $W^{-1}B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "- Compute matrix of class centroids $M \\in \\mathbb{R}^{K*p}$\n",
    "$$M_k = \\frac{1}{N_k} \\sum_{g_i = k} x_i$$\n",
    "\n",
    "- Compute within class covariance matrix $W \\in \\mathbb{R}^{p*p}$\n",
    "\n",
    "$$W = \\sum_{k=1}^K \\sum_{g_i = k} (x_i - M_k) (x_i - M_k)^T$$\n",
    "\n",
    "- Compute between class covariance matrix $B \\in \\mathbb{R}^{p*p}$\n",
    "$$\\mu = \\frac{1}{K} \\sum_{k=1}^K M_k$$\n",
    "\n",
    "$$B = \\sum_{k=1}^K N_k (M_k - \\mu) (M_k - \\mu)^T$$\n",
    "\n",
    "- Diagionalize $W^{-1}B$: \n",
    "$$W^{-1}B = V D V^T$$\n",
    "\n",
    "- Project the data:\n",
    "$$z_l = Xv_l, \\space z_l \\in \\mathbb{R}^N$$\n",
    "\n",
    "$$Z = XV_L, \\space Z \\in \\mathbb{R}^{N*L}$$\n",
    "\n",
    "with $V_L$ columns the $L$ eigenvectors corresponding to the largest eigeinvalues of $W^{-1}B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8701709401709402\n"
     ]
    }
   ],
   "source": [
    "N = 11700\n",
    "K = 3\n",
    "X, g = gen_toy_class(N, noise=1e-5)\n",
    "p = X.shape[1]\n",
    "\n",
    "#1) Compute class centroids M\n",
    "M = np.zeros((K, p))\n",
    "for k in range(K):\n",
    "    nk = np.sum(g == k)        \n",
    "    for i in range(N):\n",
    "        if g[i] == k:\n",
    "            M[k] += X[i]\n",
    "    M[k] /= nk\n",
    "    \n",
    "#2) Compute within-class covariance W\n",
    "W = np.zeros((p, p))\n",
    "for i in range(N):\n",
    "    W += np.outer(X[i] - M[g[i]], X[i] - M[g[i]])\n",
    "\n",
    "#3) Compute between class covariance B\n",
    "mu = np.mean(M, axis=0)\n",
    "B = np.zeros((p, p))\n",
    "for k in range(K):\n",
    "    nk = np.sum(g == k) \n",
    "    B += nk * np.outer(M[k] - mu, M[k] - mu)\n",
    "    \n",
    "#4) Diagonalize W^-1B\n",
    "d, V = np.linalg.eig(np.linalg.inv(W) @ B)\n",
    "\n",
    "#5) Project the data\n",
    "Vr = V[:, :2]\n",
    "Z = X @ Vr\n",
    "\n",
    "#6) Make predictions\n",
    "MZ = M @ Vr\n",
    "preds = np.empty((N,)).astype(np.int)\n",
    "for i in range(N):\n",
    "    \n",
    "    min_k = None\n",
    "    min_dist = float('inf')\n",
    "    for k in range(K):\n",
    "        d = (Z[i] - MZ[k]) @ (Z[i] - MZ[k])\n",
    "        if d < min_dist:\n",
    "            min_k = k\n",
    "            min_dist = d\n",
    "    \n",
    "    preds[i] = min_k\n",
    "    \n",
    "print('acc:', np.mean(g == preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined by the log-odds of the posterior probabilities.\n",
    "\n",
    "$$\\log \\frac{P(G = k | X = x)}{P(G = K | X = x)} = \\beta_{k0} + \\beta_{k}^T x, \\space k=1\\text{...}K-1$$\n",
    "\n",
    "It can be deduced that:\n",
    "\n",
    "$$P(G = k | X = x) = \\frac{\\beta_{k0} + \\beta_{k}^T x}{1 + \\sum_{l=1}^{K-1} \\beta_{l0} + \\beta_{l}^T x}, \\space k=1\\text{...}K-1$$\n",
    "$$P(G = K | X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} \\beta_{l0} + \\beta_{l}^T x}$$\n",
    "\n",
    "The log-likelihood for $N$ observations is:\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^N \\log P(G=g_i | X = x_i; \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the cases with $K = 2$, with a response $y_i$ with $y_i = 1$ when $g_i = 1$ and $y_1 = 0$ when $g_i = 2$\n",
    "\n",
    "$$l(\\beta) = \\sum_{i=1}^N y_i \\log p(x_i) + (1 - y_i) \\log(1 - p(x_i))$$\n",
    "\n",
    "$$\\text{with } p(x_i) = P(G=1|X=x) = \\frac{\\exp(\\beta^Tx)}{1 + \\exp(\\beta^Tx)}$$\n",
    "\n",
    "$$l(\\beta) = \\sum_{i=1}^N y_i \\beta^Tx_i - \\log(1 + \\exp(\\beta^Tx_i))$$\n",
    "\n",
    "In order to maximize the log-likelihood, we solve:\n",
    "\n",
    "$$\\frac{\\partial l(\\beta)}{\\partial \\beta} = 0$$\n",
    "$$\\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^N x_i(y_i - p(x_i))$$\n",
    "\n",
    "This can be solved using the Newton-Raphson algorithm, with second-derivates:\n",
    "\n",
    "$$\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T} = - \\sum_{i=1}^N x_ix_i^T p(x_i)(1-p(x_i))$$\n",
    "\n",
    "The update is:\n",
    "\n",
    "$$\\beta \\leftarrow \\beta - (\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T})^{-1} \\frac{l(\\beta)}{\\partial \\beta}$$  \n",
    "\n",
    "It can be rewritten in matrix form as:\n",
    "\n",
    "$$\\frac{\\partial l(\\beta)}{\\partial \\beta} = X^T(y-p)$$\n",
    "$$\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T} = - X^TWX$$\n",
    "\n",
    "with:\n",
    "- $X \\in \\mathbb{R}^{N * p}$ the matrix of features\n",
    "- $p \\in \\mathbb{R}^N$ the vector of predictions\n",
    "- $y \\in \\mathbb{R}^N$ the vector of labels\n",
    "- $W \\in \\mathbb{R}^{N*N}$ diagonal matrix: $W_{ii} = p_i(1-p_i)$ \n",
    "\n",
    "The update became\n",
    "\n",
    "$$\\beta \\leftarrow \\beta + (X^TWX)^{-1}X^T (y - p)$$\n",
    "$$\\beta \\leftarrow (X^TWX)^{-1}X^T Wz$$\n",
    "$$\\text{with } z = X \\beta + W^{-1} (y-p)$$  \n",
    "\n",
    "So the update is equivalent to solving a weigthed least square problem with output $z$:\n",
    "\n",
    "$$\\beta \\leftarrow \\arg \\min_\\beta (z - X\\beta)^TW(z - X\\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -81.0982201255136\n",
      "loss: -45.425338172882384\n",
      "loss: -30.34245722892736\n",
      "loss: -20.693186675835356\n",
      "loss: -15.169604917644264\n",
      "beta: [  3.06500292   5.19078785 -10.35015818  -1.07264767]\n",
      "acc: 0.9658119658119658\n"
     ]
    }
   ],
   "source": [
    "X, y = gen_toy_bin(117, noise=1e-5)\n",
    "\n",
    "\n",
    "def logreg(X, y):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    #works a lot better when init at 0\n",
    "    beta = np.zeros((p,))\n",
    "    #beta = np.random.randn(p)\n",
    "    \n",
    "    for i in range(5):\n",
    "        p = np.exp(X @ beta) / (1 + np.exp(X @ beta))\n",
    "        l = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "        W = np.diag(p * (1-p))\n",
    "        \n",
    "        #IRLS update\n",
    "        z = X @ beta + np.linalg.inv(W) @ (y - p)\n",
    "        beta = np.linalg.inv(X.T @ W @ X) @ X.T @ W @ z\n",
    "\n",
    "        '''\n",
    "        #newton update\n",
    "        g = X.T @ (y - p)\n",
    "        H = - X.T @ W @ X\n",
    "        beta = beta - np.linalg.inv(H) @ g\n",
    "        '''\n",
    "        \n",
    "        print('loss:', l)\n",
    "    \n",
    "    \n",
    "    return beta\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "Xs = np.std(X, axis=0, keepdims=True)\n",
    "X2 = (X - Xc) / Xs\n",
    "beta = logreg(X2, y)\n",
    "\n",
    "y_hat = np.exp(X2 @ beta) / (1 + np.exp(X2 @ beta))\n",
    "preds = np.round(y_hat).astype(np.int)\n",
    "print('beta:', beta)\n",
    "print('acc:', np.mean(y == preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
