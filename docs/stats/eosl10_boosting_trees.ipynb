{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost.M1\n",
    "\n",
    "The idea of boosting is to combine many weak classifiers intro a strong one.  \n",
    "A weak classifier is one slight better than random guessing.\n",
    "\n",
    "Let's define the error rate:\n",
    "\n",
    "$$\\bar{\\text{err}} = \\frac{1}{N} \\sum_{i=1}^N I(y_i \\neq G(x_i))$$\n",
    "Adaboost combine $M$ weak classifiers:\n",
    "$$G(x) = \\text{sign} \\left( \\sum_{m=1}^M \\alpha_m G_m(x) \\right) $$\n",
    "\n",
    "$\\alpha$ is the contribution vector of the classifiers, they are learned, such that a better model as an higher $\\alpha_m$.  \n",
    "\n",
    "All classifiers are trained one by one, but with weighted examples $w_i$. At first all examples have the same weight, then at each iteration the weight of misclassified examples increase, and the others decrease.  \n",
    "\n",
    "Algorithm $10.1$ page $339$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "train acc: 0.9102296450939458\n",
      "test acc: 0.8916666666666667\n",
      "train acc: 0.9617258176757133\n",
      "test acc: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from copy import deepcopy\n",
    "\n",
    "X, y = load_digits().data, load_digits().target\n",
    "y = (y < 5).astype(np.int32)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == logreg.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == logreg.predict(X_test)))\n",
    "\n",
    "\n",
    "class AdaboostM1:\n",
    "    \n",
    "    def __init__(self, model, M):\n",
    "        self.model = model\n",
    "        self.M = M\n",
    "        self.mods = []\n",
    "        self.alpha = np.empty(M)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N = len(X)\n",
    "        w = np.ones(N) / N\n",
    "        \n",
    "        \n",
    "        for m in range(self.M):\n",
    "            clf = deepcopy(self.model)\n",
    "            clf.fit(X, y, w)\n",
    "            self.mods.append(clf)\n",
    "            preds = clf.predict(X)\n",
    "            err = np.sum(w * (preds != y)) / np.sum(w)\n",
    "            self.alpha[m] = np.log((1 - err) / err)\n",
    "            w = w * np.exp(self.alpha[m] * (preds != y))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(len(X))\n",
    "        for m in range(self.M):\n",
    "            preds += self.alpha[m] * self.mods[m].predict(X)\n",
    "        preds = np.round(preds / np.sum(self.alpha)).astype(np.int32)\n",
    "        return preds\n",
    "            \n",
    "        \n",
    " \n",
    "mod = DecisionTreeClassifier(max_depth=1)\n",
    "clf = AdaboostM1(mod, 500)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == clf.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Fits an Additive Model\n",
    "\n",
    "Boosting is just a special case of additive models:\n",
    "    \n",
    "$$f(x) = \\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$$\n",
    "\n",
    "$b(x;\\gamma_m)$ are simple functions of argument $x$ and parameters $\\gamma_m$. For boosting, each basis function is a weak classifier.\n",
    "\n",
    "These models are trained by fitting $\\beta$ and $\\gamma$ minimizing a loss function over the dataset:\n",
    "\n",
    "$$\\min_{\\{\\beta_m, \\gamma_m\\}_1^M} \\sum_{i=1}^N L\\left(y_i, \\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\right)$$\n",
    "\n",
    "for any loss function $L(y, f(x))$ such as squared-error or negative log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Stagewise Additive Modeling\n",
    "\n",
    "This algorithm find an approximate solution by solving a simpler problem.   It starts with an empty model, and add a new basic function one at a time,  fitting it without modyfing the parameters of the previous ones.\n",
    "\n",
    "The problem is a lot simpler to optimize:\n",
    "\n",
    "$$\\min_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta_m b(x_i;\\gamma_m))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Loss and Adaboost\n",
    "\n",
    "AdaBoost.M1 is equivalent to forward stagewise additive modeling using the exponential loss:\n",
    "\n",
    "$$L(y, f(x)) = \\exp (-yf(x))$$\n",
    "\n",
    "The problem is:\n",
    "\n",
    "$$(\\beta_m, G_m) = \\min_{\\beta, G} \\sum_{i=1}^N \\exp [-y_i(f_{m-1}(x_i) + \\beta G(x_i))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Exponential Loss ?\n",
    "\n",
    "The principal attraction is computational: additive modeling with computational loss leads to a simple modular reweighting algorithm.\n",
    "\n",
    "$$f^*(x) = \\arg \\min_{f(x)} E_{y|x} \\exp(-yf(x)) = \\frac{1}{2} \\log \\frac{P(y=1|x)}{P(y=-1|x)}$$\n",
    "\n",
    "Thus AdaBoost estimates one-half of the log-odds, that justifies using the  sign operator.  \n",
    "\n",
    "Another loss is the deviance loss:\n",
    "\n",
    "$$l(y, f(x)) = \\log (1 + e^{-2yf(x)})$$\n",
    "\n",
    "At the population level, using either criterion leads to the same solution, but this is not true for finite datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Robustness\n",
    "\n",
    "## Robust Loss functions for classification\n",
    "\n",
    "deviance and exponential loss are both monotone decreasing functions of the margin yf(x).  \n",
    "With $G(x) = \\text{sign}(f(x))$, observations with positive margin are classified corretly, and those with negative magin are misclassified.  \n",
    "Any loss criterion should penalize negative margin more heavily than positive ones.  \n",
    "\n",
    "The difference between deviance and exponential loss is how much they penalize negative margins. The penalty for deviance increase linearly, where the one for exponential loss increase exponentially. In noisy settings, with misclassifications in the training data, the deviance gives better results.  \n",
    "\n",
    "Mean Squared error increases quadratically when $yf(x) > 1$, therefore increasing error for correctly classified examples with increasing certainty. Thus MSE is a terrible choice of loss function.  \n",
    "\n",
    "The problem generalize to K-class classification:\n",
    "$$G(x) = \\arg \\max_{k} p_k(x)$$\n",
    "with $p_k(x)$ the probability that $x$ belongs to class $k$:\n",
    "$$p_k(x) = \\frac{e^{f_k(x)}}{\\sum_{l=1}^K e^{f_l(x)}}$$\n",
    "\n",
    "We can use the K-class multinomial deviance loss function:\n",
    "\n",
    "$$L(y, p(x)) = \\sum_{k=1}^K I(y = k) \\log p_k(x)$$\n",
    "\n",
    "## Robust Loss functions for regression\n",
    "\n",
    "For regression, both the squared error: $L(y, f(x)) = (y - f(x))^2$ and absolute loss $L(y, f(x)) = |y - f(x)|$ leads to the same populations results, but vary for finite datasets.  \n",
    "Squared error loss places much empahish on obersation with large residuals, which if far less robust for outliers. Absolute loss performs much better in these situations.  \n",
    "\n",
    "Another solution to resist outliers is the Huber loss:\n",
    "$$\n",
    "L(y, f(x)) = \n",
    "\\begin{cases}\n",
    "    (y - f(x))^2 & \\text{if } |y - f(x)| \\leq \\delta \\\\\n",
    "    2 \\delta |y - f(x)| - \\delta^2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Trees\n",
    "\n",
    "A tree can be expressed as:\n",
    "\n",
    "$$T(x;\\theta) = \\sum_{j=1}^J \\gamma_j I(x \\in R_j)$$\n",
    "\n",
    "The parameters are found by minimizing the empirical risk:\n",
    "\n",
    "$$\\theta = \\arg \\min_\\theta \\sum_{j=1}^J \\sum_{x_i \\in R_j} L(y_i, \\gamma_j)$$  \n",
    "\n",
    "The boosted tree model is a sum of such trees:\n",
    "$$f_M(x) = \\sum_{m=1}^M T(x;\\theta_m)$$\n",
    "\n",
    "With a forward stagewise procedure, one must solve at each step:\n",
    "$$\\hat{\\theta_m} = \\arg \\min_{\\theta_m} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i, \\theta_m))$$\n",
    "For MSE, we simply need to fit a new regression tree with the residual errors.  \n",
    "\n",
    "For binary classificaton with exponential loss, we get the following criterion for each tree:\n",
    "$$\\hat{\\theta_m} = \\arg \\min_{\\theta} \\sum_{i=1}^N w_i^{(m)} \\exp (-y_i T(x_i;\\theta_m))$$\n",
    "This criterion can be implemented by updating the criterion of splitting for the classical tree growing algorithms.  \n",
    "\n",
    "Using other loss such as the absolute error, the Huber Loss, or the deviance gives most robust models, but there is no simple algorithms to optimize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization via Gradient Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
