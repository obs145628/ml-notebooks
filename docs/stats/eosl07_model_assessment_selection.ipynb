{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias, Variance, and Model Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of quantitative reponse. We have a target variable $Y$, a vector of inputs $X$, and a prediction model $\\hat{f}(X)$ estimated from the training set $\\mathcal{T}$.  \n",
    "The loss function measure the error between $Y$ and $\\hat{f}(X)$:\n",
    "- squared error: $L(Y, \\hat{f}(X) = (Y - \\hat{f}(X))^2$\n",
    "- abslute error: $L(Y, \\hat{f}(X) = |Y - \\hat{f}(X)|$\n",
    "\n",
    "The test error, also called generalization error, is the prediction error over an independant test sample:\n",
    "$$\\text{Err}_\\mathcal{T} = E[L(Y,\\hat{f}(X))|\\mathcal{T}]$$\n",
    "\n",
    "The training set $\\mathcal{T}$ is fixed, and the this is the error for this specific training set.  \n",
    "\n",
    "The expected prediction error (or expected test error) is:\n",
    "$$\\text{E} = E[L(Y, \\hat{f}(X)] = E[\\text{Err}_\\mathcal{T}]$$\n",
    "\n",
    "This expectation averages over all sources of randomness, included the randomness in the used training dataset $\\mathcal{T}$.  \n",
    "\n",
    "The training error is the average loss over the training sample:\n",
    "$$\\bar{\\text{err}} = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{f}(x_i))$$\n",
    "\n",
    "As the model complexity increases, the bias decreases but the variance increases. There is an intermediate model complexity that gives minimum expected test error.  \n",
    "\n",
    "The training error is not a good estimate of the test error, it decreases to 0 as we increases the model complexity, giving us o model that overfit the training data and generalizes poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story is similar for a qualitative response $G$ taking on $K$ values.  We usually model the probabilities $p_k(X) = P(G=K|X)$, or some other monotone transformations $f_k(X)$.  \n",
    "The prediction is $\\hat{G}(X) = \\arg \\max_k \\hat{p}_k(X)$.  \n",
    "The typical loss functions are:\n",
    "\n",
    "- 0-1 loss: $:L(G, \\hat{G}(X)) = I(G \\neq \\hat{G}(X)$\n",
    "- negative log likelihhod: $L(G, \\hat{p}(X)) = -2\\log \\hat{p}_G(X)$  \n",
    "\n",
    "We will use $Y$ and $f(X)$ to represent all situations, transformations to other situations (qualitative reponse) is obvious.  \n",
    "We describe methods to estimate the expected test error of a model.  \n",
    "Our model will have tunning parameters $\\alpha$ that varies the model complexity, and we look for the $\\alpha$ that manimizes the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 separate goals:\n",
    "- Model selection: estimate the performance of different models in order to choose the best one.\n",
    "- Model assessment: estimate the prediction error of the chosen model.  \n",
    "\n",
    "If we have a lot of data, the best approach is to divide the training set in 3 parts:\n",
    "- The training set, used to fit the models.\n",
    "- The validation set, used to estimate prediction error for model selection.\n",
    "- The test set, used for assessment of the prediction error of the choosen model.  \n",
    "\n",
    "The test set shouldn't be used for model selection, otherwhise the test set error of the choosen model will underestimate the true test error.  \n",
    "A typical split it 50\\% training, 25\\% validation, 25\\% test.  \n",
    "The methods in this chapter are for situations where there is insufficient data to perform this 3-parts split. They give an approximation of the validation set analytically, or by efficient sample re-use.  \n",
    "Beside for model selection, they can give us a reliable estimate of test error for the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y = f(X) + \\epsilon$, with $E(\\epsilon)=0$ and $\\text{Var}(\\epsilon)=\\sigma_\\epsilon^2$.  \n",
    "The expected prediction error of a regression fit $\\hat{f}(X)$ at an input point $x_0$ is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Err}(x_0) & = E[(Y-\\hat{f}(x_0))^2|X=x_0]\\\\\n",
    "& = \\sigma_\\epsilon^2 + [E \\hat{f}(x_0) - f(x_0)]^2 + E[\\hat{f}(x_0) - E \\hat{f}(x_0)]^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This equation can be split in 3 therms:\n",
    "- the irreductible error, it's the variance of the target, cannot be avoided: $\\sigma_\\epsilon^2$\n",
    "- the squared bias: $[E \\hat{f}(x_0) - f(x_0)]^2$\n",
    "- the variance of $\\hat{f}(x_0)$: $E[\\hat{f}(x_0) - E \\hat{f}(x_0)]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors\n",
    "\n",
    "$$\\text{Err}(x_0) = \\sigma^e_\\epsilon + (f(x_0) - \\frac{1}{k} \\sum_{l=1}^k f(x_l))^2 + \\frac{\\sigma^2_\\epsilon}{k}$$\n",
    "\n",
    "As the number of neighbors $k$ increases, the model complexity decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression\n",
    "\n",
    "$$\\text{Err}(x_0) = \\sigma_\\epsilon^2 + [f(x_0) - E \\hat{f}_p(x_0)]^2 + ||h(x_0)||^2\\sigma_\\epsilon^2$$\n",
    "\n",
    "$$\\text{with } h(x_0) = X(X^TX)^{-1}x_0$$\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\text{Err}(x_i) = \\sigma_\\epsilon^2 + \\frac{1}{N} \\sum_{i=1}^N [f(x_i) - E \\hat{f}_p(x_i)]^2 + \\frac{p}{N} \\sigma_\\epsilon^2$$\n",
    "\n",
    "As the number of parameters $p$ increase, the model complexity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge regression\n",
    "\n",
    "Identical to expression for linear regression, except for $h(x_0)$:\n",
    "$$h(x_0) = X(X^TX + \\alpha I)^{-1}x_0$$.\n",
    "\n",
    "Linear model as a bias of 0, contrary to ridge, wich has a positive bias. Bit it has lower variance than a linear model.  \n",
    "It's worthwhile when the decrease in variance exceeds the increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimism of the training error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set $\\mathcal{T} = \\{(x_1,y_1),(x_2,y_2),\\text{...},(x_N,y_N)\\}$.  \n",
    "The generalization error of a model $\\hat{f}$ is:\n",
    "$$\\text{Err}_\\mathcal{T} = E_{X^0,Y^0}[L(Y^0,\\hat{f}(X^0))|\\mathcal{T}]$$\n",
    "\n",
    "with $(X^0,Y^0)$ a new point draw from the data distribution $F$.  \n",
    "\n",
    "The expected error is:\n",
    "$$\\text{Err} = E_\\mathcal{T}E_{X^0,Y^0}[L(Y^0,\\hat{f}(X^0))|\\mathcal{T}]$$\n",
    "\n",
    "Most methods try to estimate $\\text{Err}$.  \n",
    "\n",
    "The training error is:\n",
    "$$\\bar{\\text{err}} = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{f}(x_i))$$\n",
    "\n",
    "The training error will be an overly optimistic estimate of $\\text{Err}_\\mathcal{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the in-sample error:\n",
    "\n",
    "$$\\text{Err}_\\text{in} = \\frac{1}{N} \\sum_{i=1}^N E_{Y^0}[L(Y_i^0, \\hat{f}(x_i))|\\mathcal{T}]$$\n",
    "\n",
    "$Y^0$ indicates that we oberse $N$ new reponse values for each training point $x_i$\n",
    "\n",
    "Let define the optimis as:\n",
    "$$\\text{op} \\equiv \\text{Err}_\\text{in} - \\bar{\\text{err}}$$\n",
    "\n",
    "We can define the average optimist, it's expecation over all training sets:\n",
    "$$\\omega \\equiv E_Y[\\text{op}]$$\n",
    "\n",
    "For most loss functons, it's expression generally is:\n",
    "$$\\omega = \\frac{2}{N} \\sum_{i=1}^N \\text{Cov}(\\hat{y_i}, y_i)$$\n",
    "\n",
    "Thus the amount by which the training error underestimates the true error depends on how strongly $y_i$ affcts its own prediction. The harder we fit the data, the greater the optimism will be.  \n",
    "\n",
    "We get the relation:\n",
    "\n",
    "$$E_y[\\text{Err}_\\text{in}] = E_y[\\bar{\\text{err}}] + \\frac{2}{N} \\sum_{i=1}^N \\text{Cov}(\\hat{y_i}, y_i)$$\n",
    "\n",
    "If the model is linear with $d$ parameters, the optimism expression simplifies:\n",
    "$$\\sum_{i=1}^N \\text{Cov}(\\hat{y_i}, y_i) = d\\sigma_\\epsilon^2$$\n",
    "\n",
    "Thus we have:\n",
    "$$E_y[\\text{Err}_\\text{in}] = E_y[\\bar{\\text{err}}] + 2 \\frac{d}{N} \\sigma^2_\\epsilon$$\n",
    "\n",
    "The optimism increases with the number $d$ of parameters, and decrease when the training size $N$ increase.  \n",
    "\n",
    "We can estimate the prediction error by estimating the optimum and add it to the training error.  \n",
    "In-sample error is not usually of direct interest because future $X$ are not likely to coincide with the training set, but it's convenient for model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimates of In-Sample Prediction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-saple estimate form is:\n",
    "\n",
    "$$\\hat{\\text{Err}_\\text{in}} = \\bar{\\text{err}} + \\hat{\\omega}$$\n",
    "\n",
    "For linear models, we get the $C_p$ statistic:\n",
    "$$C_p = \\bar{err} + 2 \\frac{d}{N}\\hat{\\sigma}_\\epsilon^2$$\n",
    "\n",
    "$\\hat{\\sigma}_\\epsilon^2$ is an estimate of the noise variance.  \n",
    "\n",
    "The Akaike Information Critetion (AIC) is a similar, but more generally applied estimate of $\\text{Err}_\\text{in}$, when $N \\to \\infty$:\n",
    "\n",
    "$$-2E[\\log P_\\hat{\\theta}(Y)] \\approx -\\frac{2}{N} E[\\text{loglik}] + 2 \\frac{d}{N}$$\n",
    "\n",
    "$$\\text{with logligk } = \\sum_{i=1}^N \\log P_\\hat{\\theta}(y_i)$$\n",
    "\n",
    "For model selection, given a set models $f_\\alpha(x)$, with $\\alpha$ tunning parameter, whe chose the $\\alpha$ that gives the minimum AIC:\n",
    "$$\\text{AIC}(\\alpha) = \\bar{\\text{err}}(\\alpha) + 2 \\frac{d(\\alpha)}{N} \\sigma_\\epsilon^2$$\n",
    "\n",
    "with $\\bar{\\text{err}}(\\alpha)$ and $d(\\alpha)$ respectively the training error and the number of parameters of model $f_\\alpha(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effective Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(100, 4)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "S = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "preds = S @ y\n",
    "print(np.trace(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear fitting model is one for which we can write:\n",
    "$$\\hat{y} = Sy$$\n",
    "with $S \\in \\mathbb{R}^{N*N}$ depending on the $x_i$ but not on the $y_i$.\n",
    "\n",
    "The effective number of parameters is defined as:\n",
    "$$\\text{df}(S) = \\text{trace}(S)$$\n",
    "\n",
    "If $y$ is from an additive model $Y = f(X) + \\epsilon$, in can be show that:\n",
    "$$\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i) = \\text{trace}(S)\\sigma^2_\\epsilon$$\n",
    "It motivates the more general definition:\n",
    "$$df(\\hat{y}) = \\frac{\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i)}{\\sigma^2_\\epsilon}$$  \n",
    "\n",
    "For neural networks models where we minimize an error function $R(w)$ with  penalty $\\alpha \\sum_m w_m^2$, the effective number of paremeters is:\n",
    "$$\\text{df}(\\alpha) = \\sum_{m=1}^M \\frac{\\theta_m}{\\theta_m+\\alpha}$$\n",
    "\n",
    "with $\\theta_m$ eigenvalues for the Hessian matrix:\n",
    "$$\\frac{\\partial^2 R(w)}{\\partial w \\partial w^T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayesian Approach and BIC\n",
    "\n",
    "The Bayesian information Criterion (BIC)\n",
    "\n",
    "$$\\text{BIC} =  - 2 * \\text{loglik} + \\log N * d$$\n",
    "\n",
    "Uner a Gaussian model, we can write it:\n",
    "\n",
    "$$\\text{BIC} = \\frac{N}{\\sigma_\\epsilon^2}[\\bar{\\text{err}} + \\log N \\frac{d}{N} \\sigma_\\epsilon^2]$$\n",
    "\n",
    "For model selection, there is not clear choice between AIC and BIC.  \n",
    "When $N \\to infty$, BIC tends to select the correct model, but AIC tends to select too complex models.  \n",
    "For finite $N$, BIC often choose too simple models, because of its heavy penalty on complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Description Length\n",
    "\n",
    "We define the Shanon entropy of a random variable $Z$ as:\n",
    "$$H(Z) = - \\sum_{z_i} P(z_i) \\log_2 P(z_i)$$\n",
    "\n",
    "This value represents the minimum average message length to transmit messages from $Z$, in bits.  \n",
    "\n",
    "We have a model $M$ with parameters $t\\theta$, and data $Z = (X, y)$. The condition probability of the outputs under the model is $P(y|\\theta,M,X).  \n",
    "Assuming the receiver know the inputs, and we wish to transmit the outputs, the message length is:\n",
    "\n",
    "$$\\text{length } = - \\log P(y|\\theta,M,X) - \\log P(\\theta|M)$$\n",
    "\n",
    "The MDL principe says we should choose the model that minimizes this length, ie that minimizes the negative log-posterior distribution.  \n",
    "Hence me chose the model that maximize the posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vapnik-Chervonenkis Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-Validation is a technique to estimate the expected prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "We divite the data into $K$ parts of equal size.  \n",
    "For the $k$-th part, we fit a model on the $K-1$ other parts of the data, and calculate the prediction error of this model on the $k$-th part.  \n",
    "We do this $k=1,\\text{...},K$ an combine the $K$ prediction error estimates.  \n",
    "\n",
    "Let $\\kappa: \\{1,\\text{...},N\\} \\to \\{1,\\text{...},K\\}$ indicates the partition $k$ into which observation $i$ is allocated. This allocation should be random.  \n",
    "Let $\\hat{f}^{-k}(x)$ the model fitted without the $k$-th part of the data. The Cross-Validation estimate of prediction error is:\n",
    "$$\\text{CV}(\\hat{f}) = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{f}^{-\\kappa(i)}(x_i))$$\n",
    "\n",
    "Typical choices of $K$ are $5$ or $10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.]\n",
      "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[0,:15])\n",
    "print(y[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y 1D-array of integers, represent correct class\n",
    "#preds 2d-array of probas, a row represent probra of 1obs for all classes \n",
    "def neg_log_loss(y, preds):\n",
    "    res = 0\n",
    "    for i in range(len(y)):\n",
    "        res += np.log(preds[i,y[i]])\n",
    "    return -res\n",
    "\n",
    "def cv_split(X, y, K):\n",
    "    \n",
    "    #shuffle data\n",
    "    p = np.random.permutation(len(X))\n",
    "    X, y = X[p], y[p]\n",
    "\n",
    "    #split into K\n",
    "    split_size = int((len(X) + K - 1) / K)\n",
    "    splits = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        Xk1 = X[:k*split_size]\n",
    "        yk1 = y[:k*split_size]\n",
    "        Xk2 = X[(k+1)*split_size:]\n",
    "        yk2 = y[(k+1)*split_size:]\n",
    "        Xk = np.concatenate((Xk1, Xk2), axis=0)\n",
    "        yk = np.concatenate((yk1, yk2), axis=0)\n",
    "        \n",
    "        Xo = X[k*split_size: (k+1)*split_size]\n",
    "        yo = y[k*split_size: (k+1)*split_size]\n",
    "        \n",
    "        splits.append((Xk, yk, Xo, yo))\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 318.9508387627867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "def cv_eval(X, y, K, model, loss_fn):\n",
    "    splits = cv_split(X, y, K)\n",
    "    loss = 0\n",
    "\n",
    "    for (X_train, y_train, X_test, y_test) in splits:\n",
    "        clf = clone(model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict_proba(X_test)\n",
    "        lossk = loss_fn(y_test, preds)\n",
    "        loss += lossk\n",
    "        #print('accuracy:', np.mean(y_test == clf.predict(X_test)))\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "loss = cv_eval(X, y, 10, model, neg_log_loss)\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case where $K=N$ is called the leave-one-out cross valiadation, $N$ models are estimated using all but one observations, and the model is tested on that single obversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 316.0156542201092\n"
     ]
    }
   ],
   "source": [
    "Xs, ys = X[:100], y[:100]\n",
    "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "loss = cv_eval(X, y, len(Xs), model, neg_log_loss)\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Cross-Validation to find a tuning parameter $\\alpha$:\n",
    "    \n",
    "$$\\text{CV}(\\hat{f}, \\alpha) = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{f}^{-\\kappa(i)}(x_i, \\alpha))$$\n",
    "\n",
    "$\\text{CV}$ produces an estimate of the test error curve, and we find $\\alpha$ that minimizes it.  \n",
    "\n",
    "one-standard-rule: choose the model with lowest complexity  whose error is no more than one standard deviation away from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval model 1/100...\n",
      "Eval model 2/100...\n",
      "Eval model 3/100...\n",
      "Eval model 4/100...\n",
      "Eval model 5/100...\n",
      "Eval model 6/100...\n",
      "Eval model 7/100...\n",
      "Eval model 8/100...\n",
      "Eval model 9/100...\n",
      "Eval model 10/100...\n",
      "Eval model 11/100...\n",
      "Eval model 12/100...\n",
      "Eval model 13/100...\n",
      "Eval model 14/100...\n",
      "Eval model 15/100...\n",
      "Eval model 16/100...\n",
      "Eval model 17/100...\n",
      "Eval model 18/100...\n",
      "Eval model 19/100...\n",
      "Eval model 20/100...\n",
      "Eval model 21/100...\n",
      "Eval model 22/100...\n",
      "Eval model 23/100...\n",
      "Eval model 24/100...\n",
      "Eval model 25/100...\n",
      "Eval model 26/100...\n",
      "Eval model 27/100...\n",
      "Eval model 28/100...\n",
      "Eval model 29/100...\n",
      "Eval model 30/100...\n",
      "Eval model 31/100...\n",
      "Eval model 32/100...\n",
      "Eval model 33/100...\n",
      "Eval model 34/100...\n",
      "Eval model 35/100...\n",
      "Eval model 36/100...\n",
      "Eval model 37/100...\n",
      "Eval model 38/100...\n",
      "Eval model 39/100...\n",
      "Eval model 40/100...\n",
      "Eval model 41/100...\n",
      "Eval model 42/100...\n",
      "Eval model 43/100...\n",
      "Eval model 44/100...\n",
      "Eval model 45/100...\n",
      "Eval model 46/100...\n",
      "Eval model 47/100...\n",
      "Eval model 48/100...\n",
      "Eval model 49/100...\n",
      "Eval model 50/100...\n",
      "Eval model 51/100...\n",
      "Eval model 52/100...\n",
      "Eval model 53/100...\n",
      "Eval model 54/100...\n",
      "Eval model 55/100...\n",
      "Eval model 56/100...\n",
      "Eval model 57/100...\n",
      "Eval model 58/100...\n",
      "Eval model 59/100...\n",
      "Eval model 60/100...\n",
      "Eval model 61/100...\n",
      "Eval model 62/100...\n",
      "Eval model 63/100...\n",
      "Eval model 64/100...\n",
      "Eval model 65/100...\n",
      "Eval model 66/100...\n",
      "Eval model 67/100...\n",
      "Eval model 68/100...\n",
      "Eval model 69/100...\n",
      "Eval model 70/100...\n",
      "Eval model 71/100...\n",
      "Eval model 72/100...\n",
      "Eval model 73/100...\n",
      "Eval model 74/100...\n",
      "Eval model 75/100...\n",
      "Eval model 76/100...\n",
      "Eval model 77/100...\n",
      "Eval model 78/100...\n",
      "Eval model 79/100...\n",
      "Eval model 80/100...\n",
      "Eval model 81/100...\n",
      "Eval model 82/100...\n",
      "Eval model 83/100...\n",
      "Eval model 84/100...\n",
      "Eval model 85/100...\n",
      "Eval model 86/100...\n",
      "Eval model 87/100...\n",
      "Eval model 88/100...\n",
      "Eval model 89/100...\n",
      "Eval model 90/100...\n",
      "Eval model 91/100...\n",
      "Eval model 92/100...\n",
      "Eval model 93/100...\n",
      "Eval model 94/100...\n",
      "Eval model 95/100...\n",
      "Eval model 96/100...\n",
      "Eval model 97/100...\n",
      "Eval model 98/100...\n",
      "Eval model 99/100...\n",
      "Eval model 100/100...\n",
      "min = 263.60046994140384, max = 346.2980171032806, avg = 299.11094324040374\n",
      "best: C = 0.15848484848484848, loss = 263.60046994140384\n",
      "osr solution: C = 0.029797979797979796\n"
     ]
    }
   ],
   "source": [
    "def cv_tune(X, y, K, loss_fn, vals):\n",
    "    errs = np.empty(len(vals))\n",
    "    for i in range(len(vals)):\n",
    "        print('Eval model {}/{}...'.format(i+1, len(vals)))\n",
    "        model = LogisticRegression(solver='liblinear', \n",
    "                                   multi_class='ovr',\n",
    "                                  C=vals[i])\n",
    "        errs[i] = cv_eval(X, y, K, model, loss_fn)\n",
    "    return errs\n",
    "    \n",
    "vals = np.linspace(0.01, 0.99, 100)\n",
    "errs = cv_tune(X, y, 10, neg_log_loss, vals)\n",
    "\n",
    "\n",
    "mini = np.argmin(errs)\n",
    "print('min = {}, max = {}, avg = {}'.format(np.min(errs),\n",
    "                                            np.max(errs),\n",
    "                                            np.mean(errs)))\n",
    "print('best: C = {}, loss = {}'.format(vals[mini], errs[mini]))\n",
    "\n",
    "minp1s = np.min(errs) + np.std(errs)\n",
    "osr_c = vals[errs < minp1s][0]\n",
    "print('osr solution: C = {}'.format(osr_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $K = N$, the CV estimator is unbiased for the expected prediction error, but can have high variance.  \n",
    "Decreasing $K$ lower the variance but increase the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Cross-Validation provite an approximation to leave-one-out Cross Validation, for linear models fitted with least sqares. Thus models of this form:\n",
    "\n",
    "$$\\hat{y} = Sy$$\n",
    "\n",
    "The GVC approximation is:\n",
    "$$GCV(\\hat{f}) = \\frac{1}{N} \\sum_{i=1}^N (\\frac{y_i - \\hat{f}(x_i)}{1 - \\text{trace}(S)/N})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wrong and Right Way to Do Cross-validation\n",
    "\n",
    "With a multi-step modeling procedure, cross-validation must be applied to the entiere sequence of modeling steps.  \n",
    "There can't be a step that see the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap is used to estimate the expected prediction error.  \n",
    "Let's suppose we have a training set $Z = (z_1,\\text{..},z_N$, with $z_i=(x_i,y_i)$. We sample from $Z$ several datasets (with replacement) of size $N$. We produce $B$ bootstrap datasets.  \n",
    "\n",
    "We are trying to estimate $S(Z)$, with $S$ any quantity computed from $Z$. We can estimate the variance of $S(Z)$:\n",
    "\n",
    "$$\\hat{\\text{Var}(S(Z))} = \\frac{1}{B-1} \\sum_{b=1}^B (S(Z^{*b}) - \\bar{S}^*)^2$$\n",
    "\n",
    "We can use this to estimate the prediction error, with the leave-one-out boostrap estimate:\n",
    "\n",
    "$$\\hat{\\text{err}}^{(1)} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{|C^{-i}|} \\sum_{b \\in C^{-i}} L(y_i, \\hat{f}^{*b}(x_i))$$\n",
    "\n",
    "With $C^{-i}$ all bostramp samples that do not contains $i$. We need to choose $B$ big enough so that $C^{-i}$ is never empty, or ignore the empty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431.0110937028146"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap_data(X, y, B):\n",
    "    res = []\n",
    "    for i in range(B):\n",
    "        p = np.random.randint(0, len(X), len(X))\n",
    "        Xo = np.delete(X, p, axis=0)\n",
    "        yo = np.delete(y, p, axis=0)\n",
    "        res.append((X[p], y[p], Xo, yo))\n",
    "    return res\n",
    "\n",
    "def bootstrap_eval(X, y, B, model, loss_fn):\n",
    "    sets = bootstrap_data(X, y, B)\n",
    "    loss = 0\n",
    "    for Xk, yk, Xo, yo in sets:\n",
    "        clf = clone(model)\n",
    "        clf.fit(Xk, yk)\n",
    "        preds = clf.predict_proba(Xo)\n",
    "        lossk = loss_fn(yo, preds)\n",
    "        loss += lossk\n",
    "        \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', multi_class='ovr')        \n",
    "bootstrap_eval(X, y, 10, model, neg_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
