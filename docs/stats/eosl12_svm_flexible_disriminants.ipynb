{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import cvxopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "[Support-Vector Networks - Cortes, Vapnik, 1995](http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf)\n",
    "\n",
    "We have $N$ training points $(x_i, y_i)$ with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\{ -1, 1 \\}$.\n",
    "\n",
    "We can define an hyperplane as $\\{x: x^T\\beta + \\beta_0 = 0 \\}$.  \n",
    "Separating hyperplane can be used to classify the data: $G(x) = \\text{sign} (x^T\\beta + \\beta_0)$.\n",
    "We are trying to find an hyperplane, parameterized by $\\beta$ and $\\beta_0$, that correctly separate the training data.  \n",
    "\n",
    "This correspond to solving the following problem:\n",
    "$$\\min_{\\beta, \\beta_0} ||\\beta||$$\n",
    "$$\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1, i=1,\\text{...},N$$\n",
    "\n",
    "A solution only exists if the training is linearly separable.  \n",
    "\n",
    "To handle the non separable clases, we may allow some points to be on the wrong side of the margin.  \n",
    "Lets define some slack variables: $\\zeta = (\\zeta_1, \\text{...}, \\zeta_N)$.  \n",
    "We modify the constraints as:\n",
    "$$y_i(x_i^T\\beta + \\beta_0) \\geq M(1 - \\zeta_i)$$\n",
    "$$\\zeta_i \\geq 0, \\space \\sum_{i=1}^N \\zeta_i \\leq \\text{constant}$$\n",
    "$\\zeta_i$ is the proportional amount by wich prediction for $x_i$ is on the wrong side of the margin. Misclassification occurs when $\\zeta_i > 1$.\n",
    "\n",
    "The problem became:\n",
    "$$\\min_{\\beta, \\beta_0} ||\\beta||$$\n",
    "$$\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\zeta_i, i=1,\\text{...},N$$\n",
    "$$\\text{subject to } \\zeta_i \\geq 0, \\space \\sum_{i=1}^N \\zeta_i \\leq \\text{constant}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can be reformulated as:\n",
    "$$\\min_{\\beta, \\beta_0} \\frac{1}{2} ||\\beta||^2 + C \\sum_{i=1}^N \\zeta_i$$\n",
    "$$\\text{subject to } \\zeta_i \\geq 0, \\space y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\zeta_i, i=1,\\text{...},N$$\n",
    "\n",
    "with $C$ the cost parameter, an hyperparameter. The hard margin case corresponds to $C = \\infty$.  \n",
    "\n",
    "We get the following Lagrange (primal) problem:\n",
    "\n",
    "$$L_P(\\beta, \\beta_0, \\zeta) = \\frac{1}{2} ||\\beta||^2 + C \\sum_{i=1}^N \\zeta_i - \\sum_{i=1}^N \\alpha_i [y_i(x_i^T\\beta + \\beta_0) - (1 - \\zeta_i)] - \\sum_{i=1}^N \\mu_i \\zeta_i$$\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0, \\zeta; \\zeta_i \\geq 0} \\max_{\\alpha, \\mu ; \\alpha_i \\geq 0, \\mu_i \\geq 0} L_P(\\beta, \\beta_0, \\zeta)$$\n",
    "\n",
    "Instead of solving the primal, we solve the dual, that gaves the same result:\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0, \\zeta; \\zeta_i \\geq 0} \\max_{\\alpha, \\mu ; \\alpha_i \\geq 0, \\mu_i \\geq 0} L_P(\\beta, \\beta_0, \\alpha) = max_{\\alpha, \\mu ; \\alpha_i \\geq 0, \\mu_i \\geq 0} \\min_{\\beta, \\beta_0, \\zeta; \\zeta_i \\geq 0} L_P(\\beta, \\beta_0, \\zeta)$$ \n",
    "\n",
    "We start by minimizing $L_P(\\beta, \\beta_0, \\zeta)$ with respect to $\\beta$, $\\beta_0$, and $\\zeta$.  \n",
    "\n",
    "Solving $\\frac{\\partial L_P(\\beta, \\beta_0, \\zeta)}{\\partial \\beta} = 0$, we get:\n",
    "$$\\beta = \\sum_{i=1}^N \\alpha_i y_i x_i$$\n",
    "Solving $\\frac{\\partial L_P(\\beta, \\beta_0, \\zeta)}{\\partial \\beta_0} = 0$, we get:\n",
    "$$\\sum_{i=1}^N \\alpha_i y_i = 0$$\n",
    "Solving $\\frac{\\partial L_P(\\beta, \\beta_0, \\zeta)}{\\partial \\zeta} = 0$, we get:\n",
    "$$\\alpha_i = C - \\mu_i, \\space i=1,\\text{...},N$$\n",
    "\n",
    "Remplacing on the original equation, we get:\n",
    "\n",
    "$$L_D(\\alpha) = \\sum_{i=1}^N\\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{k=1}^N \\alpha_i \\alpha_k y_i y_k x_i^Tx_k$$\n",
    "\n",
    "The problem became:\n",
    "$$\\max_{\\alpha} L_D(\\alpha)$$\n",
    "$$\\text{s.t. } 0 \\leq \\alpha_i \\leq C \\space i=1,\\text{...}, N$$\n",
    "$$\\text{s.t. } \\sum_{i=1}^N \\alpha_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another condition is satisfied:\n",
    "    \n",
    "$$\\alpha_i[y_i(x_i^T\\beta + \\beta_0) - (1 - \\zeta_i)] = 0 \\space i = 1,\\text{...},N$$\n",
    "\n",
    "- $\\alpha_i =0$:  $y_i(x_i^T + \\beta_0) > (1 - \\zeta_i)$\n",
    "- $\\alpha_i > 0$ and $\\zeta_i = 0$: $y_i(x_i^T + \\beta_0) = 1$, which means $x_i$ is on the boundary of the slab, and $0 < \\alpha_i < C$\n",
    "- $\\alpha_i > 0$ and $\\zeta_i > 0$: $\\alpha_i = C$\n",
    "\n",
    "$\\beta$ is defined in terms of a linear combination of the support points $x_i$ (where $\\alpha_i > 0$):\n",
    "\n",
    "$$\\beta = \\sum_{i=1}^N \\alpha_i y_i x_i$$\n",
    "\n",
    "$\\beta_0$ is obtained by solving $y_i(x_i^T + \\beta_0) = 1$ for any support points with $\\zeta_i = 0$.\n",
    "\n",
    "Predictions are made using:\n",
    "$$\\hat{G}(x) = \\text{sign}(x^T\\hat{\\beta} + \\hat{\\beta_0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the problem into a standard quadradic programming problem:\n",
    "    \n",
    "Let $H \\in \\mathbb{R}^{N*N}$: $H_{ij} = y_i y_j x_i^Tx_j$\n",
    "\n",
    "The problem became:\n",
    "$$\\max_{\\alpha} 1^T\\alpha - \\frac{1}{2} \\alpha^TH\\alpha$$\n",
    "$$\\text{s.t. } \\alpha_i \\geq 0$$\n",
    "$$\\text{s.t. } \\alpha_i \\leq C$$\n",
    "$$\\text{s.t. } y^T \\alpha = 0$$\n",
    "\n",
    "We reverse the sign and turn it into a minimization:\n",
    "\n",
    "$$\\min_{\\alpha} \\frac{1}{2} \\alpha^TH\\alpha - 1^T\\alpha$$\n",
    "$$\\text{s.t. } - \\alpha_i \\leq 0$$\n",
    "$$\\text{s.t. } \\alpha_i \\leq C$$\n",
    "$$\\text{s.t. } y^T \\alpha = 0$$\n",
    "\n",
    "We can compute the parameters:\n",
    "\n",
    "$$\\beta = (y \\odot \\alpha) X$$\n",
    "$$\\beta_0 = y_i - x_i \\beta, \\space \\forall i: \\alpha_i > 0 \\text{ and } \\zeta_i = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.7395833333333334\n",
      " test acc: 0.8082191780821918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_data(b, b0, X, y):\n",
    "\n",
    "    #bo + b1x1 + b2x2 = 0 => x2 = -b1/b2*x1 - b0/b2 \n",
    "    x1 = np.linspace(np.min(X[:,0]) - .5, np.max(X[:,0]) + .5, 1000)\n",
    "    plt.plot(x1, -b[0]/b[1]*x1 -b0/b[1])\n",
    "    \n",
    "    b10 = b / np.linalg.norm(b) * 10\n",
    "    plt.plot([0, b10[0]], [-b0/b[1], b10[1]-b0/b[1]], c='y')\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        plt.scatter(X[i][0], X[i][1], c=('r' if y[i] == 1 else 'g'))\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "X, y = load_digits().data, load_digits().target\n",
    "X = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "X = X[(y == 1) | (y == 7)]\n",
    "y = y[(y == 1) | (y == 7)]\n",
    "\n",
    "y[y == 1] = -1\n",
    "y[y == 7] = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "\n",
    "clf = SVC(C=1, kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('train acc:', np.mean(clf.predict(X_train) == y_train))\n",
    "print(' test acc:', np.mean(clf.predict(X_test) == y_test))\n",
    "\n",
    "plot_data(clf.coef_[0], clf.intercept_[0], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.7395833333333334\n",
      " test acc: 0.8082191780821918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def svm_soft(X, y, C):\n",
    "    n, p = X.shape\n",
    "    X = X.astype(np.double)\n",
    "    y = y.astype(np.double)\n",
    "    \n",
    "    H = (y.reshape(-1, 1)*X @ (y.reshape(-1, 1)*X).T)\n",
    "    \n",
    "    P = cvxopt.matrix(H)\n",
    "    q = cvxopt.matrix(-np.ones((n, 1)))\n",
    "    G = cvxopt.matrix(np.vstack((-np.eye(n),np.eye(n))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros((n,)), C*np.ones(n))))\n",
    "    A = cvxopt.matrix(y.reshape(1, -1))\n",
    "    b = cvxopt.matrix(np.zeros((1,)))\n",
    "    \n",
    "    cvxopt.solvers.options['show_progress'] = False\n",
    "    cvxopt.solvers.options['abstol'] = 1e-10\n",
    "    cvxopt.solvers.options['reltol'] = 1e-10\n",
    "    cvxopt.solvers.options['feastol'] = 1e-10\n",
    "    \n",
    "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(sol['x']).flatten()\n",
    "    \n",
    "    beta = (y * alpha) @ X\n",
    "    S = (alpha > 1e-4) & ((alpha - C)**2 > 1e-8)\n",
    "    beta0 = y[S] - (X[S] @ beta)\n",
    "    beta0 = np.mean(beta0)\n",
    "    \n",
    "    return alpha, beta, beta0\n",
    "    \n",
    "alpha, beta, beta0 = svm_soft(X_train, y_train, C=1)\n",
    "\n",
    "tr_preds = np.sign(X_train @ beta + beta0)\n",
    "te_preds = np.sign(X_test @ beta + beta0)\n",
    "print('train acc:', np.mean(tr_preds == y_train))\n",
    "print(' test acc:', np.mean(te_preds == y_test))\n",
    "plot_data(beta, beta0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: [-0.17638245 -0.0945775 ]\n",
      "beta_0: -1.0342960501338918\n",
      "Number of support vectors for each class: [95 94]\n",
      "Accuracy: 0.7395833333333334\n"
     ]
    }
   ],
   "source": [
    "S = (alpha > 1e-4)\n",
    "S_idxs = np.arange(len(X_train))[S]\n",
    "S_vecs = X_train[S_idxs]\n",
    "S_nc = np.array([\n",
    "    np.sum(y_train[S_idxs] == -1),\n",
    "    np.sum(y_train[S_idxs] == +1),\n",
    "])\n",
    "\n",
    "y_pred = np.sign(X_train @ beta + beta0)\n",
    "acc = np.average(y_train == y_pred)\n",
    "\n",
    "print('beta:', beta)\n",
    "print('beta_0:', beta0)\n",
    "#print('Indices of support vectors:', S_idxs)\n",
    "#print('Support vectors:', S_vecs)\n",
    "print('Number of support vectors for each class:', S_nc)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: [[-0.1763627  -0.09456784]]\n",
      "beta_0: [-1.03423123]\n",
      "Number of support vectors for each class: [95 94]\n",
      "Accuracy: 0.7395833333333334\n"
     ]
    }
   ],
   "source": [
    "#Comparing with sklearn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C = 1, kernel = 'linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "acc = np.average(y_train == y_pred)\n",
    "\n",
    "print('beta:', clf.coef_)\n",
    "print('beta_0:', clf.intercept_)\n",
    "#print('Indices of support vectors:', clf.support_)\n",
    "#print('Support vectors:', clf.support_vectors_)\n",
    "print('Number of support vectors for each class:', clf.n_support_)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM and Kernels\n",
    "\n",
    "We can apply basis expansions to the input features before using an SVM classifier:\n",
    "\n",
    "$$h(x_i) = (h_1(x_i), h_2(x_i), \\text{...}, h_M(x_i))$$\n",
    "\n",
    "An extension of the SVM allows to use very large basis expansions, even infintie ones, at a cheap cost.  \n",
    "For learning and inference, we only need to compute dot products between vectors of the input space. This can be done more cheaply than transforming the vectors on the new basis, and then computing their dot product.\n",
    "\n",
    "The dual function to maximize is:\n",
    "\n",
    "$$L_D(\\alpha) = \\sum_{i=1}^N\\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{k=1}^N \\alpha_i \\alpha_k y_i y_k \\langle  h(x_i), h(x_k) \\rangle$$\n",
    "\n",
    "The prediction function can be written as:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^N \\alpha_i y_i \\langle h(x), h(x_i) \\rangle + \\beta_\n",
    "0$$  \n",
    "\n",
    "We only need to know the kernel function:\n",
    "\n",
    "$$K(x,x') = \\langle h(x), h(x') \\rangle$$\n",
    "\n",
    "Popular choices of kernel are:\n",
    "\n",
    "- The $d$-th degree polynomial:\n",
    "$$K(x, x') = (1 + \\langle x, x' \\rangle )^d$$\n",
    "\n",
    "- The radial basis function (RBF):\n",
    "$$K(x, x') = \\exp (- \\gamma ||x - x'||^2)$$\n",
    "\n",
    "- Neural network:\n",
    "$$K(x, x') = \\tanh(k_1 \\langle x, x' \\rangle + k_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    \n",
    "    def __init__(self, C, k, alpha_tol=1e-4):\n",
    "        self.C = C\n",
    "        self.k = k\n",
    "        self.alpha_tol=alpha_tol\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n, p = X.shape\n",
    "        X = X.astype(np.double)\n",
    "        y = y.astype(np.double)\n",
    "    \n",
    "        H = np.empty((len(X), len(X)))\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X)):\n",
    "                H[i,j] = y[i] * y[j] * self.k(X[i], X[j])\n",
    "        \n",
    "        P = cvxopt.matrix(H)\n",
    "        q = cvxopt.matrix(-np.ones((n, 1)))\n",
    "        G = cvxopt.matrix(np.vstack((-np.eye(n),np.eye(n))))\n",
    "        h = cvxopt.matrix(np.hstack((np.zeros((n,)), self.C*np.ones(n))))\n",
    "        A = cvxopt.matrix(y.reshape(1, -1))\n",
    "        b = cvxopt.matrix(np.zeros((1,)))\n",
    "\n",
    "        cvxopt.solvers.options['show_progress'] = False\n",
    "        cvxopt.solvers.options['abstol'] = 1e-10\n",
    "        cvxopt.solvers.options['reltol'] = 1e-10\n",
    "        cvxopt.solvers.options['feastol'] = 1e-10\n",
    "\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        alpha = np.array(sol['x']).flatten()\n",
    "\n",
    "        beta = (y * alpha) @ X\n",
    "        S = (alpha > self.alpha_tol) & ((alpha - self.C)**2 > 1e-6)\n",
    "        \n",
    "        beta0 = np.empty(len(y[S]))\n",
    "        for j in range(len(beta0)):\n",
    "            beta0[j] = y[S][j]\n",
    "            for i in range(len(X)):\n",
    "                beta0[j] -= alpha[i] * y[i] * self.k(X[S][j], X[i])\n",
    "        beta0 = np.mean(beta0)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta0 = beta0\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_pred(X[i])\n",
    "        return y\n",
    "        \n",
    "    def get_pred(self, x):\n",
    "        y = self.beta0\n",
    "        for i in range(len(self.X)):\n",
    "            y += self.alpha[i] * self.y[i] * self.k(x, self.X[i])\n",
    "        return np.sign(y)\n",
    "    \n",
    "def kernel_linear():\n",
    "    return lambda x, y: x @ y\n",
    "\n",
    "def kernel_poly(d):\n",
    "    return lambda x, y: (1 + x @ y) ** d\n",
    "\n",
    "def kernel_rbf(gamma):\n",
    "    return lambda x, y: np.exp(-gamma * (x - y)@(x - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear kernel\n",
    "\n",
    "$$K(x, x') = \\langle x, x' \\rangle$$\n",
    "$$h(x) = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -1.10350876418606\n",
      "Number of support vectors for each class: [113 113]\n",
      "Accuracy: 0.7534626038781164\n"
     ]
    }
   ],
   "source": [
    "clf = SVMClassifier(C=2, k=kernel_linear())\n",
    "clf.fit(X, y)\n",
    "\n",
    "S = (clf.alpha > 1e-4)\n",
    "S_idxs = np.arange(len(X))[S]\n",
    "S_nc = np.array([\n",
    "    np.sum(y[S_idxs] == -1),\n",
    "    np.sum(y[S_idxs] == +1),\n",
    "])\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.beta0)\n",
    "print('Number of support vectors for each class:', S_nc)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -1.1032004102141195\n",
      "Number of support vectors for each class: [113 113]\n",
      "Accuracy: 0.7534626038781164\n"
     ]
    }
   ],
   "source": [
    "#Comparing with sklearn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=2, kernel='linear')\n",
    "clf.fit(X, y)\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.intercept_[0])\n",
    "print('Number of support vectors for each class:', clf.n_support_)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "$$K(x, x') = (1 + \\langle x, x' \\rangle)^d$$\n",
    "\n",
    "$h(x)$ perform a polynomial expansion of degree $d$. For a vector $x \\in \\mathbb{R}^p$, we get a feature space of size:\n",
    "$$\\binom{p+d}{d}$$\n",
    "\n",
    "When $d = 2$:\n",
    "\n",
    "$$h(x) = (1, \\sqrt{2} x_1, \\text{...}, \\sqrt{2} x_p,  x_1^2, \\text{...}, x_p^2, \\sqrt{2} x_p x_{p-1}, \\text{...}, \\sqrt{2} x_p x_1, \\sqrt{2} x_{p-1} x_{p-2}, \\sqrt{2} x_{p-1} x_1, \\text{...}, \\sqrt{2} x_2 x_1)$$\n",
    "\n",
    "When $d=2, p=3, x=(x_1, x_2, x_3)$:\n",
    "$$h(x) = (1, \\sqrt{2} x_1, \\sqrt{2} x_2, \\sqrt{2} x_3, x_1^2, x_2^2, x_3^2, \\sqrt{2} x_3 x_2, \\sqrt{2} x_3 x_1, \\sqrt{2} x_2 x_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -2.555446699499953\n",
      "Number of support vectors for each class: [66 64]\n",
      "Accuracy: 0.853185595567867\n"
     ]
    }
   ],
   "source": [
    "clf = SVMClassifier(C=2, k=kernel_poly(3), alpha_tol=1e-8)\n",
    "clf.fit(X, y)\n",
    "\n",
    "S = (clf.alpha > clf.alpha_tol)\n",
    "S_idxs = np.arange(len(X))[S]\n",
    "S_nc = np.array([\n",
    "    np.sum(y[S_idxs] == -1),\n",
    "    np.sum(y[S_idxs] == +1),\n",
    "])\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.beta0)\n",
    "print('Number of support vectors for each class:', S_nc)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -43.87659197123928\n",
      "Number of support vectors for each class: [43 43]\n",
      "Accuracy: 0.8310249307479224\n"
     ]
    }
   ],
   "source": [
    "#Comparing with sklearn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=2, kernel='poly', degree=3, gamma=1.0, coef0=1.0)\n",
    "clf.fit(X, y)\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.intercept_[0])\n",
    "print('Number of support vectors for each class:', clf.n_support_)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "\n",
    "$$K(x, x') = \\exp (- \\frac{||x - x'||^2}{2\\sigma^2})$$ \n",
    "We often use the $\\gamma$ parameter instead:\n",
    "$$\\gamma = \\frac{1}{2 \\sigma^2}$$\n",
    "\n",
    "The feature space of $h(x)$ has an infite number of dimensions.  \n",
    "\n",
    "The RBF function acts as a similarity measure. The closer are $x$ and $x'$, the higher is the value, and the more far, the lower is the value. When the distance is $\\infty$ the value is $0$, and when $x=x'$ the value is 1.\n",
    "\n",
    "The $\\gamma$ parameter defines how far the influence of a single training exmample reaches, with low values meaning far, and high value meaning closes.  \n",
    "With $\\gamma$ too large, the area of influence of a support vector only includes the support vector itself, and the model is prone to huge overfitting.  \n",
    "With $\\gamma$ too small, the area of influence of a support vector includes the whole training set, and the model converges to a linear one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -0.1482645146885623\n",
      "Number of support vectors for each class: [162 157]\n",
      "Accuracy: 0.9695290858725761\n"
     ]
    }
   ],
   "source": [
    "clf = SVMClassifier(C=2, k=kernel_rbf(1.5))\n",
    "clf.fit(X, y)\n",
    "\n",
    "S = (clf.alpha > clf.alpha_tol)\n",
    "S_idxs = np.arange(len(X))[S]\n",
    "S_nc = np.array([\n",
    "    np.sum(y[S_idxs] == -1),\n",
    "    np.sum(y[S_idxs] == +1),\n",
    "])\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.beta0)\n",
    "print('Number of support vectors for each class:', S_nc)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: -0.14826613354920232\n",
      "Number of support vectors for each class: [162 157]\n",
      "Accuracy: 0.9695290858725761\n"
     ]
    }
   ],
   "source": [
    "#Comparing with sklearn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=2, kernel='rbf', gamma=1.5)\n",
    "clf.fit(X, y)\n",
    "acc = np.average(clf.predict(X) == y)\n",
    "\n",
    "print('beta_0:', clf.intercept_[0])\n",
    "print('Number of support vectors for each class:', clf.n_support_)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM as a Penalization mthod\n",
    "\n",
    "The SVM problem is:\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0} \\frac{1}{2} ||\\beta||^2 + C \\sum_{i=1}^N \\zeta_i$$\n",
    "$$\\text{subject to } \\zeta_i \\geq 0, \\space y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\zeta_i, i=1,\\text{...},N$$\n",
    "\n",
    "This is equivalent to the following problem:\n",
    "\n",
    "$$\\min_{\\beta_0, \\beta} \\sum_{i=1}^N (1 - y_if(x_i))_+ + \\frac{\\lambda}{2} ||\\beta||^2$$\n",
    "\n",
    "with $\\lambda = \\frac{1}{C}$\n",
    "\n",
    "This is based on the hingle loss function:\n",
    "$$L(y,f) = (1 - yf)_+ = \\max(0, 1 - yf)$$\n",
    "\n",
    "This formulation shrinks the weights $\\beta$ toward $0$. It also helps to see that $C$ act as a regularization parameter.  \n",
    "Lower values of $C$ gives more shrinkage, and regularize the model. Higher values gives more complex models, that may overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Path algorithm\n",
    "\n",
    "In order to chose the $C$ hyperparameter, one solution is to train several SVM with different values of $C$, in order to evaluate, compare and choose the best one.  \n",
    "The Path algorithm is an algorithm to fit the entiere sequence of SVM more efficiently.\n",
    "\n",
    "The solution for $\\beta$ is given by:\n",
    "$$\\beta_\\lambda = \\frac{1}{\\lambda} \\sum_{i=1}^N \\alpha_i y_i x_i$$\n",
    "\n",
    "At the initialisation, we start with a huge value of $\\lambda$, giving huge margins, with all points on the training set inside them, with $\\alpha_i = 1$.  \n",
    "We slowly decrease $\\lambda$, several training points move from inside to outside the margin, setting $\\alpha_i = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Regression\n",
    "\n",
    "[Support Vector Regression Machines](https://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf)\n",
    "\n",
    "The linear model is:\n",
    "$$f(x) = x^T \\beta + \\beta_0$$\n",
    "\n",
    "We minimise the following criterion:\n",
    "$$H(\\beta_0, \\beta) = \\sum_{i=1}^N V(y_i - f(x_i)) + \\frac{\\lambda}{2} ||\\beta||^2$$\n",
    "\n",
    "with the error measure $V_\\epsilon$:\n",
    "$$\n",
    "V_\\epsilon(r) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if } |r| < \\epsilon \\\\\n",
    "    |r| - \\epsilon & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is an $\\epsilon$-sensitive mesure, ignores errors less than $\\epsilon$.  \n",
    "Points in the training set that are close to the model prediction have an error of $0$, and are ignored by the model.  \n",
    "This is an analogy to the SVM, where points correctly classified and outisde the margins ($\\alpha_i = 0$) are ignored by the model.  \n",
    "\n",
    "The solution has the form:\n",
    "$$\\hat{\\beta} = \\sum_{i=1}^N (\\hat{\\alpha}^*_i - \\hat{\\alpha}_i) x_i$$\n",
    "$$\\hat{f}(x) = \\sum_{i=1}^N (\\hat{\\alpha}^*_i - \\hat{\\alpha}_i) \\langle x,x_i \\rangle + \\beta_0$$\n",
    "\n",
    "$\\hat{\\alpha}_i$ and $\\hat{\\alpha}^*_i$ are found by solving the following quadratic problem:\n",
    "$$\\min_{\\alpha_i, \\alpha^*_i} \\epsilon \\sum_{i=1}^N (\\alpha^*_i + \\alpha_i) - \\sum_{i=1}^N y_i(\\alpha^*_i - \\alpha_i) + \\frac{1}{2} \\sum_{i,i'=1}^N (\\alpha^*_i - \\alpha_i) (\\alpha^*_{i'} - \\alpha_{i'}) \\langle x_i, x_{i'} \\rangle$$\n",
    "$$\\text{s.t. } 0 \\leq \\alpha_i, \\space \\alpha^*_i \\leq 1/\\lambda$$\n",
    "$$\\text{s.t. } \\sum_{i=1}^N (\\alpha^*_i - \\alpha_i) = 0$$\n",
    "$$\\text{s.t. } \\alpha^*_i \\alpha_i = 0$$\n",
    "\n",
    "The support vectors are the training points such that$(\\alpha^*_i - \\alpha_i)$ is not null. The model output only depends of those.  \n",
    "As for classification, the input values are only used through the inner product $\\langle x, x_i \\rangle$, so the the Kernel trick can also be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing Linear Discriminant Analysis\n",
    "\n",
    "LDA is a simple and low variance classifier that produces linear decision boundaries.  \n",
    "Several ideas can be applied to generalize this model to non-linear cases:\n",
    "- FDA: cast LDA as regression, and do it into an enlarged space, as for SVM kernels\n",
    "- PDA: penalize its coefficients\n",
    "- MDA: model each class by a mixture of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Discriminant Analysis\n",
    "\n",
    "[Flexible Discriminant Analysis by Optimal Scoring - Hastie, Tibshirani, 2000](https://www.researchgate.net/publication/2889611_Flexible_Discriminant_Analysis_by_Optimal_Scoring)\n",
    "\n",
    "We use the fact that LDA is equivalent to canonical correlation analysis, it gives solution to a problem that we describe.  \n",
    "\n",
    "Suppose $\\theta: \\mathcal{G} \\to \\mathbb{R}$ assign scores to the classes such that the transformed class labels are optimally predicted by linear regression on $X$.  \n",
    "We can find $K$ set of independents scoring $\\theta_k$ and corresponding linear map: $\\mu_k(X) = X^T\\beta_k$.  \n",
    "\n",
    "$\\theta_k$ and $\\beta_k$ are chosen to minimize:\n",
    "$$\\text{ASR} = \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^N (\\theta_k(g_i) - x^T_i\\beta_k)^2$$\n",
    "\n",
    "The scores are assumed to be mutually orthogonal and normalized to avoid trivial zero solutions.  \n",
    "The sequence of LDA vectors $u_k$ is identical to the sequence $\\beta_k$ up to a constant.  \n",
    "\n",
    "Let $Y \\in \\mathbb{R}^{N*J}$ the indicator matrix corresponding to the dummy-variable coding for the classes.  \n",
    "Let $\\Theta \\in \\mathbb{R}^{J*K}$ the score matrix, such that $\\Theta_{jk} = \\theta_k(j)$  \n",
    "Let $\\Theta^* \\in \\mathbb{R}^{N*K}$ the matrix of transformed values of the classes: $\\Theta^* = Y\\Theta$.  \n",
    "\n",
    "If the scores were fixed, we would just have to regress $\\Theta^*$ on $X$ to minimize ASR.  \n",
    "\n",
    "Let $P_X$ the projection matrix such that $\\hat{Y} = P_XY$, with $\\hat{Y}$ the predictions of the model.\n",
    "\n",
    "$$\\text{ASR}(\\theta) = \\frac{1}{N} \\text{tr} (\\Theta^TY^T (I - P_X)Y\\theta )$$  \n",
    "\n",
    "In order to minimize ASR, find the optimal scoring $\\Theta$ amounts to find the eigenvectors of $Y^TP_XY$. This can be done without computing $P_X$.  \n",
    "\n",
    "LDA optimal scoring algorithm:\n",
    "\n",
    "1. Initialize the indicator matrix $Y$\n",
    "2. Find the coefficients matrix $B \\in \\mathbb{R}^{p*J}$ by regressing $Y$ on $X$, and set $\\hat{Y} = XB$\n",
    "3. Obtain the eigenvector matrix $\\theta$ of $Y^T\\hat{Y} = Y^TP_XY$\n",
    "4. Obtain the final weights matrix: $B \\in \\mathbb{R}^{p*K}: B \\leftarrow B \\theta$, with $\\beta_k = B_{:,k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be extended to non-linear cases: we replace the linear projection operator $P_X$ by any non-linear regression procedure $S(\\lambda)$.\n",
    "\n",
    "FDA optimal scoring algorithm:\n",
    "1. Choose an initial score matrix $\\Theta_0$ satisfying the constraints $\\Theta^T D_P \\Theta - I$\n",
    "2. Set $\\Theta^*_0 = Y\\Theta_0$\n",
    "3. Fit a regression model of $\\Theta_0^*$ on $X$, giving us $S(\\hat{\\lambda})$ and $\\mu(x)$ the vector of fitted regression functions.\n",
    "4. Get fitted valus $\\hat{\\Theta}^*_0$\n",
    "5. Obtain the eigenvector matrix $\\Phi$ of $\\Theta^{*T}_0  \\hat{\\Theta}^*_0 = \\Theta^*_0S(\\hat{\\lambda})\\Theta_0^*$\n",
    "6. Get optimal scores: $\\Theta = \\Theta_0 \\Phi$\n",
    "7. Update final model from step 3 using the optimal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Discriminant Analysis\n",
    "\n",
    "[Penalized Discriminant Analysis - Hastie, Tibshirani, Buja, 2000](https://www.researchgate.net/publication/2923857_Penalized_Discriminant_Analysis)\n",
    "\n",
    "FDA can be seen as a regualized form of LDA. Suppose we perform a linear regression into a basis expansions $h(X)$, adding a quadratic penalty on the coefficients:\n",
    "\n",
    "$$\\text{ASR} = \\frac{1}{N} \\sum_{l=1}^L  \\left[ \\sum_{i=1}^N (\\theta_l(g_i) - h^T(x_i)\\beta_l)^2 + \\lambda \\beta_l^T \\Omega \\beta_l \\right]$$ \n",
    "\n",
    "$\\Omega$ depends on the problem, it might constrait $h(x)\\beta_l$ to be smooth for example.\n",
    "\n",
    "PDA algorithm:\n",
    "1. Enlarge $X$ via basis expansion $h(X)$\n",
    "2. Use penalized LDA on the enlarged space. The penalized Mahalbonis distance is:\n",
    "    $$D(x, \\mu) = (h(x) - h(\\mu))^T (\\Sigma_W + \\lambda \\Omega)^{-1} (h(x) - h(\\mu))$$\n",
    "    with $\\Sigma_W$ the within-class covariance matrix of $h(x_i)$\n",
    "3. Solve the penalized LDA problem:\n",
    "    $$\\max u^T \\Sigma_B u$$\n",
    "    $$\\text{s.t. } u^T(\\Sigma_W + \\lambda \\Omega)u = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Discriminant Analysis\n",
    "\n",
    "[Discriminant Analysis by Gaussian Mixtures - Hastie, Tibshirani, 1996](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.3633)\n",
    "\n",
    "A Gausssian mixture model for $k$-th class has density:\n",
    "$$P(X|G=k) = \\sum_{r=1}^{R_k} \\pi_{kr} \\phi(X;\\mu_{kr},\\Sigma)$$\n",
    "\n",
    "The $k$-th class has $K$ Gaussian, each with it's one mean $\\mu_{kr}$ and mixing proportion $\\pi_{kr}$. All Gaussians share the same covariance matrix $\\Sigma$.\n",
    "\n",
    "The posterior probabilities are given by:\n",
    "$$p(G=k|X=x) = \\frac{\\sum_{r=1}^{R_k} \\pi_{kr} \\phi(X;\\mu_{kr},\\Sigma) \\Pi_k }{\\sum_{l=1}^K \\sum_{r=1}^{R_l} \\pi_{lr} \\phi(X;\\mu_{lr},\\Sigma) \\Pi_l}$$\n",
    "\n",
    "with $\\Pi_k$ the prior probability of class $k$.  \n",
    "\n",
    "The parameters are estimated with maximum likelihood of $P(G,X)$ on the training data:\n",
    "$$\\sum_{k=1}^K \\sum_{g_i=k} \\log \\left[ \\sum_{r=1}^{R_k} \\pi_{kr} \\phi(x_i;\\mu_{kr},\\Sigma) \\Pi_k \\right]$$\n",
    "\n",
    "The model is fitted with the EM algorithm:\n",
    "\n",
    "- E-step: Compute the responsability of subclasses $c_{kr}$ for each class-$k$ observations. It indicates how much each observations is close to the centroid of every Gaussian for it's class $k$.\n",
    "- M-step: Perform MLE using weights from the E-step.  \n",
    "    This step turns out to be a weighted version of LDA. Furthermore, we can use FDA or PDA instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
