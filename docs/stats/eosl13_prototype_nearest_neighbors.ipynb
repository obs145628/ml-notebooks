{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: N points $(x_i, g_i)$ with $g_i$ class label (1-K).  \n",
    "Prototype methods represent the training data by a set of points in feature space, each associated with a label.  \n",
    "Prediction is made by assignint to it the class of it's closest prototype and.  \n",
    "Closest can be for example Euclidian distance, after we standardize the data.  \n",
    "The challenge is to find how many prototypes and where to put them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "K-Means find $R$ clusters and cluster centers in unlabelled data.  \n",
    "The procedure iterately moves the centers to minimize the total within cluster variance.\n",
    "\n",
    "Algorithm:\n",
    "1. Initialize the $R$ clusters randomly (from training set)\n",
    "2. Repeat until convergence:\n",
    "    - Assign each training point to the closest centroid\n",
    "    - The center of each cluster becomes the mean of all its assigned points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris().data, load_iris().target\n",
    "X = X - np.mean(X, axis=0)\n",
    "X = X / np.std(X, axis=0)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "class KMeansClustering:\n",
    "    \n",
    "    def __init__(self, R):\n",
    "        self.R = R\n",
    "        \n",
    "    def fit(self, X):\n",
    "        N, p = X.shape      \n",
    "        self.means = X[np.random.choice(N, self.R)]\n",
    "        \n",
    "        while True:\n",
    "            old_means = self.means.copy()\n",
    "            \n",
    "            #assign each point to the closest cluster\n",
    "            ctrs = [list() for _ in range(self.R)]\n",
    "            for x in X:\n",
    "                ctrs[self.get_closest_ctr_idx(x)].append(x)\n",
    "                \n",
    "            \n",
    "            # compute the new center position of every cluster\n",
    "            for i in range(self.R):\n",
    "                if len(ctrs[i]) != 0:\n",
    "                    self.means[i] = np.mean(np.vstack(ctrs[i]), axis=0)\n",
    "            \n",
    "            \n",
    "            if np.linalg.norm(old_means - self.means) < 1e-6:\n",
    "                break\n",
    "        \n",
    "    \n",
    "    def get_closest_ctr_idx(self, x):\n",
    "        min_idx = None\n",
    "        min_dist = float('inf')\n",
    "        for i in range(self.R):\n",
    "            dist = (x - self.means[i]) @ (x - self.means[i])\n",
    "            if dist < min_dist:\n",
    "                min_idx = i\n",
    "                min_dist = dist\n",
    "\n",
    "        return min_idx\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_closest_ctr_idx(X[i])\n",
    "        return y\n",
    "                \n",
    "mod = KMeansClustering(3)\n",
    "mod.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means can be used for classification. We fit one K-Means for each class of the training set.  \n",
    "For prediction, we find the closest centroid among all K-Means. The prediction is the class the centroid belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.9416666666666667\n",
      "test acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class KMeansClassifier:\n",
    "    \n",
    "    def __init__(self, R, K):\n",
    "        self.R = R\n",
    "        self.K = K\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.kmod = [None] * self.K\n",
    "        for k in range(self.K):\n",
    "            self.kmod[k] = KMeansClustering(self.R)\n",
    "            self.kmod[k].fit(X[y==k])\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_pred(X[i])\n",
    "        return y\n",
    "    \n",
    "    def get_pred(self, x):\n",
    "        min_dist = float('inf')\n",
    "        min_k = None\n",
    "        for k in range(self.K):\n",
    "            ctl = self.kmod[k].means[self.kmod[k].get_closest_ctr_idx(x)]\n",
    "            dist = (x - ctl) @ (x - ctl)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_k = k\n",
    "        return min_k\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "mod = KMeansClassifier(5, 3)\n",
    "mod.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == mod.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == mod.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Vector Quantization\n",
    "\n",
    "Better than K-Means for classification, uses all the data to place the centroids. Training points attract prototypes of the correct class, and repel other prototypes.  \n",
    "\n",
    "Algorithm page 462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.975\n",
      "test acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LVQ:\n",
    "    \n",
    "    def __init__(self, R, K, lr_begin, lr_end, lr_fact):\n",
    "        self.R = R\n",
    "        self.K = K\n",
    "        self.lr_begin = lr_begin\n",
    "        self.lr_end = lr_end\n",
    "        self.lr_fact = lr_fact\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N, p = X.shape\n",
    "        \n",
    "        self.centers = np.empty((self.R * self.K, p))\n",
    "        for k in range(self.K):\n",
    "            Xk = X[y == k]\n",
    "            p = np.random.choice(len(Xk), self.R)\n",
    "            self.centers[k*self.R:(k+1)*self.R] = Xk[p]\n",
    "            \n",
    "        lr = self.lr_begin\n",
    "        it = 0\n",
    "        while lr > self.lr_end:\n",
    "            \n",
    "            p = np.random.choice(len(X))\n",
    "            sx, sy = X[p], y[p]\n",
    "            ctr = self.get_closest_ctr_idx(sx)\n",
    "            k = ctr // self.R\n",
    "            step = lr if k == sy else -lr\n",
    "            self.centers[ctr] += step * (sx - self.centers[ctr])\n",
    "            \n",
    "            it += 1\n",
    "            lr *= self.lr_fact\n",
    "        \n",
    "    def get_closest_ctr_idx(self, x):\n",
    "        min_idx = None\n",
    "        min_dist = float('inf')\n",
    "        for i in range(len(self.centers)):\n",
    "            dist = (x - self.centers[i]) @ (x - self.centers[i])\n",
    "            if dist < min_dist:\n",
    "                min_idx = i\n",
    "                min_dist = dist\n",
    "\n",
    "        return min_idx\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_closest_ctr_idx(X[i]) // self.R\n",
    "        return y\n",
    "        \n",
    "\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "mod = LVQ(5, 3, 1e-1, 1e-5, 0.999)\n",
    "mod.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == mod.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == mod.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures\n",
    "\n",
    "Gaussian mixtures can also be thought as a prototype method. Each cluster is a Gaussian with a centroid (mean) and a covariance matrix.  \n",
    "The two steps of the EM algorithm are similar to K-Means:\n",
    "- E-step: Each obsevation is assigned a weight to each cluster, according to far they are to each Gaussian.\n",
    "- M-Step: Each observation contributes to the weighted mean and covariance  for every cluster.  \n",
    "\n",
    "The Gaussian mixture is referred as a soft clustering method, and K-Means as a hard one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest-Neighbor\n",
    "\n",
    "KNN doesn't have any learning procedure, it just stores the whole training set.  \n",
    "At prediction if find the $k$ closest points of $x$, and returns the average for regression, or the class with the highest number of votes for classification.  \n",
    "One example is the Euclidian distance with standardized training set.  \n",
    "\n",
    "with $k=1$, the bias is often low, but the variance is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.9333333333333333\n",
      "test acc: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "class KNNClassifier:\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_pred(X[i])\n",
    "        return y\n",
    "    \n",
    "    def get_pred(self, x):\n",
    "        dists = []\n",
    "        for i in range(len(self.X)):\n",
    "            dists.append(( (X[i] - x) @ (X[i] - x), y[i]))\n",
    "        dists.sort()\n",
    "        \n",
    "        _, vals = zip(*dists[:self.K])\n",
    "        return mode(np.array(vals))[0][0]\n",
    "    \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "mod = KNNClassifier(5)\n",
    "mod.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == mod.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == mod.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some problems, training features are invariant under certain natural transformations, and a KNN classifier can exploit it by using a specific metric.  \n",
    "For example, for image recognition, rotation often doesn't change the class. We wish to remove the effect of rotation when measuring distance between 2 objects of the same class.  \n",
    "Distance between 2 images: shortest euclidian distance between any rotated version of the 2 images.  \n",
    "But it's quite expensive, another solution is the tangent distance.  \n",
    "\n",
    "We compute the invariant tangent line for each training image.  \n",
    "To classify a new image, we compute it's invariant tangent line and find the closest lines to it in the training set.  \n",
    "This tangent can be computed by estimating direction vector from small rotations of the image.  \n",
    "\n",
    "The tangeant distance can also be used to capture other invariance like scaling, translation for example.  \n",
    "\n",
    "Efficient pattern recognition using a new transformation distance - Simard, P., Cun, Y. L. and Denker, J. (1993) - [PDF](https://pdfs.semanticscholar.org/8314/dda1ec43ce57ff877f8f02ed89acb68ca035.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Nearest-Neighbor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high-dimensional spaces, class probabilities might only change in a low dimensional subspace. This can be corrected if we have a metric so that the resulting neighborhoods stretch out in directions for which the feature probabilities doesn't change much.  \n",
    "\n",
    "Discrimininant Adaptive Nearest-neighbor adapt the metric locally.  \n",
    "The metric used to compute the distance at the query point $x_0$ is:\n",
    "$$D(x, x_0) = (x - x_0)^T \\Sigma (x - x_0)$$\n",
    "$$\\text{with } \\Sigma = W^{-1/2}(W^{-1/2}BW^{-1/2} + \\epsilon I)W^{-1/2}$$\n",
    "\n",
    "with $W$ and $B$ respectively the within and between class covariance matrices computed using only the $M$ nearest neighbors around $x_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global dimension reduction can also be applied.  \n",
    "One strategy is to project the data into a reduced subspace, and perform KNN into that subspace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
