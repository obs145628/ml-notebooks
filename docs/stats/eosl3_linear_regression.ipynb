{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual sum of squares\n",
    "\n",
    "$$\\text{RSS}(\\beta) = \\sum_{i=1}^N  (y_i - f(X_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find $\\beta$ to minimize RSS\n",
    "\n",
    "$$X^T(y - X\\beta) = 0$$\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 7.852440105669972\n",
      "coeffs [1.34469402 2.66954109 0.00335914]\n",
      "train error: 0.09013343424315516\n",
      "test error: 0.08305351785720833\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(313, 3) * 2.56 + 3.6\n",
    "y = 1.34 * X[:, 0] + 2.67 * X[:, 1] + 7.89 + 0.3 * np.random.randn(len(X))\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def add_col1(m):\n",
    "    return np.append(np.ones((len(m),1)), m, axis=1)\n",
    "\n",
    "X_train2 = add_col1(X_train)\n",
    "beta = np.linalg.inv(X_train2.T @ X_train2) @ X_train2.T @ y_train\n",
    "\n",
    "print('intercept:', beta[0])\n",
    "print('coeffs', beta[1:])\n",
    "print('train error:', np.mean((y_train - X_train2 @ beta)**2))\n",
    "print('test error:', np.mean((y_test - add_col1(X_test) @ beta)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define $\\sigma^2$ the constant variance of the obervations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Var}(\\hat{\\beta}) = (X^TX)^{-1}\\sigma^2$$\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{N - p - 1} \\sum_{i=1}^N (y_i - \\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\mathbb{E}(Y|X1,\\text{...},X_p) + \\epsilon, \\space \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "$$\\hat{\\beta} \\sim \\mathcal{N}(\\beta, (X^TX)^{-1}\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null hypotesis $\\beta_j = 0$\n",
    "\n",
    "$$z_j = \\frac{\\hat{\\beta}}{\\hat{\\sigma} \\sqrt{v_j}}, \\space v_j = (X^TX)^{-1}_{jj}$$\n",
    "\n",
    "If $|z_j|$ greater than a threshold, the null-hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $P(z > |z_j|)$ is greater than a threshold, the null-hypothesis is rejected.\n",
    "\n",
    "$$P(z > |z_j|) = 2 * (1 - P(z < |z_j|))$$\n",
    "\n",
    "$P(z < |z_j|)$ is the CDF of the t-distribution (can be computed with t-table or software packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for a group of coefficient\n",
    "\n",
    "F-statistic:\n",
    "$$F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1)/(p_1-p_0)}{\\text{RSS}_1/(N - p_1 - 1)}$$\n",
    "\n",
    "- $\\text{RSS}_1$: residual sum of square of complete model with $p_1 + 1$ parameters\n",
    "- $\\text{RSS}_0$: residual sum of square of reduced model with $p_0 + 1$ parameters (without params to be tested for significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard error of parameters\n",
    "\n",
    "$$\\text{se}(\\hat{b_j}) = \\hat{\\sigma} \\sqrt{v_j}$$\n",
    "\n",
    "$1 - 2\\alpha$ confidence interval:\n",
    "$$(\\hat{\\beta_j} - z^{1-\\alpha} \\text{se}(\\hat{b_j}), \\hat{\\beta_j} + z^{1-\\alpha} \\text{se}(\\hat{b_j}))$$\n",
    "\n",
    "$z^{1-\\alpha}$: $1-\\alpha$ percentile of the normal distribution.\n",
    "\n",
    "$$z^{1 - 0.025} = 1.96$$\n",
    "$$z^{1 - 0.05} = 1.645$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute T-Score and P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.518\n",
      "Model:                            OLS   Adj. R-squared:                  0.507\n",
      "Method:                 Least Squares   F-statistic:                     46.27\n",
      "Date:                Fri, 28 Dec 2018   Prob (F-statistic):           3.83e-62\n",
      "Time:                        14:52:44   Log-Likelihood:                -2386.0\n",
      "No. Observations:                 442   AIC:                             4794.\n",
      "Df Residuals:                     431   BIC:                             4839.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        152.1335      2.576     59.061      0.000     147.071     157.196\n",
      "x1           -10.0122     59.749     -0.168      0.867    -127.448     107.424\n",
      "x2          -239.8191     61.222     -3.917      0.000    -360.151    -119.488\n",
      "x3           519.8398     66.534      7.813      0.000     389.069     650.610\n",
      "x4           324.3904     65.422      4.958      0.000     195.805     452.976\n",
      "x5          -792.1842    416.684     -1.901      0.058   -1611.169      26.801\n",
      "x6           476.7458    339.035      1.406      0.160    -189.621    1143.113\n",
      "x7           101.0446    212.533      0.475      0.635    -316.685     518.774\n",
      "x8           177.0642    161.476      1.097      0.273    -140.313     494.442\n",
      "x9           751.2793    171.902      4.370      0.000     413.409    1089.150\n",
      "x10           67.6254     65.984      1.025      0.306     -62.065     197.316\n",
      "==============================================================================\n",
      "Omnibus:                        1.506   Durbin-Watson:                   2.029\n",
      "Prob(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\n",
      "Skew:                           0.017   Prob(JB):                        0.496\n",
      "Kurtosis:                       2.726   Cond. No.                         227.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ds = load_diabetes()\n",
    "X = ds.data\n",
    "y = ds.target\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est = est.fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n",
      " -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n",
      "   67.62538639]\n"
     ]
    }
   ],
   "source": [
    "# Compute coeff\n",
    "\n",
    "X_int = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "b = np.linalg.inv(X_int.T @ X_int) @ X_int.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.65044279e+00 3.57826746e+03 3.75688877e+03 4.43700957e+03\n",
      " 4.28998406e+03 1.74029274e+05 1.15211710e+05 4.52751482e+04\n",
      " 2.61350159e+04 2.96190102e+04 4.36404173e+03]\n"
     ]
    }
   ],
   "source": [
    "# Compute estimate of variance sigma**2\n",
    "\n",
    "preds = X_int @ b\n",
    "varhat = np.sum((y - preds)**2) / (X_int.shape[0] - X_int.shape[1] - 1)\n",
    "varb = varhat * np.diag(np.linalg.inv(X_int.T @ X_int))\n",
    "print(varb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.99287077 -0.16737594 -3.91263721  7.80412709  4.95267912 -1.89895643\n",
      "  1.40455451  0.47487908  1.09526528  4.3653208   1.02368258]\n"
     ]
    }
   ],
   "source": [
    "# Compute the T-Scores\n",
    "\n",
    "ts = b / np.sqrt(varb)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.8671509801832249, 0.0001057031114957141, 4.39648317751562e-14, 1.0445511304801869e-06, 0.05822257652567764, 0.16085771224787582, 0.635108244728211, 0.27399823463429795, 1.583097392376942e-05, 0.3065464256087216]\n"
     ]
    }
   ],
   "source": [
    "# Compute p-values\n",
    "df = X_int.shape[0] - 1\n",
    "ps  =[2*(1-stats.t.cdf(np.abs(x), df)) for x in ts]\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16927935111708448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - stats.f.cdf(1.67, 4, 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.52726834,  58.62224894,  60.06759504,  65.27866411,\n",
       "        64.18801049, 408.82479733, 332.63993512, 208.52398498,\n",
       "       158.43001375, 168.65970875,  64.73967622])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute standard errors\n",
    "\n",
    "(1.96 / 2) * np.sqrt(varb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares estimates of $\\beta$ have the smallest variances among all unbiased estimates.  \n",
    "Biased estimates (eg Ridge regression) are also usefull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + [\\mathbb{E}(\\hat{\\theta}) - \\theta)]^2$$\n",
    "\n",
    "Variance: $\\text{Var}(\\hat{\\theta})$  \n",
    "Bias: $[\\mathbb{E}(\\hat{\\theta}) - \\theta)]^2$  \n",
    "\n",
    "Gauss-Makov theorem implies least squares estimator has the smallest MSE among all unbiased estimators.  \n",
    "But it might exist a biased estimator with a smaller MSE (trade a little bias for larger reduction in variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression by orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\langle x,y \\rangle = x^Ty$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate regression\n",
    "\n",
    "$$y = x\\beta + \\epsilon$$\n",
    "$$\\hat{\\beta} = \\frac{\\langle x,y \\rangle}{\\langle x,x \\rangle}$$\n",
    "\n",
    "Residual:\n",
    "$$r = y - x \\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ data matrix with columns $x_1$, ..., $x_p$.  \n",
    "Suppose $X$ colums are orthogonal: $\\langle x_j,x_k \\rangle = 0 \\space \\forall j\\neq k$\n",
    "$$\\hat{\\beta_j} = \\frac{\\langle x_j,y \\rangle}{\\langle x_j,x_j \\rangle}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression by Successive Orthogonalization\n",
    "\n",
    "For $j = 1, 2, \\text{...}, p$:\n",
    "$$\\hat{\\gamma_{lj}} = \\frac{\\langle z_l,x_j \\rangle}{\\langle z_l,z_l \\rangle}$$\n",
    "$$z_j = x_j - \\sum_{k=0}^{j-1} \\hat{\\gamma_{kj}}z_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_p} = \\frac{\\langle z_p,y \\rangle}{\\langle z_p,z_p \\rangle}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $x_j$ is a linear combination of $z_0$, $z_1$, ..., $z_j$.  \n",
    "All $z_j$ are orthogonal.  \n",
    "Hence all $z_j$ form a basis for the column space of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Var}(\\hat{\\beta_p}) = \\frac{\\sigma^2}{||z_p||^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesive orthogonalization can be represented by:\n",
    "$$X = Z \\Gamma$$  \n",
    "$Z \\in \\mathbb{R}^{n*p}$: columns $z_j$  \n",
    "$\\Gamma \\in \\mathbb{R}^{p*p}$: upper triangular matrix with entries $\\gamma_{kj}$\n",
    "\n",
    "Normalize all $z_j$ by introducting diagonal matrix $D$:\n",
    "$$D_{jj} = ||z_j||$$\n",
    "\n",
    "$$X = Z D^{-1} D \\Gamma$$\n",
    "$$X = QR$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the $QR$ decomposition.  \n",
    "Find orthogonal basis for column space of $X$.  \n",
    "\n",
    "$$\\hat{\\beta} = R^{-1}Q^Ty$$\n",
    "$$\\hat{y} = QQ^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition by Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.081449488321716e-15\n",
      "5.72324159413454e-16\n",
      "0.005083550671442089\n",
      "0.013463898998340329\n"
     ]
    }
   ],
   "source": [
    "def qr(X):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    Q = np.zeros((n, p))\n",
    "    R = np.eye(p)\n",
    "    \n",
    "    for j in range(p):\n",
    "        for l in range(j):\n",
    "            R[l, j] = (Q[:,l] @ X[:,j]) / (Q[:,l] @ Q[:,l])\n",
    "        Q[:,j] = X[:,j]\n",
    "        for k in range(j):\n",
    "            Q[:, j] -= R[k, j] * Q[:, k]\n",
    "        \n",
    "    D = np.zeros(p)\n",
    "    for j in range(p):\n",
    "        D[j] = np.linalg.norm(Q[:,j])\n",
    "    \n",
    "    Q = Q @ np.diag(1/D)\n",
    "    R = np.diag(D) @ R\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "X = np.random.randn(13, 5)\n",
    "y = np.random.randn(13)\n",
    "Q, R = qr(X)\n",
    "bp = Q[:, -1] @ y #no need to divide by <z_j,z_j> (already normalized)\n",
    "print(metrics.tdist(Q @ R, X))\n",
    "print(metrics.tdist(Q.T @ Q, np.eye(X.shape[1])))\n",
    "\n",
    "b = np.linalg.inv(R) @ Q.T @ y\n",
    "\n",
    "\n",
    "print(b[-1])\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple outputs\n",
    "\n",
    "Outputs $Y_1$, $Y_2$, $Y_K$.  \n",
    "One linear model for each output.\n",
    "\n",
    "$$Y_k = \\sum_{j=0}^p X_j \\beta_{jk} + \\epsilon_k$$\n",
    "$$Y = XB + E$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RSS(B) = \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik} - f_k(x_i))^2$$\n",
    "$$\\hat{B} = (X^TX)^{-1})X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only retain a subset of the variables, and eliminate the rest from the model. Coefficients of retained inputs are computer with least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best-Subset Selection\n",
    "\n",
    "Find for $k \\in {0, 1, 2, \\text{...}, p}$ the subset that gives the smallest RSS.  \n",
    "Test all possible subsets, feasible up to $p=30$.  \n",
    "Chose smallest model that mninimizes an estimate of prediction error.  \n",
    "We can use for example cross-validator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward stepwise selection\n",
    "\n",
    "Start with the intercept, and at each step adds to the model the predictor that most improves the fit.  \n",
    "Greedy, suboptimal algorithm.  \n",
    "Computational cost much lower than best-subset, and less variance because the search is more constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward stepwise selection\n",
    "\n",
    "Start with full model, and delete predictor that as least impact on fit (eg: smallest z-score).\n",
    "Contrary to forward, can only be used when  $N > p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Stagewise Regression\n",
    "\n",
    "Start with intercept at $\\bar{y}$, and all other coefficients at 0.  \n",
    "At each step, algorithm identifies the most corelated variable with the current residual, and update it by linear regression.  \n",
    "It can be very slow to find least squares fit, but can be usefull in high-dimensional problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shrink coefficient by adding a penalty on their size\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=0}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "$\\lambda \\geq 0$ is the complexity parameter that controls the amount of shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is equivalent to:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=0}^p (x_{ij}\\beta_j)^2$$\n",
    "$$\\text{subject to} \\sum_{j=1}^p \\beta_{j}^2 \\leq t$$  \n",
    "\n",
    "There is a one-to-one correspondance between $t$ and $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input must be normalized before solving.  \n",
    "$\\beta_0$ is not in the penalty term.  \n",
    "\n",
    "It's possible to center the data first, then apply ridge regression:\n",
    "Each $x_ij$ is replaced by $x_ij$ - $\\bar{x_j}$\n",
    "$$\\beta_0 = \\bar{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{RSS}(\\lambda) = (y - X\\beta)^T(y-X\\beta) + \\lambda \\beta^T\\beta$$\n",
    "$$\\beta = (X^TX + \\lambda I)^{-1} X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.22630102  1.82940796  0.08406004  0.11733299 -0.21012907]\n",
      "7.113856877098789\n",
      "0.06999764974598609\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(13, 5)\n",
    "y = 4.5 * X[:, 0] + 2.1 * X[:, 1] + 5.7 + 0.1 * X[:, 2]**2\n",
    "ld = 0.4\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc \n",
    "y2 = y - yc\n",
    "\n",
    "beta = np.linalg.inv(X2.T @ X2 + ld * np.eye(X.shape[1])) @ X2.T @ y2\n",
    "\n",
    "preds = (X - Xc) @ beta + yc \n",
    "err = np.mean((y - preds) ** 2)\n",
    "print(beta)\n",
    "print(yc)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes the problem non singular, even $X^T X$ is not full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with SVD\n",
    "\n",
    "$$X = U D V^T$$\n",
    "$U \\in \\mathbb{R}^{N*p}$, spans column space of $X$.  \n",
    "$V \\in \\mathbb{R}^{p*p}$, spans row space of $X$.  \n",
    "$D \\in \\mathbb{R}^{p*p}$, diagonal matrix with singular values of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X \\beta = X(X^TX)^{-1}X^Ty$$\n",
    "$$X \\beta = UU^Ty$$\n",
    "$$V^T \\beta = z \\text{, with } z_i = \\frac{u_i^T y}{\\sigma_i}$$\n",
    "\n",
    "$\\beta$ found by solving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.50079410e+00 2.10046285e+00 4.78984296e-04]\n",
      "6.619639848300166\n",
      "0.00010847269231719683\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(117, 3)\n",
    "y = 4.5*X[:, 0] + 2.1*X[:, 1] + 5.7 + np.random.randn(117) * 0.01\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc \n",
    "y2 = y - yc\n",
    "\n",
    "u, s, vt = np.linalg.svd(X2, full_matrices=False)\n",
    "z = u.T @ y2 / s\n",
    "beta = np.linalg.solve(vt, z)\n",
    "\n",
    "preds = (X - Xc) @ beta + yc \n",
    "err = np.mean((y - preds) ** 2)\n",
    "print(beta)\n",
    "print(yc)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X \\beta = X(X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "$$X \\beta = \\sum_{j=1}^p u_j \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}u_j^Ty$$\n",
    "\n",
    "$$ \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\leq 1$$\n",
    "Ridge regression compute the coordinates of $y$ with respect to the orthogonal basis $U$, and shrik it by $\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample covariance matrix $S = \\frac{X^T}{N}$\n",
    "\n",
    "$$X^TX = VD^2V^T$$\n",
    "\n",
    "This is the eigein decomposition of $X^TX$. The eigeinvectors $v_j$ are the principal component directions of $X$\n",
    "\n",
    "$$z_1 = X v_1 = u_1d_1$$\n",
    "\n",
    "is the first principal component of $X$. It has the largest sample variance among all the column space of $X$\n",
    "\n",
    "$$\\text{Var}(z_i) = \\text{Var}(X v_1) = \\frac{\\sigma_1^2}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small singular values correspond to directions in column space of $X$ having small variance, and ridge regression shrink these directions the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective degrees of freedom\n",
    "\n",
    "$$\\text{df}(\\lambda) = \\text{tr} [X(X^TX + \\lambda I)^{-1}X^T]$$\n",
    "$$\\text{df}(\\lambda) = \\text{tr} (H_\\lambda)$$\n",
    "$$\\text{df}(\\lambda) = \\sum_{j=1}^p \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}$$  \n",
    "\n",
    "$$\\lambda = 0 \\implies \\text{df}(\\lambda) = p$$\n",
    "$$\\lim_{\\lambda \\to \\infty} \\text{df}(\\lambda) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\frac{1}{2} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is equivalent to:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2$$\n",
    "$$\\text{subject to} \\sum_{j=1}^p |\\beta_{j}| \\leq t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraint makes the solution non linear, and there is no closed-form solution.  \n",
    "It's a quadratic programming problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making $t$ small enogh cause some of the coefficients to be $0$.  \n",
    "Thus lasso does a kind of continious subset selection.\n",
    "\n",
    "If $t > t_0 = \\sum_{j=1}^p |\\hat{\\beta_j}|$, with $\\hat{\\beta_j}$ coefficients of linear regression, then lasso gives the same coefficient.\n",
    "For $t = t_0/2$, the least squares coefficient are shrunk by 50\\% on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization of Ridge and Lasso\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|^q $$\n",
    "\n",
    "for $q \\geq 0$.\n",
    "\n",
    "- $q = 0$: variable selection\n",
    "- $q = 1$: lasso\n",
    "- $q = 2$: ridge regression\n",
    "\n",
    "Corresponds to Bayes estimates with different priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Penalty\n",
    "\n",
    "$$\\lambda \\sum_{j=1}^p (\\alpha \\beta_j^2 + (1 - \\alpha) |\\beta_j|)$$\n",
    "\n",
    "Compromise between ridge and lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Angle Regression\n",
    "\n",
    "This algorithm is similar to forward stepwise regression.  \n",
    "It starts with 0 variables, and add them one by one, and each time update the parameters with least squares.  \n",
    "But at each entered value, the parameters aren't completely fit, they only moves a little in the direction of least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize the data\n",
    "- Start with an empty set of variables $\\pi$\n",
    "- Repeat p times:\n",
    "    - compute the residual $r = y - X_\\pi\\beta_pi$\n",
    "    - find the variable $X_j$ that is the most correlated with the residual $r$\n",
    "    - $\\pi \\leftarrow \\pi \\cup \\{j\\}$\n",
    "    - Regress $X_\\pi$ onto residual: $\\delta_\\pi = (X_\\pi^T X_\\pi)^{-1} X_\\pi^Tr$\n",
    "    - Find $\\hat{\\gamma}$ and update $\\beta_\\pi$: $\\beta_\\pi \\leftarrow \\beta_\\pi + \\hat{\\gamma} * \\delta_\\pi$\n",
    "    \n",
    "  \n",
    "$$\\hat{\\gamma} = \\arg \\min_\\gamma [ \\text{corr}(r - X_\\pi(\\beta_\\pi + \\gamma \\delta_\\pi), X_j) \\leq \\max_{k \\not \\in \\pi} \\text{corr}(r - X_\\pi(\\beta_\\pi + \\gamma \\delta_\\pi), X_k)]$$\n",
    "\n",
    "For the final step, $X_\\pi$ is completely reduced on the residual $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeffs [ 3.05039469e-01  9.31536074e-01 -3.47679477e-04]\n",
      "error: 0.00013125523077676246\n"
     ]
    }
   ],
   "source": [
    "def lar(X, y):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    used = list()\n",
    "    unused = list(range(p))\n",
    "    \n",
    "    beta = np.zeros((p,))\n",
    "    \n",
    "    def find_gamma(X_a, beta_a, r, d, j):\n",
    "        if not len(unused):\n",
    "            return 1.\n",
    "        \n",
    "        step = 0.001\n",
    "        gamma = 10\n",
    "\n",
    "        while True:\n",
    "            corr_j = np.abs((r - X_a @ (beta_a + gamma * d)) @ X[:, j])\n",
    "            corr_k = np.abs((r - X_a @ (beta_a + gamma * d)) @ X[:, unused])\n",
    "            if corr_j < np.max(corr_k):\n",
    "                break\n",
    "            gamma -= step\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    for i in range(p):\n",
    "        \n",
    "        #compute residual\n",
    "        r = y - X @ beta\n",
    "        \n",
    "        #find x^j, unused variable most correlated with residual \n",
    "        #add j to parameters\n",
    "        corr = r @ X\n",
    "        j = unused[np.argmax(np.abs(corr[unused]))]\n",
    "        used.append(j)\n",
    "        unused.remove(j)\n",
    "        \n",
    "        # Compute the direction to update the used params\n",
    "        X_a = X[:, used]\n",
    "        beta_a = beta[used]\n",
    "        d = np.linalg.inv(X_a.T @ X_a) @ X_a.T @ r\n",
    "        \n",
    "        #Compute the right amounr of update\n",
    "        gamma = find_gamma(X_a, beta_a, r, d, j)\n",
    "        #print('gamma = {}'.format(gamma))\n",
    "        beta[used] += gamma * d\n",
    "    \n",
    "    return beta\n",
    "\n",
    "\n",
    "X = np.random.randn(13, 3) * 2.56 + 3.6\n",
    "y = 1.34 * X[:, 0] + 2.67 * X[:, 1] + 7.89 + 0.01 * np.random.randn(len(X))\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "Xs = np.std(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "ys = np.std(y)\n",
    "X2 = (X - Xc) / Xs\n",
    "y2 = (y - yc) / ys\n",
    "beta = lar(X2, y2)\n",
    "\n",
    "preds = (((X - Xc) / Xs) @ beta) * ys + yc\n",
    "print('coeffs', beta)\n",
    "print('error:', np.mean((y - preds)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modification of the algorithm can be used to perform lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees-of-Freedom for adaptively-fitted model\n",
    "\n",
    "$$\\text{df}(\\hat{y}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\text{Cov}(\\hat{y_i}, y_i)$$\n",
    "\n",
    "$\\text{Cov}(\\hat{y_i}, y_i)$: sampling covariance between predictions and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods using derived input directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input represented by $Z_1$, ..., $Z_M$, with $M$ usually smaller than $p$.\n",
    "Linear regression applied to $Z$ instead of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an orthogonal representation of $X$: $Z \\in \\mathbb{R}^{n * M}$, with $M \\leq p$.  \n",
    "For $M < p$, we get a reducet regression, with the $p - M$ smallest eigenvalue components discarded.  \n",
    "\n",
    "$$X = UDV^T$$\n",
    "$$z_m = X v_m$$\n",
    "\n",
    "We compute the least-square solution of $y$ onto $Z$:\n",
    "$$\\hat{y} = Z \\hat{\\theta}$$\n",
    "\n",
    "Because $Z$ is orthogonal, the solutions are:\n",
    "$$\\hat{\\theta_m} = \\frac{\\langle z_m, y \\rangle}{\\langle z_m, z_m \\rangle}$$\n",
    "\n",
    "We can get back the $\\hat{\\beta}$ coefficients:\n",
    "$$\\hat{y} = X \\hat{\\beta}$$\n",
    "$$\\hat{\\beta} = \\sum_{m=1}^M \\theta_m v_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta [-0.9492278   2.90663683 -1.61006235 -4.36197046 -4.0412172 ]\n",
      "beta [ 1.19998780e+00  4.99949032e-01 -3.09999343e+00  6.00001796e+00\n",
      "  1.18049245e-05]\n",
      "error: 9.380797688911178e-07\n"
     ]
    }
   ],
   "source": [
    "X = 3.4 * np.random.randn(113, 5)**2 + 12.4\n",
    "y = 5.7 + 1.2*X[:,0] + 0.5*X[:,1] -3.1*X[:,2] + 6*X[:, 3] + 0.001*np.random.randn(len(X))\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc\n",
    "y2 = y - yc\n",
    "\n",
    "u, s, vt = np.linalg.svd(X2)\n",
    "\n",
    "Z = X2 @ vt.T\n",
    "\n",
    "theta = (Z.T @ y2) / np.diag(Z.T @ Z)\n",
    "preds = Z @ theta + yc\n",
    "beta = vt.T @ theta\n",
    "\n",
    "\n",
    "print('theta', theta)\n",
    "print('beta', beta)\n",
    "print('error:', np.mean((y - preds)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta [ 3.62788634 -4.11075898 -1.92585975]\n",
      "beta [-0.86760962  0.54589741 -0.07933561  5.7152708   0.21787521]\n",
      "error: 0.014798188997611433\n"
     ]
    }
   ],
   "source": [
    "def linear(X):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    A = 3.5\n",
    "    B = 4.2\n",
    "    \n",
    "    y = np.ones(n) * (A * np.random.randn() + B)\n",
    "    y += 0.001 * np.random.randn(n)\n",
    "    for j in range(p):\n",
    "        y += X[:, i] * (A * np.random.randn() + B)\n",
    "    return y\n",
    "\n",
    "def correl(X, p):\n",
    "    Y = np.empty((len(X), p))\n",
    "    for i in range(p):\n",
    "        Y[:,i] = 0.04 * linear(X)**2 + - 0.3 * linear(X) + 1.4\n",
    "    return Y\n",
    "\n",
    "S = 3.4 * np.random.randn(113, 5)**2 + 12.4\n",
    "X = correl(S, 5)\n",
    "y = 5.7 + 1.2*X[:,0] + 0.5*X[:,1] -3.1*X[:,2] + 6*X[:, 3] + 0.0001*np.random.randn(len(X))\n",
    "\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc\n",
    "y2 = y - yc\n",
    "\n",
    "u, s, vt = np.linalg.svd(X2)\n",
    "V_red = vt.T[:, :3]\n",
    "\n",
    "Z = X2 @ V_red\n",
    "\n",
    "theta = (Z.T @ y2) / np.diag(Z.T @ Z)\n",
    "theta = np.linalg.inv(Z.T @ Z) @ Z.T @ y2\n",
    "preds = Z @ theta + yc\n",
    "beta = V_red @ theta\n",
    "\n",
    "print('theta', theta)\n",
    "print('beta', beta)\n",
    "print('error:', np.mean((y - preds)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares\n",
    "\n",
    "Like Principal Component Regression, this method produces an orthogonal linear combination of $X$ before applying least squares.  \n",
    "But partial least squares also uses $y$.  \n",
    "\n",
    "It builds a matrix $Z \\in \\mathbb{R}^{M*p}$ with orthogonal columns, such that $Z = XV$, and least squares is applied on $Z$ over $y$\n",
    "\n",
    "$M \\leq p$, and when $M = p$, partial least squares find the same coefficients than classical least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize $X$ and $y$\n",
    "- Initialize $x_j^{(0)} = x_j$\n",
    "- For $m = 1 \\to M$:\n",
    "\n",
    "$$\\hat{\\gamma_{mj}} = \\langle x_j^{(m-1)}, y \\rangle$$\n",
    "$$z_m = \\sum_{j=1}^p \\gamma_{mj} x_j^{(m-1)}$$\n",
    "$$x_j^{(m)} = x_j^{(m)} - \\frac{\\langle z_m, x_j^{(m-1)} \\rangle}{\\langle z_m, z_m \\rangle} z_m$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeffs [ 1.33981764e+00  2.67025754e+00 -8.04068394e-05]\n",
      "intercept 7.890109571351328\n",
      "error: 6.019205946091328e-07\n"
     ]
    }
   ],
   "source": [
    "def pls(X, y, m):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    Xk = X.copy()\n",
    "    Z = np.empty((n, m))\n",
    "    \n",
    "    for k in range(m):\n",
    "        g = y @ Xk        \n",
    "        z_k = Xk @ g\n",
    "        \n",
    "        for j in range(p):\n",
    "            v = (z_k @ Xk[:, j]) / (z_k @ z_k)\n",
    "            Xk[:, j] = Xk[:, j] - v * z_k\n",
    "        \n",
    "        Z[:, k] = z_k\n",
    "    \n",
    "    V = np.linalg.pinv(X2) @ Z #need to find way to invert transfs on X\n",
    "    return Z, V\n",
    "\n",
    "def ls_orth(X, y):\n",
    "    beta = np.empty((X.shape[1],))\n",
    "    for i in range(len(beta)):\n",
    "        beta[i] = (X[:, i] @ y) / (X[:, i] @ X[:, i])\n",
    "    return beta\n",
    "    \n",
    "\n",
    "X = np.random.randn(13, 3) * 2.56 + 3.6\n",
    "y = 1.34 * X[:, 0] + 2.67 * X[:, 1] + 7.89 + 0.001 * np.random.randn(len(X))\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "Xs = np.std(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "ys = np.std(y)\n",
    "X2 = (X - Xc) / Xs\n",
    "y2 = (y - yc) / ys\n",
    "\n",
    "Z, V = pls(X2, y2, m=3)\n",
    "theta = ls_orth(Z, y2)\n",
    "preds = (Z @ theta) * ys + yc\n",
    "\n",
    "c1 = V @ theta\n",
    "beta = ys * c1/Xs[0]\n",
    "intercept = - (Xc[0]*ys/Xs[0])@c1 + yc\n",
    "\n",
    "print('coeffs', beta)\n",
    "print('intercept', intercept)\n",
    "print('error:', np.mean((y - preds)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial least squares seeks directictions that have high variance and high correlation with the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Outcome Shrinkage and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Correlation Analysys (CCA)\n",
    "\n",
    "Find a sequence of uncorellated linear combinations $Xv_m$ and another sequence of uncorrelated linear combinations $Yu_m$, $m=1$...$M$.  \n",
    "Such that $\\text{Corr}^2(Yu_m, Xv_m)$ are maximized.  \n",
    "$M \\leq min(K, p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced-Rank Regression (RR)\n",
    "\n",
    "Given an error covariance matrix Cov$(\\epsilon) = \\Sigma$, we solve:\n",
    "\n",
    "$$\\hat{B}^{rr}(m) = \\arg \\min_{\\text{rank}(B)=m} \\sum_{i=1}^n (y_i - B^Tx_i)^T \\Sigma^{-1} (y_i - B^Tx_i)$$\n",
    "\n",
    "When $\\Sigma$ is the sample cross-covariance matrix $\\frac{Y^TX}{N}$, the solution is given by CCA:\n",
    "$$\\hat{B}^{rr}(m) = \\hat{B}U_mU_m^-$$\n",
    "$$\\hat{B}^{rr}(m) =  (X^TX)^{-1}X^TY U_mU_m^-$$\n",
    "\n",
    "with $U_m$ sub-matrix of first $m$ columns of $U$, and $U_m^-$ pseudo inverse of $U_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed Reduced-Rank Regression\n",
    "\n",
    "Shrinkage of the canonical variates between $X$ and $Y$:\n",
    "$\\hat{B}*{c+w} = \\hat{B} U \\Lambda U^{-1}$$\n",
    "\n",
    "$\\Lambda$ diagonal shrinkage matrix with entries $\\lambda_m$, $m=1$, ..., $M$:\n",
    "$$\\lambda_m = \\frac{c_m^2}{c_m^2 + \\frac{p}{N}(1 - c_m^2)}$$\n",
    "\n",
    "$c_m$: $m$-th canonical correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on the Lasso and Related Path Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Forward Stagewise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize all parameters $\\beta_j = 0$\n",
    "- Repeat until the residual is correlated with all predictors:\n",
    "    - Update residual $r = y - X\\beta$\n",
    "    - Let $x_j = \\arg \\max_{x_k} |\\text{corr}(x_k, r)|$\n",
    "    - $\\beta_j \\leftarrow \\beta_j + \\epsilon * \\text{sign}(\\text{corr}(x_j, r))$, with $\\epsilon$ a small step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's similar to Forward Stagewise Regression, except it uses a small step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise-Linear Path Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A piecewise-linear function is a function in different pieces, each begin linear. It may or may not be continous.  \n",
    "Suppose we solve:\n",
    "\n",
    "$$\\hat{\\beta}(\\lambda) = \\arg \\min_{\\beta} [R(\\beta) + \\lambda J(\\beta)]$$\n",
    "$$R(\\beta) = \\sum_{i=1}^N L(y_i, \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j)$$\n",
    "\n",
    "$J(\\beta)$ penalty function and $L(y, \\hat{y})$ loss function, both convex.\n",
    "\n",
    "If:\n",
    "- $R$ is piecewise quadratic in $\\beta$\n",
    "- $J$ is piecewise linear in $\\beta$\n",
    "\n",
    "Then $\\beta(\\lambda)$ is piecewise linear.  \n",
    "The solution path can be in principle easy computed.  \n",
    "Examples are $L_1$ and $L_2$ loss, Huberized loss, hinge loss, $L_1$ and and $L_\\infty$ penality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dantzig Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{\\beta} ||\\beta||_1 \\text{ subject to } ||X^T(y - X\\beta)||_\\infty \\leq s$$\n",
    "\n",
    "It can be written equivalently as:\n",
    "\n",
    "$$\\min_{\\beta} ||X^T(y - X\\beta)||_\\infty \\text{ subject to } ||\\beta||_1  \\leq s$$\n",
    "\n",
    "$L_\\infty$ norm: maximum value of all vector components.  \n",
    "$X^T(y - X\\beta)$ is the MSE gradient for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very similar to the lasso, especially when $t$ gets large:\n",
    "- if $p < N$: they both return the least squares solution.\n",
    "- if $p >= N$: they both return the least squares solution with minimum $L_1$ norm.  \n",
    "\n",
    "For smaller values of $t$, DS differs from lasso.  \n",
    "It can include a variable in a model that has smaller correlation with the residual than some of the excluded variables. This might explains its inferior prediction accuracy compared to lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Grouped Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grouped lasso helps select and shrinks members of groups altogether.  \n",
    "All $p$ predictors are divided into $L$ groups, with $p_l$ the number of predictors in group $l$.  \n",
    "\n",
    "Grouped-lasso mnimizes the critetion:\n",
    "\n",
    "$$\\min_{\\beta} ||y - \\beta_0 - \\sum_{l=1}^L X_l \\beta_l||_2^2 + \\lambda \\sum_{l=1}^L \\sqrt{p_l} ||\\beta_l||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedures encourages sparsity at both the group and the individuals levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further properties of the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso shrinkage may causes the estimates of the non-zero coefficients to be biased toward zero.  \n",
    "One solution is to use lasso to identify the set of non-zero coefficients, then run an unrestricted model to find these coefficients.  \n",
    "Another solution is to use lasso a first time, then a second time on non-zero coefficients only, this is called relaxed lasso. Since there is less competition from noise variables on second run, cross-validation tend to pick a smaller $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothly Clipped Absolute Deviation (SCAD) uses a modified lasso penalty function to that larger coefficients are shrunken less severely.\n",
    "\n",
    "$$\\frac{dJ_a(\\beta, \\lambda)}{d\\beta} = \\lambda \\text{sign}(\\beta)[I(|\\beta| \\leq \\lambda) + \\frac{a \\lambda - |\\beta|}{(a - 1)\\lambda}I(|\\beta|>\\lambda)]$$\n",
    "for some $a \\geq 2$. It reduces the amount of shrinkage for larger values of $\\lambda$, with no shrinkage when $a \\to \\infty$.  \n",
    "But this criterion is not convex, which makes the computation much more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathwise Coordinate Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to fox the $\\lambda$ lasso hyperparameter, and optimize successively over each parameter.\n",
    "\n",
    "$$R(\\tilde{\\beta}(\\lambda), \\beta_j) = \\frac{1}{2} \\sum_{i=1}^N(y_i - \\sum_{k \\neq j} x_{ik} \\tilde{\\beta_k}(\\lambda) - x_{ij} \\beta_j)^2 + \\sum_{k \\neq j} |\\tilde{\\beta_k}(\\lambda)| + \\lambda |\\beta_j| $$\n",
    "\n",
    "with $\\tilde{\\beta_k}(\\lambda)$ the current estimate for $\\beta_k$ at penalty paramter $\\lambda$.  \n",
    "\n",
    "The update is:\n",
    "$$\\tilde{\\beta_j}(\\lambda) \\leftarrow S(\\sum_{i=1}^N x_{ij}(y_i - \\tilde{y}_i^{(j)}, \\lambda)$$\n",
    "\n",
    "with $S(\\hat{\\beta_j}, \\lambda) = \\text{sign}(\\hat{\\beta_j})(|\\hat{\\beta_j}| - \\lambda)_+$.\n",
    "\n",
    "Updating each variable in turn yields the lasso estimate $\\hat{\\beta}(\\lambda)$.  \n",
    "\n",
    "It's possible compute the solutions for a grid of $\\lambda$.  \n",
    "We start with the smallest $\\lambda_{max}$ such that $\\hat{\\beta}(\\lambda_{max}) = 0$.  \n",
    "We decrease $\\lambda$ a little and cycle through the variables until convergence.  \n",
    "Then $\\lambda$ is decresed again the process is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- least squares is usually solver with the Cholesky decomposition of $X^TX$ or the $QR$ factorization of $X$.\n",
    "- Cholesky: $O(p^3 + Np^2)$\n",
    "- QR: $O(Np^3)$\n",
    "- Cholesky is sometimes faster, but is less numerically stable.\n",
    "- Solving Lasso via LAR algorithm is same complexity than least squares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
