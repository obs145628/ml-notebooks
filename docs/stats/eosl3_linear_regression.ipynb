{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual sum of squares\n",
    "\n",
    "$$\\text{RSS}(\\beta) = \\sum_{i=1}^N  (y_i - f(X_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find $\\beta$ to minimize RSS\n",
    "\n",
    "$$X^T(y - X\\beta) = 0$$\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 7.852440105669972\n",
      "coeffs [1.34469402 2.66954109 0.00335914]\n",
      "train error: 0.09013343424315516\n",
      "test error: 0.08305351785720833\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(313, 3) * 2.56 + 3.6\n",
    "y = 1.34 * X[:, 0] + 2.67 * X[:, 1] + 7.89 + 0.3 * np.random.randn(len(X))\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def add_col1(m):\n",
    "    return np.append(np.ones((len(m),1)), m, axis=1)\n",
    "\n",
    "X_train2 = add_col1(X_train)\n",
    "beta = np.linalg.inv(X_train2.T @ X_train2) @ X_train2.T @ y_train\n",
    "\n",
    "print('intercept:', beta[0])\n",
    "print('coeffs', beta[1:])\n",
    "print('train error:', np.mean((y_train - X_train2 @ beta)**2))\n",
    "print('test error:', np.mean((y_test - add_col1(X_test) @ beta)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define $\\sigma^2$ the constant variance of the obervations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Var}(\\hat{\\beta}) = (X^TX)^{-1}\\sigma^2$$\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{N - p - 1} \\sum_{i=1}^N (y_i - \\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\mathbb{E}(Y|X1,\\text{...},X_p) + \\epsilon, \\space \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "$$\\hat{\\beta} \\sim \\mathcal{N}(\\beta, (X^TX)^{-1}\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null hypotesis $\\beta_j = 0$\n",
    "\n",
    "$$z_j = \\frac{\\hat{\\beta}}{\\hat{\\sigma} \\sqrt{v_j}}, \\space v_j = (X^TX)^{-1}_{jj}$$\n",
    "\n",
    "If $|z_j|$ greater than a threshold, the null-hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $P(z > |z_j|)$ is greater than a threshold, the null-hypothesis is rejected.\n",
    "\n",
    "$$P(z > |z_j|) = 2 * (1 - P(z < |z_j|))$$\n",
    "\n",
    "$P(z < |z_j|)$ is the CDF of the t-distribution (can be computed with t-table or software packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for a group of coefficient\n",
    "\n",
    "F-statistic:\n",
    "$$F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1)/(p_1-p_0)}{\\text{RSS}_1/(N - p_1 - 1)}$$\n",
    "\n",
    "- $\\text{RSS}_1$: residual sum of square of complete model with $p_1 + 1$ parameters\n",
    "- $\\text{RSS}_0$: residual sum of square of reduced model with $p_0 + 1$ parameters (without params to be tested for significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard error of parameters\n",
    "\n",
    "$$\\text{se}(\\hat{b_j}) = \\hat{\\sigma} \\sqrt{v_j}$$\n",
    "\n",
    "$1 - 2\\alpha$ confidence interval:\n",
    "$$(\\hat{\\beta_j} - z^{1-\\alpha} \\text{se}(\\hat{b_j}), \\hat{\\beta_j} + z^{1-\\alpha} \\text{se}(\\hat{b_j}))$$\n",
    "\n",
    "$z^{1-\\alpha}$: $1-\\alpha$ percentile of the normal distribution.\n",
    "\n",
    "$$z^{1 - 0.025} = 1.96$$\n",
    "$$z^{1 - 0.05} = 1.645$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute T-Score and P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.518\n",
      "Model:                            OLS   Adj. R-squared:                  0.507\n",
      "Method:                 Least Squares   F-statistic:                     46.27\n",
      "Date:                Fri, 28 Dec 2018   Prob (F-statistic):           3.83e-62\n",
      "Time:                        14:52:44   Log-Likelihood:                -2386.0\n",
      "No. Observations:                 442   AIC:                             4794.\n",
      "Df Residuals:                     431   BIC:                             4839.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        152.1335      2.576     59.061      0.000     147.071     157.196\n",
      "x1           -10.0122     59.749     -0.168      0.867    -127.448     107.424\n",
      "x2          -239.8191     61.222     -3.917      0.000    -360.151    -119.488\n",
      "x3           519.8398     66.534      7.813      0.000     389.069     650.610\n",
      "x4           324.3904     65.422      4.958      0.000     195.805     452.976\n",
      "x5          -792.1842    416.684     -1.901      0.058   -1611.169      26.801\n",
      "x6           476.7458    339.035      1.406      0.160    -189.621    1143.113\n",
      "x7           101.0446    212.533      0.475      0.635    -316.685     518.774\n",
      "x8           177.0642    161.476      1.097      0.273    -140.313     494.442\n",
      "x9           751.2793    171.902      4.370      0.000     413.409    1089.150\n",
      "x10           67.6254     65.984      1.025      0.306     -62.065     197.316\n",
      "==============================================================================\n",
      "Omnibus:                        1.506   Durbin-Watson:                   2.029\n",
      "Prob(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\n",
      "Skew:                           0.017   Prob(JB):                        0.496\n",
      "Kurtosis:                       2.726   Cond. No.                         227.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ds = load_diabetes()\n",
    "X = ds.data\n",
    "y = ds.target\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est = est.fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n",
      " -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n",
      "   67.62538639]\n"
     ]
    }
   ],
   "source": [
    "# Compute coeff\n",
    "\n",
    "X_int = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "b = np.linalg.inv(X_int.T @ X_int) @ X_int.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.65044279e+00 3.57826746e+03 3.75688877e+03 4.43700957e+03\n",
      " 4.28998406e+03 1.74029274e+05 1.15211710e+05 4.52751482e+04\n",
      " 2.61350159e+04 2.96190102e+04 4.36404173e+03]\n"
     ]
    }
   ],
   "source": [
    "# Compute estimate of variance sigma**2\n",
    "\n",
    "preds = X_int @ b\n",
    "varhat = np.sum((y - preds)**2) / (X_int.shape[0] - X_int.shape[1] - 1)\n",
    "varb = varhat * np.diag(np.linalg.inv(X_int.T @ X_int))\n",
    "print(varb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.99287077 -0.16737594 -3.91263721  7.80412709  4.95267912 -1.89895643\n",
      "  1.40455451  0.47487908  1.09526528  4.3653208   1.02368258]\n"
     ]
    }
   ],
   "source": [
    "# Compute the T-Scores\n",
    "\n",
    "ts = b / np.sqrt(varb)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.8671509801832249, 0.0001057031114957141, 4.39648317751562e-14, 1.0445511304801869e-06, 0.05822257652567764, 0.16085771224787582, 0.635108244728211, 0.27399823463429795, 1.583097392376942e-05, 0.3065464256087216]\n"
     ]
    }
   ],
   "source": [
    "# Compute p-values\n",
    "df = X_int.shape[0] - 1\n",
    "ps  =[2*(1-stats.t.cdf(np.abs(x), df)) for x in ts]\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16927935111708448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - stats.f.cdf(1.67, 4, 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.52726834,  58.62224894,  60.06759504,  65.27866411,\n",
       "        64.18801049, 408.82479733, 332.63993512, 208.52398498,\n",
       "       158.43001375, 168.65970875,  64.73967622])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute standard errors\n",
    "\n",
    "(1.96 / 2) * np.sqrt(varb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares estimates of $\\beta$ have the smallest variances among all unbiased estimates.  \n",
    "Biased estimates (eg Ridge regression) are also usefull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + [\\mathbb{E}(\\hat{\\theta}) - \\theta)]^2$$\n",
    "\n",
    "Variance: $\\text{Var}(\\hat{\\theta})$  \n",
    "Bias: $[\\mathbb{E}(\\hat{\\theta}) - \\theta)]^2$  \n",
    "\n",
    "Gauss-Makov theorem implies least squares estimator has the smallest MSE among all unbiased estimators.  \n",
    "But it might exist a biased estimator with a smaller MSE (trade a little bias for larger reduction in variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression by orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\langle x,y \\rangle = x^Ty$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate regression\n",
    "\n",
    "$$y = x\\beta + \\epsilon$$\n",
    "$$\\hat{\\beta} = \\frac{\\langle x,y \\rangle}{\\langle x,x \\rangle}$$\n",
    "\n",
    "Residual:\n",
    "$$r = y - x \\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ data matrix with columns $x_1$, ..., $x_p$.  \n",
    "Suppose $X$ colums are orthogonal: $\\langle x_j,x_k \\rangle = 0 \\space \\forall j\\neq k$\n",
    "$$\\hat{\\beta_j} = \\frac{\\langle x_j,y \\rangle}{\\langle x_j,x_j \\rangle}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression by Successive Orthogonalization\n",
    "\n",
    "For $j = 1, 2, \\text{...}, p$:\n",
    "$$\\hat{\\gamma_{lj}} = \\frac{\\langle z_l,x_j \\rangle}{\\langle z_l,z_l \\rangle}$$\n",
    "$$z_j = x_j - \\sum_{k=0}^{j-1} \\hat{\\gamma_{kj}}z_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_p} = \\frac{\\langle z_p,y \\rangle}{\\langle z_p,z_p \\rangle}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $x_j$ is a linear combination of $z_0$, $z_1$, ..., $z_j$.  \n",
    "All $z_j$ are orthogonal.  \n",
    "Hence all $z_j$ form a basis for the column space of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Var}(\\hat{\\beta_p}) = \\frac{\\sigma^2}{||z_p||^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesive orthogonalization can be represented by:\n",
    "$$X = Z \\Gamma$$  \n",
    "$Z \\in \\mathbb{R}^{n*p}$: columns $z_j$  \n",
    "$\\Gamma \\in \\mathbb{R}^{p*p}$: upper triangular matrix with entries $\\gamma_{kj}$\n",
    "\n",
    "Normalize all $z_j$ by introducting diagonal matrix $D$:\n",
    "$$D_{jj} = ||z_j||$$\n",
    "\n",
    "$$X = Z D^{-1} D \\Gamma$$\n",
    "$$X = QR$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the $QR$ decomposition.  \n",
    "Find orthogonal basis for column space of $X$.  \n",
    "\n",
    "$$\\hat{\\beta} = R^{-1}Q^Ty$$\n",
    "$$\\hat{y} = QQ^ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition by Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.081449488321716e-15\n",
      "5.72324159413454e-16\n",
      "0.005083550671442089\n",
      "0.013463898998340329\n"
     ]
    }
   ],
   "source": [
    "def qr(X):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    Q = np.zeros((n, p))\n",
    "    R = np.eye(p)\n",
    "    \n",
    "    for j in range(p):\n",
    "        for l in range(j):\n",
    "            R[l, j] = (Q[:,l] @ X[:,j]) / (Q[:,l] @ Q[:,l])\n",
    "        Q[:,j] = X[:,j]\n",
    "        for k in range(j):\n",
    "            Q[:, j] -= R[k, j] * Q[:, k]\n",
    "        \n",
    "    D = np.zeros(p)\n",
    "    for j in range(p):\n",
    "        D[j] = np.linalg.norm(Q[:,j])\n",
    "    \n",
    "    Q = Q @ np.diag(1/D)\n",
    "    R = np.diag(D) @ R\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "X = np.random.randn(13, 5)\n",
    "y = np.random.randn(13)\n",
    "Q, R = qr(X)\n",
    "bp = Q[:, -1] @ y #no need to divide by <z_j,z_j> (already normalized)\n",
    "print(metrics.tdist(Q @ R, X))\n",
    "print(metrics.tdist(Q.T @ Q, np.eye(X.shape[1])))\n",
    "\n",
    "b = np.linalg.inv(R) @ Q.T @ y\n",
    "\n",
    "\n",
    "print(b[-1])\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple outputs\n",
    "\n",
    "Outputs $Y_1$, $Y_2$, $Y_K$.  \n",
    "One linear model for each output.\n",
    "\n",
    "$$Y_k = \\sum_{j=0}^p X_j \\beta_{jk} + \\epsilon_k$$\n",
    "$$Y = XB + E$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RSS(B) = \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik} - f_k(x_i))^2$$\n",
    "$$\\hat{B} = (X^TX)^{-1})X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only retain a subset of the variables, and eliminate the rest from the model. Coefficients of retained inputs are computer with least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-Subset Selection\n",
    "\n",
    "Find for $k \\in {0, 1, 2, \\text{...}, p}$ the subset that gives the smallest RSS.  \n",
    "Test all possible subsets, feasible up to $p=30$.  \n",
    "Chose smallest model that mninimizes an estimate of prediction error.  \n",
    "We can use for example cross-validator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise selection\n",
    "\n",
    "Start with the intercept, and at each each adds to the model the predictor that most improves the fit.  \n",
    "Greedy, suboptimal algorithm.  \n",
    "Computational cost much lower than best-subset, and less variance because the search is more constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise selection\n",
    "\n",
    "Start with full model, and delete predictor that as least impact on fit (eg: smallest z-score).\n",
    "Contrary to forward, can only be used when  $N > p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stagewise Regression\n",
    "\n",
    "Start with intercept at $\\bar{y}$, and all other coefficients at 0.  \n",
    "At each step, algorithm identifies the most corelated variable with the current residual, and update it by linear regression.  \n",
    "It can be very slow to find least squares fit, but can be usefull in high-dimensional problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shrink coefficient by adding a penalty on their size\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=0}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "$\\lambda \\geq 0$ is the complexity parameter that controls the amount of shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is equivalent to:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=0}^p (x_{ij}\\beta_j)^2$$\n",
    "$$\\text{subject to} \\sum_{j=1}^p \\beta_{j}^2 \\leq t$$  \n",
    "\n",
    "There is a one-to-one correspondance between $t$ and $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input must be normalized before solving.  \n",
    "$\\beta_0$ is not in the penalty term.  \n",
    "\n",
    "It's possible to center the data first, then apply ridge regression:\n",
    "Each $x_ij$ is replaced by $x_ij$ - $\\bar{x_j}$\n",
    "$$\\beta_0 = \\bar{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{RSS}(\\lambda) = (y - X\\beta)^T(y-X\\beta) + \\lambda \\beta^T\\beta$$\n",
    "$$\\beta = (X^TX + \\lambda I)^{-1} X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.22630102  1.82940796  0.08406004  0.11733299 -0.21012907]\n",
      "7.113856877098789\n",
      "0.06999764974598609\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(13, 5)\n",
    "y = 4.5 * X[:, 0] + 2.1 * X[:, 1] + 5.7 + 0.1 * X[:, 2]**2\n",
    "ld = 0.4\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc \n",
    "y2 = y - yc\n",
    "\n",
    "beta = np.linalg.inv(X2.T @ X2 + ld * np.eye(X.shape[1])) @ X2.T @ y2\n",
    "\n",
    "preds = (X - Xc) @ beta + yc \n",
    "err = np.mean((y - preds) ** 2)\n",
    "print(beta)\n",
    "print(yc)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes the problem non singular, even $X^X X$ is not full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with SVD\n",
    "\n",
    "$$X = U D V^T$$\n",
    "$U \\in \\mathbb{R}^{N*p}$, spans column space of $X$.  \n",
    "$V \\in \\mathbb{R}^{p*p}$, spans row space of $X$.  \n",
    "$D \\in \\mathbb{R}^{p*p}$, diagonal matrix with singular values of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X \\beta = X(X^TX)^{-1}X^Ty$$\n",
    "$$X \\beta = UU^Ty$$\n",
    "$$V^T \\beta = z \\text{, with } z_i = \\frac{u_i^T y}{\\sigma_i}$$\n",
    "\n",
    "$\\beta$ found by solving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.50079410e+00 2.10046285e+00 4.78984296e-04]\n",
      "6.619639848300166\n",
      "0.00010847269231719683\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(117, 3)\n",
    "y = 4.5*X[:, 0] + 2.1*X[:, 1] + 5.7 + np.random.randn(117) * 0.01\n",
    "\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "X2 = X - Xc \n",
    "y2 = y - yc\n",
    "\n",
    "u, s, vt = np.linalg.svd(X2, full_matrices=False)\n",
    "z = u.T @ y2 / s\n",
    "beta = np.linalg.solve(vt, z)\n",
    "\n",
    "preds = (X - Xc) @ beta + yc \n",
    "err = np.mean((y - preds) ** 2)\n",
    "print(beta)\n",
    "print(yc)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X \\beta = X(X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "$$X \\beta = \\sum_{j=1}^p u_j \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}u_j^Ty$$\n",
    "\n",
    "$$ \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\leq 1$$\n",
    "Ridge regression compute the coordinates of $y$ with respect to the orthogonal basis $U$, and shrik it by $\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample covariance matrix $S = \\frac{X^T}{N}$\n",
    "\n",
    "$$X^TX = VD^2V^T$$\n",
    "\n",
    "This is the eigein decomposition of $X^TX$. The eigeinvectors $v_j$ are the principal component directions of $X$\n",
    "\n",
    "$$z_1 = X v_1 = u_1d_1$$\n",
    "\n",
    "is the first principal component of $X$. It has the largest sample variance among all the column space of $X$\n",
    "\n",
    "$$\\text{Var}(z_i) = \\text{Var}(X v_1) = \\frac{\\sigma_1^2}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small singular values correspond to directions in column space of $X$ having small variance, and ridge regression shrink these directions the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective degrees of freedom\n",
    "\n",
    "$$\\text{df}(\\lambda) = \\text{tr} [X(X^TX + \\lambda I)^{-1}X^T]$$\n",
    "$$\\text{df}(\\lambda) = \\text{tr} (H_\\lambda)$$\n",
    "$$\\text{df}(\\lambda) = \\sum_{j=1}^p \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}$$  \n",
    "\n",
    "$$\\lambda = 0 \\implies \\text{df}(\\lambda) = p$$\n",
    "$$\\lim_{\\lambda \\to \\infty} \\text{df}(\\lambda) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\frac{1}{2} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is equivalent to:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2$$\n",
    "$$\\text{subject to} \\sum_{j=1}^p |\\beta_{j}| \\leq t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraint makes the solution non linear, and there is no closed-form solution.  \n",
    "It's a quadratic programming problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making $t$ small enogh cause some of the coefficients to be $0$.  \n",
    "Thus lasso does a kind of continious subset selection.\n",
    "\n",
    "If $t > t_0 = \\sum_{j=1}^p |\\hat{\\beta_j}|$, with $\\hat{\\beta_j}$ coefficients of linear regression, then lasso gives the same coefficient.\n",
    "For $t = t_0/2$, the least squares coefficient are shrunk by 50\\% on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization of Ridge and Lasso\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^p (x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|^q $$\n",
    "\n",
    "for $q \\geq 0$.\n",
    "\n",
    "- $q = 0$: variable selection\n",
    "- $q = 1$: lasso\n",
    "- $q = 2$: ridge regression\n",
    "\n",
    "Corresponds to Bayes estimates with different priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Penalty\n",
    "\n",
    "$$\\lambda \\sum_{j=1}^p (\\alpha \\beta_j^2 + (1 - \\alpha) |\\beta_j|)$$\n",
    "\n",
    "Compromise between ridge and lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeffs [ 6.23402126e-01  7.15671519e-01 -2.47114517e-04]\n",
      "error: 2.4735593116935953e-05\n"
     ]
    }
   ],
   "source": [
    "def lar(X, y):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    used = list()\n",
    "    unused = list(range(p))\n",
    "    \n",
    "    beta = np.zeros((p,))\n",
    "    \n",
    "    def find_gamma(X_a, beta_a, r, d, j):\n",
    "        if not len(unused):\n",
    "            return 1.\n",
    "        \n",
    "        step = 0.001\n",
    "        gamma = 0\n",
    "\n",
    "        while True:\n",
    "            corr_j = np.abs((r - X_a @ (beta_a + gamma * d)) @ X[:, j])\n",
    "            corr_k = np.abs((r - X_a @ (beta_a + gamma * d)) @ X[:, unused])\n",
    "            if corr_j < np.max(corr_k):\n",
    "                break\n",
    "            gamma += step\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    for i in range(p):\n",
    "        \n",
    "        #compute residual\n",
    "        r = y - X @ beta\n",
    "        \n",
    "        #find x^j, unused variable most correlated with residual \n",
    "        #add j to parameters\n",
    "        corr = r @ X\n",
    "        j = unused[np.argmax(np.abs(corr[unused]))]\n",
    "        used.append(j)\n",
    "        unused.remove(j)\n",
    "        \n",
    "        X_a = X[:, used]\n",
    "        beta_a = beta[used]\n",
    "        d = np.linalg.inv(X_a.T @ X_a) @ X_a.T @ r\n",
    "        \n",
    "        gamma = find_gamma(X_a, beta_a, r, d, j)\n",
    "        \n",
    "        beta[used] += gamma * d\n",
    "    \n",
    "    return beta\n",
    "\n",
    "\n",
    "X = np.random.randn(13, 3) * 2.56 + 3.6\n",
    "y = 1.34 * X[:, 0] + 2.67 * X[:, 1] + 7.89 + 0.01 * np.random.randn(len(X))\n",
    "Xc = np.mean(X, axis=0, keepdims=True)\n",
    "Xs = np.std(X, axis=0, keepdims=True)\n",
    "yc = np.mean(y)\n",
    "ys = np.std(y)\n",
    "X2 = (X - Xc) / Xs\n",
    "y2 = (y - yc) / ys\n",
    "beta = lar(X2, y2)\n",
    "\n",
    "preds = (((X - Xc) / Xs) @ beta) * ys + yc\n",
    "print('coeffs', beta)\n",
    "print('error:', np.mean((y - preds)**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
