{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import revdiff as rd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse-mode Autodifferientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y} = \\sum_{x_i \\in preds(E)} \\frac{\\partial E}{\\partial x_i} * \\frac{\\partial x_i}{\\partial y}$$\n",
    "$$\\frac{\\partial E}{\\partial x} = \\sum_{y_i \\in succs(x)} \\frac{\\partial y_i}{\\partial x} * \\frac{\\partial E}{\\partial y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to compute $\\frac{\\partial E}{\\partial x}$ ?\n",
    "\n",
    "1) Test begin: if $x == E$, just return $1$\n",
    "\n",
    "2) let $dx$ = ones($x$.shape)\n",
    "\n",
    "3) For each succesor $y_i$:  \n",
    "---- 1) Compute recursively $\\frac{\\partial E}{\\partial y_i}$.  \n",
    "---- 2) Call a node specific function that computes grad_yi = $\\frac{\\partial y_i}{\\partial x} * \\frac{\\partial E}{\\partial y_i}$ optimally from $x$ and $\\frac{\\partial E}{\\partial y_i}$.  \n",
    "---- 3) $dx +=$ grad_yi  \n",
    "\n",
    "4) returns $dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut: if $x$ is not a predecessor (recursive) of $E$, the gradient is $0$.  \n",
    "The same idea can be applied for each succesor $y_i$ of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = rd.build_val(2)\n",
    "b = rd.build_val(7)\n",
    "c = rd.build_val(3.5)\n",
    "\n",
    "x = a * b\n",
    "y = x / c - 2 * -a\n",
    "print(metrics.tdist(14, x.eval()))\n",
    "print(metrics.tdist(14 / 3.5 + 4, y.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "IN_SIZE = 28 * 28\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 256\n",
    "OUT_SIZE = 10\n",
    "\n",
    "NEPOCHS = 5\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader, test_loader = utils.load_mnist(BATCH_SIZE)\n",
    "train_loader_01, test_loader_01 = utils.load_mnist_01(BATCH_SIZE)\n",
    "\n",
    "def compute_accuracy(y_preds, y):\n",
    "    total = len(y_preds)\n",
    "    correct = np.equal(y_preds, y).sum()\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TNet, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = torch.nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = torch.nn.Linear(HIDDEN2_SIZE, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IN_SIZE)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        y_logits = self.l3(x).view(-1)\n",
    "        return y_logits\n",
    "\n",
    "\n",
    "tnet = TNet()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNet(rd.Network):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = self.dense_layer(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = self.dense_layer(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = self.dense_layer(HIDDEN2_SIZE, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rd.build_reshape(x, (-1, IN_SIZE))\n",
    "        x = rd.build_vrelu(self.l1(x))\n",
    "        x = rd.build_vrelu(self.l2(x))\n",
    "        y_logits = self.l3(x)\n",
    "        y_logits = rd.build_reshape(y_logits, (-1,))\n",
    "        return y_logits\n",
    "        \n",
    "dnet = DNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06461529 -0.0060272  -0.04025982 -0.01614732 -0.04020036 -0.06330499\n",
      " -0.06153796 -0.05931276 -0.057026    0.00039813]\n",
      "[-0.06461532 -0.00602722 -0.04025982 -0.01614732 -0.04020034 -0.06330499\n",
      " -0.06153798 -0.05931275 -0.05702603  0.00039811]\n",
      "1.7751653e-07\n",
      "45.021507\n",
      "45.021507\n",
      "0.0\n",
      "Checking gradients\n",
      "3.1160503e-06\n",
      "2.2359293e-07\n",
      "5.871506e-06\n",
      "3.5282864e-07\n",
      "4.311466e-06\n",
      "4.7683716e-07\n"
     ]
    }
   ],
   "source": [
    "X_sample = None\n",
    "y_sample = None\n",
    "\n",
    "for x, y in train_loader_01:\n",
    "    X_sample = x.data.numpy()\n",
    "    y_sample = y.data.numpy()\n",
    "    break\n",
    "    \n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    dnet.params_[i].update(tparams[i].data.numpy().T)\n",
    "    \n",
    "tX = torch.tensor(X_sample)\n",
    "ty = torch.tensor(y_sample).type(torch.float32)\n",
    "ty_logits = tnet(tX)\n",
    "tloss = criterion(ty_logits, ty)\n",
    "tnet.zero_grad()\n",
    "tloss.backward()\n",
    "\n",
    "dX = rd.build_val(X_sample)\n",
    "dy = rd.build_val(y_sample)\n",
    "dy_logits = dnet(dX)\n",
    "dloss = rd.build_bce_loss(dy_logits, dy)\n",
    "\n",
    "\n",
    "print(ty_logits.data.numpy()[:10].T)\n",
    "print(dy_logits.eval()[:10].T)\n",
    "print(metrics.tdist(ty_logits.data.numpy(), dy_logits.eval()))\n",
    "print(tloss.data.numpy())\n",
    "print(dloss.eval())\n",
    "print(metrics.tdist(tloss.data.numpy(), dloss.eval()))\n",
    "\n",
    "print('Checking gradients')\n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    grad = rd.build_node_grad(dloss, dnet.params_[i]).eval()\n",
    "    grad_sol = tparams[i].grad.data.numpy().T\n",
    "    print(metrics.tdist(grad, grad_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TNet, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = torch.nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = torch.nn.Linear(HIDDEN2_SIZE, OUT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IN_SIZE)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        y_logits = self.l3(x)\n",
    "        return y_logits\n",
    "\n",
    "\n",
    "tnet = TNet()\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNet(rd.Network):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = self.dense_layer(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = self.dense_layer(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = self.dense_layer(HIDDEN2_SIZE, OUT_SIZE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = rd.build_reshape(x, (-1, IN_SIZE))\n",
    "        x = rd.build_vrelu(self.l1(x))\n",
    "        x = rd.build_vrelu(self.l2(x))\n",
    "        y_logits = self.l3(x)\n",
    "        return y_logits\n",
    "        \n",
    "dnet = DNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9710405e-07\n",
      "147.31425\n",
      "147.31424\n",
      "1.5258789e-05\n",
      "Checking gradients\n",
      "3.6772453e-06\n",
      "2.6905576e-07\n",
      "5.310021e-06\n",
      "4.73366e-07\n",
      "4.2969423e-06\n",
      "1.2828853e-06\n"
     ]
    }
   ],
   "source": [
    "X_sample = None\n",
    "y_sample = None\n",
    "\n",
    "for x, y in train_loader:\n",
    "    X_sample = x.data.numpy()\n",
    "    y_sample = y.data.numpy()\n",
    "    break\n",
    "    \n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    dnet.params_[i].update(tparams[i].data.numpy().T)\n",
    "    \n",
    "tX = torch.tensor(X_sample)\n",
    "ty = torch.tensor(y_sample)\n",
    "ty_logits = tnet(tX)\n",
    "tloss = criterion(ty_logits, ty)\n",
    "tnet.zero_grad()\n",
    "tloss.backward()\n",
    "\n",
    "dX = rd.build_val(X_sample)\n",
    "dy = rd.build_val(utils.vec2one_hot(y_sample, 10))\n",
    "dy_logits = dnet(dX)\n",
    "dloss = rd.build_cross_entropy_loss(dy_logits, dy)\n",
    "\n",
    "\n",
    "print(metrics.tdist(ty_logits.data.numpy(), dy_logits.eval()))\n",
    "print(tloss.data.numpy())\n",
    "print(dloss.eval())\n",
    "print(metrics.tdist(tloss.data.numpy(), dloss.eval()))\n",
    "\n",
    "print('Checking gradients')\n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    grad = rd.build_node_grad(dloss, dnet.params_[i]).eval()\n",
    "    grad_sol = tparams[i].grad.data.numpy().T\n",
    "    print(metrics.tdist(grad, grad_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tnet = TNet()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNet(rd.Network):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = self.conv2d_layer(1, 6, 5, 5)\n",
    "        self.conv2 = self.conv2d_layer(6, 16, 5, 5)\n",
    "        self.fc = self.dense_layer(16 * 5 * 5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rd.build_vrelu(self.conv1(x))\n",
    "        x = rd.build_max_pooling(x, 2, 2, 2, 2)\n",
    "        \n",
    "        x = rd.build_vrelu(self.conv2(x))\n",
    "        x = rd.build_max_pooling(x, 2, 2, 2, 2)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = rd.build_reshape(x, (x.shape[0], -1))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dnet = DNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0470651e-07\n",
      "15.654362\n",
      "15.654361724853516\n",
      "0.0\n",
      "Checking gradients\n",
      "2.9476935e-06\n",
      "6.2956303e-07\n",
      "4.8972583e-06\n",
      "4.3136222e-07\n",
      "2.9718476e-06\n",
      "2.6656008e-07\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 1, 32, 32).astype(np.float32)\n",
    "y = np.random.randn(3, 4).astype(np.float32)\n",
    "    \n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    \n",
    "    if len(tparams[i].shape) == 2:\n",
    "        dnet.params_[i].update(tparams[i].data.numpy().T)\n",
    "    else:\n",
    "        dnet.params_[i].update(tparams[i].data.numpy())\n",
    "    \n",
    "tX = torch.tensor(X)\n",
    "ty = torch.tensor(y)\n",
    "ty_logits = tnet(tX)\n",
    "tloss = criterion(ty_logits, ty)\n",
    "tnet.zero_grad()\n",
    "tloss.backward()\n",
    "\n",
    "dX = rd.build_val(X)\n",
    "dy = rd.build_val(y)\n",
    "dy_logits = dnet(dX)\n",
    "dloss = rd.op_mse_loss(dy_logits, dy)\n",
    "\n",
    "\n",
    "print(metrics.tdist(ty_logits.data.numpy(), dy_logits.eval()))\n",
    "print(tloss.data.numpy())\n",
    "print(dloss.eval())\n",
    "print(metrics.tdist(tloss.data.numpy(), dloss.eval()))\n",
    "\n",
    "\n",
    "print('Checking gradients')\n",
    "tparams = list(tnet.parameters())\n",
    "for i in range(len(tparams)):\n",
    "    grad = rd.build_node_grad(dloss, dnet.params_[i]).eval()\n",
    "    \n",
    "    if len(tparams[i].shape) == 2:\n",
    "        grad_sol = tparams[i].grad.data.numpy().T\n",
    "    else:\n",
    "        grad_sol = tparams[i].grad.data.numpy()\n",
    "    print(metrics.tdist(grad, grad_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
