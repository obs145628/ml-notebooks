{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "Let $X$ training input of size $n * p$.  \n",
    "It contains $n$ examples, each with $p$ features.  \n",
    "Let $y$ training target of size $n$.  \n",
    "Each input $X_i$, vector of size $p$, is associated with it's target, $y_i$, which is $0$ or $1$.  \n",
    "Logistic regression tries to fit a linear model to predict the target $y$ of a new input vector $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are denoted $\\hat{y}$.\n",
    "$$o_i = X_i\\beta = \\sum_{j=1}^{p} X_{ij}\\beta_j$$\n",
    "$$P(y_i = 1 | X_i) = \\hat{y_i} = \\sigma(o_i)$$\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "The cost function is the cross-entropy.  \n",
    "$$J(\\beta) = - \\sum_{i=1}^n (y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\beta)}{\\partial \\hat{y_i}} = \\frac{\\hat{y_i} - y_i}{\\hat{y_i}(1 - \\hat{y_i})}$$\n",
    "$$\\frac{\\partial J(\\beta)}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.678722\n",
      "10.678722\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "y_out = np.random.randn(13).astype(np.float32)\n",
    "y_true = np.random.randint(0, 2, (13)).astype(np.float32)\n",
    "y_pred = sigmoid(y_out)\n",
    "j = - np.sum(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "\n",
    "ty_true = torch.tensor(y_true, requires_grad=False)\n",
    "ty_pred = torch.tensor(y_pred, requires_grad=True)\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "tj = criterion(ty_pred, ty_true)\n",
    "tj.backward()\n",
    "\n",
    "print(j)\n",
    "print(tj.data.numpy())\n",
    "print(metrics.tdist(j, tj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6231388 -2.9766939  2.274354  -6.4779763 -1.4708843  1.2155157\n",
      " -1.9948862  1.8867183  1.4462028 18.669147   1.5500078 -1.6234685\n",
      " -1.3342199]\n",
      "[-1.6231389 -2.976694   2.274354  -6.477976  -1.4708843  1.2155157\n",
      " -1.9948862  1.8867184  1.4462028 18.669147   1.5500077 -1.6234685\n",
      " -1.3342199]\n",
      "5.717077e-07\n"
     ]
    }
   ],
   "source": [
    "dy_pred = (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "tdy_pred_sol = ty_pred.grad.data.numpy()\n",
    "print(dy_pred)\n",
    "print(tdy_pred_sol)\n",
    "print(metrics.tdist(dy_pred, tdy_pred_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\beta)}{\\partial o_i} = \\hat{y_i} - y_i$$\n",
    "$$\\frac{\\partial J(\\beta)}{\\partial o} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.849605\n",
      "10.849605\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "y_out = np.random.randn(13).astype(np.float32)\n",
    "y_true = np.random.randint(0, 2, (13)).astype(np.float32)\n",
    "y_pred = sigmoid(y_out)\n",
    "j = - np.sum(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "\n",
    "ty_true = torch.tensor(y_true, requires_grad=False)\n",
    "ty_out = torch.tensor(y_out, requires_grad=True)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "tj = criterion(ty_out, ty_true)\n",
    "tj.backward()\n",
    "\n",
    "print(j)\n",
    "print(tj.data.numpy())\n",
    "print(metrics.tdist(j, tj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7712122   0.5310385  -0.7378207  -0.13447696  0.20648097  0.28622478\n",
      " -0.7465389   0.5608791   0.53383535 -0.75912154 -0.4418677   0.6848638\n",
      "  0.35961235]\n",
      "[-0.7712122   0.5310385  -0.7378207  -0.13447696  0.20648097  0.28622478\n",
      " -0.7465389   0.5608791   0.53383535 -0.75912154 -0.4418677   0.6848638\n",
      "  0.35961235]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dy_out = y_pred - y_true\n",
    "dy_out_sol = ty_out.grad.data.numpy()\n",
    "print(dy_out)\n",
    "print(dy_out_sol)\n",
    "print(metrics.tdist(dy_out, dy_out_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be trained with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Error = 71.14744133609668\n",
      "SGD Error = 49.65028785288255\n",
      "SGD Error = 48.91772028291884\n",
      "SGD Error = 48.888462052036814\n",
      "SGD Error = 48.88680421514018\n",
      "SGD Error = 48.88669058552164\n",
      "SGD Error = 48.88668168135676\n",
      "SGD Error = 48.886680916022215\n",
      "SGD Error = 48.88668084643879\n",
      "SGD Error = 48.88668083991474\n",
      "SGD Error = 48.886680839293305\n",
      "SGD Error = 48.88668083923365\n",
      "SGD Error = 48.8866808392279\n",
      "SGD Error = 48.886680839227346\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922729\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiw/rep/ml-notebooks/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SGD Error = 48.88668083922728\n",
      "SK Error = 48.89414553261788\n",
      "SGD Error = 48.88668083922728\n",
      "[ 0.26431092 -0.29452546 -0.21100844 -0.03313959]\n",
      "[ 0.28791843 -0.31416328 -0.22456349 -0.03571747]\n"
     ]
    }
   ],
   "source": [
    "def log_reg_sk(X, y):\n",
    "    \n",
    "    m = LogisticRegression(fit_intercept=False)\n",
    "    m.fit(X, y)\n",
    "    return m.coef_\n",
    "\n",
    "def get_error(X, y, w):\n",
    "    y_pred = sigmoid(X @ w)\n",
    "    err = - np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))\n",
    "    return err\n",
    "\n",
    "def log_reg(X, y):\n",
    "\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        \n",
    "        y_pred = sigmoid(X @ w)\n",
    "        dy_out = y_pred - y\n",
    "        dw = X.T @ dy_out\n",
    "        \n",
    "        w -= 0.001 * dw\n",
    "        if epoch % 100 == 0:\n",
    "            err = get_error(X, y, w)\n",
    "            print('SGD Error = {}'.format(err))\n",
    "        \n",
    "    return w\n",
    "    \n",
    "\n",
    "\n",
    "X = np.random.randn(73, 4).astype(np.float32)\n",
    "y = np.random.randint(0, 2, (73)).astype(np.float32)\n",
    "\n",
    "    \n",
    "w1 = log_reg_sk(X, y)[0]\n",
    "w2 = log_reg(X, y)\n",
    "print('SK Error = {}'.format(get_error(X, y, w1)))\n",
    "print('SGD Error = {}'.format(get_error(X, y, w2)))\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_e = np.exp(x)\n",
    "    return x_e / np.sum(x_e, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.84998\n",
      "148.85\n",
      "3.0517578e-05\n"
     ]
    }
   ],
   "source": [
    "y_out = np.random.randn(93, 4).astype(np.float32)\n",
    "y_true = np.zeros((93, 4)).astype(np.float32)\n",
    "for i in range(y_true.shape[0]):\n",
    "    y_true[i][np.random.randint(0, y_true.shape[1])] = 1\n",
    "y_pred = softmax(y_out)\n",
    "\n",
    "j = - np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "ty_true = torch.tensor(y_true, requires_grad=False)\n",
    "ty_true = torch.argmax(ty_true, dim=1)\n",
    "ty_out = torch.tensor(y_out, requires_grad=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "tj = criterion(ty_out, ty_true)\n",
    "tj.backward()\n",
    "\n",
    "print(j)\n",
    "print(tj.data.numpy())\n",
    "print(metrics.tdist(j, tj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.296462\n",
      "14.296461\n",
      "9.536743e-07\n"
     ]
    }
   ],
   "source": [
    "y_out = np.random.randn(7, 4).astype(np.float32)\n",
    "y_true = np.zeros((7, 4)).astype(np.float32)\n",
    "for i in range(y_true.shape[0]):\n",
    "    y_true[i][np.random.randint(0, y_true.shape[1])] = 1\n",
    "y_pred = softmax(y_out)\n",
    "\n",
    "j = - np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "ty_true = torch.tensor(y_true, requires_grad=False)\n",
    "ty_pred = torch.tensor(y_pred, requires_grad=True)\n",
    "tj = - torch.sum(ty_true * torch.log(ty_pred))\n",
    "tj.backward()\n",
    "\n",
    "print(j)\n",
    "print(tj.data.numpy())\n",
    "print(metrics.tdist(j, tj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.        -10.283339   -0.         -0.       ]\n",
      " [-10.58094    -0.         -0.         -0.       ]\n",
      " [ -0.         -0.         -2.7528124  -0.       ]\n",
      " [-46.90987    -0.         -0.         -0.       ]\n",
      " [ -0.         -0.         -1.3170731  -0.       ]\n",
      " [ -7.9531765  -0.         -0.         -0.       ]\n",
      " [ -0.        -10.990683   -0.         -0.       ]]\n",
      "[[ -0.        -10.283339   -0.         -0.       ]\n",
      " [-10.58094    -0.         -0.         -0.       ]\n",
      " [ -0.         -0.         -2.7528124  -0.       ]\n",
      " [-46.90987    -0.         -0.         -0.       ]\n",
      " [ -0.         -0.         -1.3170731  -0.       ]\n",
      " [ -7.9531765  -0.         -0.         -0.       ]\n",
      " [ -0.        -10.990683   -0.         -0.       ]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dy_pred = - y_true / y_pred\n",
    "dy_pred_sol = ty_pred.grad.data.numpy()\n",
    "\n",
    "print(dy_pred)\n",
    "print(dy_pred_sol)\n",
    "print(metrics.tdist(dy_pred, dy_pred_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\beta)}{\\partial o_{ij}} = \\hat{y_{ij}} - y_{ij}$$\n",
    "$$\\frac{\\partial J(\\beta)}{\\partial o} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.387552\n",
      "12.387553\n",
      "9.536743e-07\n"
     ]
    }
   ],
   "source": [
    "y_out = np.random.randn(7, 4).astype(np.float32)\n",
    "y_true = np.zeros((7, 4)).astype(np.float32)\n",
    "for i in range(y_true.shape[0]):\n",
    "    y_true[i][np.random.randint(0, y_true.shape[1])] = 1\n",
    "y_pred = softmax(y_out)\n",
    "\n",
    "j = - np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "ty_true = torch.tensor(y_true, requires_grad=False)\n",
    "ty_true = torch.argmax(ty_true, dim=1)\n",
    "ty_out = torch.tensor(y_out, requires_grad=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "tj = criterion(ty_out, ty_true)\n",
    "tj.backward()\n",
    "\n",
    "print(j)\n",
    "print(tj.data.numpy())\n",
    "print(metrics.tdist(j, tj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71088123  0.25399554  0.31700996  0.13987577]\n",
      " [ 0.02140404  0.3097546   0.29681578 -0.6279745 ]\n",
      " [ 0.60384715  0.03253903  0.0066169  -0.6430031 ]\n",
      " [ 0.22169167 -0.88766754  0.03120301  0.63477284]\n",
      " [ 0.05100057 -0.38170385  0.10363309  0.22707026]\n",
      " [ 0.02778155  0.6928965  -0.8194856   0.09880757]\n",
      " [ 0.03780703  0.9247614   0.02876937 -0.99133784]]\n",
      "[[-0.71088123  0.2539955   0.31700993  0.13987575]\n",
      " [ 0.02140405  0.30975467  0.29681584 -0.6279744 ]\n",
      " [ 0.60384715  0.03253903  0.0066169  -0.6430031 ]\n",
      " [ 0.22169165 -0.88766754  0.03120301  0.6347728 ]\n",
      " [ 0.05100057 -0.38170385  0.10363309  0.22707026]\n",
      " [ 0.02778155  0.6928965  -0.8194856   0.09880759]\n",
      " [ 0.03780702  0.9247613   0.02876936 -0.99133784]]\n",
      "2.0499465e-07\n"
     ]
    }
   ],
   "source": [
    "dy_out = y_pred - y_true\n",
    "dy_out_sol = ty_out.grad.data.numpy()\n",
    "\n",
    "print(dy_out)\n",
    "print(dy_out_sol)\n",
    "print(metrics.tdist(dy_out, dy_out_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be trained with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Error = 264.5967568728954\n",
      "SGD Error = 124.52928999771657\n",
      "SGD Error = 120.69338069535253\n",
      "SGD Error = 120.60511291188504\n",
      "SGD Error = 120.60208822782775\n",
      "SGD Error = 120.60195961583351\n",
      "SGD Error = 120.60195360857097\n",
      "SGD Error = 120.60195331813674\n",
      "SGD Error = 120.60195330392729\n",
      "SGD Error = 120.60195330322918\n",
      "SGD Error = 120.60195330319483\n",
      "SGD Error = 120.60195330319314\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiw/rep/ml-notebooks/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aiw/rep/ml-notebooks/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SGD Error = 120.60195330319306\n",
      "SK Error = 131.2755522167608\n",
      "SGD Error = 120.60195330319306\n",
      "[[ 0.04444944 -0.26198654  0.45062079 -0.44259313]\n",
      " [-0.07043236  0.21367511  0.07426233  0.01889864]\n",
      " [ 0.31956867 -0.16880807  0.30233762  0.21296133]\n",
      " [ 0.11425906  0.23945303 -0.00476698  0.11730668]]\n",
      "[[-2.73625768e-01 -4.21958176e-01 -1.22070210e-03 -2.14255251e-01]\n",
      " [-1.64177761e+00 -1.12803164e+00 -1.55859474e+00 -1.09958726e+00]\n",
      " [-5.23034387e-02 -4.19549979e-01 -1.45681396e-01 -5.18044315e-01]\n",
      " [-2.77048369e-02  4.17194543e-01  6.23181842e-01  5.35045086e-01]]\n"
     ]
    }
   ],
   "source": [
    "def get_error_multi(X, y, w):\n",
    "    y_pred = softmax(X @ w)\n",
    "    err = - np.sum(y * np.log(y_pred))\n",
    "    return err\n",
    "\n",
    "\n",
    "def multilog_reg(X, y):\n",
    "\n",
    "    w = np.random.randn(X.shape[1], y.shape[1])\n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        \n",
    "        y_pred = softmax(X @ w)\n",
    "        dy_out = y_pred - y\n",
    "        dw = X.T @ dy_out\n",
    "        \n",
    "        w -= 0.001 * dw\n",
    "        if epoch % 100 == 0:\n",
    "            err = get_error_multi(X, y, w)\n",
    "            print('SGD Error = {}'.format(err))\n",
    "        \n",
    "    return w\n",
    "\n",
    "    \n",
    "X = np.random.randn(93, 4).astype(np.float32)\n",
    "y_true = np.zeros((93, 4)).astype(np.float32)\n",
    "for i in range(y_true.shape[0]):\n",
    "    y_true[i][np.random.randint(0, y_true.shape[1])] = 1\n",
    "y_true_sk = np.argmax(y_true, axis=1)\n",
    "    \n",
    "w1 = log_reg_sk(X, y_true_sk)\n",
    "w2 = multilog_reg(X, y_true)\n",
    "print('SK Error = {}'.format(get_error_multi(X, y_true, w1)))\n",
    "print('SGD Error = {}'.format(get_error_multi(X, y_true, w2)))\n",
    "print(w1)\n",
    "print(w2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
