{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "import rlutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cp = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP: min = 35.0, avg = 86.185, max = 200.0\n"
     ]
    }
   ],
   "source": [
    "class LinearBinAgent:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def __call__(self, s, _):\n",
    "        return int(s @ self.params > 0)\n",
    "\n",
    "def random_search(env, nevals, niters):\n",
    "    space_len = len(env.observation_space.low)\n",
    "    best_params = None\n",
    "    best_val = float('-inf')\n",
    "    agent = LinearBinAgent(best_params)\n",
    "    \n",
    "    for _ in range(niters):\n",
    "        agent.params = np.random.randn(space_len)\n",
    "        val = rlutils.run_nepisodes(env, agent, nevals).mean()\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_params = agent.params\n",
    "    \n",
    "    return best_params\n",
    "    \n",
    "\n",
    "rand_params = random_search(env_cp, 20, 20)\n",
    "agent = LinearBinAgent(rand_params)\n",
    "scores = rlutils.run_nepisodes(env_cp, agent, 1000)\n",
    "print('CP: min = {}, avg = {}, max = {}'.format(scores.min(),\n",
    "                                               scores.mean(),\n",
    "                                               scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function (prediction) approximation\n",
    "\n",
    "Function approximators of any kind, must have a few properties:\n",
    "- diferentiable\n",
    "- suitable for non-stationary, non-iid data\n",
    "\n",
    "Let's define $J(w)$ a differentable function of parameters $w$, that is the mean squared error between our approximator $\\hat{v}(s,w)$ and the true value $v_\\pi(s)$:\n",
    "$$J(w) = \\mathbb{E}_\\pi[(v_\\pi(S) - \\hat{v}(S,w))^2]$$\n",
    "\n",
    "We represent the current state $S$ by a feature vector of size $p$\n",
    "\n",
    "We use mini-batch versions of gradient descent, where we compute $J(w)$ for only one example and update $w$.  \n",
    "But we don't have the true value $v_\\pi(S)$, so we use instead another target, depending on the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Policy Evaluation\n",
    "\n",
    "For Monte-Carlo, the target is the return $G_t$.  \n",
    "Converges to a local optimum, even with a non-linear model (MC target is unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2829.758752735302\n",
      "2685.9851884703457\n",
      "1835.917598220721\n",
      "3410.1158538863015\n",
      "1747.4012468590981\n",
      "1493.0163352441248\n",
      "924.1118637504383\n",
      "3325.9360769412006\n",
      "3371.96418963111\n",
      "1010.8292515726602\n",
      "3796.1706153056157\n",
      "2043.0087477062614\n",
      "2673.6764985688746\n",
      "2673.5953357922813\n",
      "2786.8431350028413\n",
      "2873.1682410692465\n",
      "2704.785889418083\n",
      "2979.576458885066\n",
      "1965.3967306509744\n",
      "1767.2035303637804\n",
      "1800.7121455442812\n",
      "3484.811832800219\n",
      "3408.2853806922417\n",
      "1516.114140470016\n",
      "2069.7024665759845\n",
      "1241.6684554080075\n",
      "1733.8290322347623\n",
      "2053.947021865621\n",
      "2057.208426759222\n",
      "2144.441439891937\n",
      "3672.199326209127\n",
      "2734.7904070852446\n",
      "2145.163943531712\n",
      "2272.385197598346\n",
      "3241.992166084059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mc_policy_eval(env, agent, ngames):\n",
    "    \n",
    "    model = SGDRegressor()\n",
    "    errs = []\n",
    "    t = 0\n",
    "    \n",
    "    for _ in range(ngames):\n",
    "        hist = []\n",
    "        s = env.reset()\n",
    "    \n",
    "        while True:\n",
    "            a = agent(s, env)\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            hist.append((s, r))\n",
    "            if done: break\n",
    "            s = s2\n",
    "        \n",
    "        gt = 0\n",
    "        for h in reversed(hist):\n",
    "            t += 1\n",
    "            gt += h[1]\n",
    "            X = h[0].reshape(1, -1)\n",
    "            y = np.array(gt).reshape(1)\n",
    "            model.partial_fit(X, y)\n",
    "            \n",
    "            err = (model.predict(X) - y)**2\n",
    "            errs.insert(0, err)\n",
    "            if len(errs) > 1000:\n",
    "                errs = errs[:-1]\n",
    "                    \n",
    "            if t % 2500 == 0:\n",
    "                avg_err = np.average(errs)\n",
    "                print(avg_err)\n",
    "            \n",
    "            \n",
    "            \n",
    "    return model\n",
    "    \n",
    "mc_policy_eval(env_cp, agent, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0) Policy Evaluation\n",
    "\n",
    "The target is $R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)$\n",
    "The TD target is biased.  \n",
    "Linear TD(0) converges (close) to a global optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701.134816367266\n",
      "3646.335627123368\n",
      "839.5511643556789\n",
      "2154.7679940988382\n",
      "1027.9382945818902\n",
      "2269.6368781070537\n",
      "3974.9207544332353\n",
      "1233.242841629993\n",
      "1841.0933387292162\n",
      "2800.099859596742\n",
      "3295.129335320453\n",
      "2130.9714386493956\n",
      "2924.741234742127\n",
      "1562.9061642140964\n",
      "2156.2298001248373\n",
      "1137.373371447198\n",
      "3391.2598715362287\n",
      "1787.46950022757\n",
      "2314.708545602369\n",
      "2026.2536344007476\n",
      "2382.971007198548\n",
      "1739.0297597911301\n",
      "1555.8770066061688\n",
      "1947.3102022324301\n",
      "2187.5696471780816\n",
      "3159.152537429964\n",
      "3055.3431110988013\n",
      "3314.0073389017007\n",
      "3284.803197901781\n",
      "1105.6177850455088\n",
      "3693.09590479369\n",
      "1971.6611633987106\n",
      "664.9511510224081\n",
      "2637.992298591326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def td0_policy_eval(env, agent, ngames, gamma):\n",
    "    \n",
    "    model = SGDRegressor()\n",
    "    errs = []\n",
    "    t = 0\n",
    "    \n",
    "    for _ in range(ngames):\n",
    "        s = env.reset()\n",
    "    \n",
    "        while True:\n",
    "            a = agent(s, env)\n",
    "            s2, r, done, _ = env.step(a)\n",
    "\n",
    "            X = s.reshape(1, -1)\n",
    "            y = r + gamma * model.predict(s2)\n",
    "            model.partial_fit(X, y)\n",
    "            \n",
    "            err = (model.predict(X) - y)**2\n",
    "            errs.insert(0, err)\n",
    "            if len(errs) > 1000:\n",
    "                errs = errs[:-1]   \n",
    "            t += 1\n",
    "            if t % 2500 == 0:\n",
    "                avg_err = np.average(errs)\n",
    "                print(avg_err)\n",
    "            \n",
    "            if done: break\n",
    "            s = s2\n",
    "            \n",
    "            \n",
    "    return model\n",
    "    \n",
    "mc_policy_eval(env_cp, agent, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$) policy evaluation\n",
    "\n",
    "The target is the $\\lambda$-return $G^\\lambda_t$\n",
    "\n",
    "The backward view can also be used with eligibilaty traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control with function approximation\n",
    "\n",
    "Repeat:\n",
    "- Evaluation: Approximate $q_\\pi$ with $\\hat{q}(s,a,w)$\n",
    "- Improvement: act with this new policy to chose actions\n",
    "\n",
    "The error function now becomes:\n",
    "\n",
    "$$J(w) = \\mathbb{E}_\\pi[(q_\\pi(S,A) - \\hat{q}(S,A,w))^2]$$\n",
    "\n",
    "We represent the state and action (S,A) by a feature vector.\n",
    "\n",
    "We replace $q_\\pi(S,A)$ by another target depending on the algorithm.  \n",
    "\n",
    "Most control algorithms have no theoric garanties of convergence, you may end up turn around the optimal policy, or even diverges from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Methods\n",
    "\n",
    "## Least Squares Prediction\n",
    "\n",
    "Let $\\mathcal{D}$ our experience consisting of (state, value) pairs:\n",
    "$$\\mathcal{D} = \\{ (s_1, v_1^\\pi), (s_2, v_2^\\pi), \\text{...}, (s_T, v_T^\\pi) \\}$$\n",
    "\n",
    "We can find the least squares solution that minimizes the mse between our prediction and the true state values:\n",
    "\n",
    "$$LS(w) = \\sum_{t=1}^T (v_t^\\pi - \\hat{v}(s_t, w))^2$$\n",
    "\n",
    "Experience Replay:  \n",
    "Repeat:\n",
    "- Store the new visited (state, value) pair in $\\mathcal{D}$\n",
    "- sample one $(s, v^\\pi) \\sim \\mathcal{D}$ and update $\\hat{v}$ with gradient descent.\n",
    "\n",
    "Helps decorelate things, experiences seen in random order. Converges to least squares solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network\n",
    "\n",
    "Combine 2 ideas:\n",
    "\n",
    "- Experience replay:  decorelates the trajectories, get much more stable updates.  \n",
    "- fixed Q-targets: Have 2 networks, freeze the old network weights, bootstrap toward these fixed targets. Every few thousands iteration, the old network weights are updated with the new one. Actions are chosen according to the latest targets.\n",
    "\n",
    "Repeat:\n",
    "- Take action $a_t$ according to $\\epsilon$-greedy policy\n",
    "- Store transition $(s_t, a_t, r_{t+1}, s_{t+1}$ in $\\mathcal{D}$\n",
    "- Sample mini-batch $(s, a, r, s') \\sim \\mathcal{D}$ (eg size 64)\n",
    "- Optimise MSE between prediction of frozen network and current prediction:\n",
    "    $$L(w) = \\mathbb{E}[(r + \\gamma \\max_{a'} Q(s',a',w^-) - Q(s,a,w))^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Least Squares Prediction\n",
    "\n",
    "Experiences replay find least squares solution on many iterations, but we can solve in closed form.\n",
    "\n",
    "The solution is the same than for classical least squares:\n",
    "\n",
    "$$w = (S^TS)^{-1} S^Tv_\\pi$$\n",
    "\n",
    "with $S$ a matrix whose rows are state-vectors, and $v_\\pi$ a vector of corresponding value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Policy Iteration\n",
    "\n",
    "Repeat:\n",
    "- Policy evalutation: fit $q(S,A)$ using least squares solution\n",
    "- Policy improvement: greedy policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
