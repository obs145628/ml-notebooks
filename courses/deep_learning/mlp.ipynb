{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import revdiff as rd\n",
    "import utils\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univer approximation theorem: A feedforward network with a linear output layer and at least one hidden layer with a non-linear activation function ca approximate any Borel mesurable function, witht any non-zero amount of error (witth enough hidden units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bias\n",
    "\n",
    "This is an operation that takes a tensor $X$ and vector $y$ and sum both of them on the last axis of $X$.  \n",
    "Example: 2D:\n",
    "$$Z = X + y, \\space X, R \\in \\mathbb{R}^{n*m}, b \\in \\mathbb{R}^m$$\n",
    "$$Z_{ij} = X_{ij} + b_{j}$$\n",
    "$$\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial Z}$$\n",
    "$$\\frac{\\partial E}{\\partial y_j} = \\sum_{i=1}^n \\frac{\\partial E}{\\partial Z_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512.8099394943562\n",
      "512.8099394943561\n",
      "1.1368683772161603e-13\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(43, 7)\n",
    "y = np.random.randn(7)\n",
    "z = (X + y).reshape(-1)\n",
    "e = z @ z\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "ty = torch.tensor(y, requires_grad=True)\n",
    "tz = (tX + ty).view(-1)\n",
    "te = torch.dot(tz, tz)\n",
    "te.backward()\n",
    "\n",
    "print(e)\n",
    "print(te.data.numpy())\n",
    "print(metrics.tdist(e, te.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[153.25935123  -3.83113279  38.98813845  46.41386373  23.31963682\n",
      "  29.90573326  79.79186959]\n",
      "[153.25935123  -3.83113279  38.98813845  46.41386373  23.31963682\n",
      "  29.90573326  79.79186959]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dz = 2 * z.reshape(X.shape)\n",
    "dX = dz\n",
    "dy = np.sum(dz, axis=0)\n",
    "\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "dy_sol = ty.grad.data.numpy()\n",
    "print(metrics.tdist(dX, dX_sol))\n",
    "print(dy)\n",
    "print(dy_sol)\n",
    "print(metrics.tdist(dy, dy_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_SIZE = 28 * 28\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 256\n",
    "OUT_SIZE = 10\n",
    "\n",
    "NEPOCHS = 5\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader, test_loader = utils.load_mnist(BATCH_SIZE)\n",
    "train_loader_01, test_loader_01 = utils.load_mnist_01(BATCH_SIZE)\n",
    "\n",
    "def compute_accuracy(y_preds, y):\n",
    "    total = len(y_preds)\n",
    "    correct = np.equal(y_preds, y).sum()\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST K classes with softmax + cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 147.83250427246094\n",
      "1[301] Loss = 24.46794891357422\n",
      "1[601] Loss = 17.228824615478516\n",
      "1[901] Loss = 15.436829566955566\n",
      "Epoch 1: test accuracy = 0.9192 (9192/10000)\n",
      "2[1] Loss = 18.49155616760254\n",
      "2[301] Loss = 24.79840850830078\n",
      "2[601] Loss = 8.40994930267334\n",
      "2[901] Loss = 14.039809226989746\n",
      "Epoch 2: test accuracy = 0.9441 (9441/10000)\n",
      "3[1] Loss = 13.433870315551758\n",
      "3[301] Loss = 27.380294799804688\n",
      "3[601] Loss = 6.725010871887207\n",
      "3[901] Loss = 4.588817596435547\n",
      "Epoch 3: test accuracy = 0.9519 (9519/10000)\n",
      "4[1] Loss = 8.174762725830078\n",
      "4[301] Loss = 12.456804275512695\n",
      "4[601] Loss = 9.772133827209473\n",
      "4[901] Loss = 2.8409600257873535\n",
      "Epoch 4: test accuracy = 0.9592 (9592/10000)\n",
      "5[1] Loss = 12.741117477416992\n",
      "5[301] Loss = 7.070180416107178\n",
      "5[601] Loss = 6.1213297843933105\n",
      "5[901] Loss = 6.556562423706055\n",
      "Epoch 5: test accuracy = 0.965 (9650/10000)\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = torch.nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = torch.nn.Linear(HIDDEN2_SIZE, OUT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IN_SIZE)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        y_logits = self.l3(x)\n",
    "        return y_logits\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y)\n",
    "    \n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        for w in net.parameters():\n",
    "            w.data.sub_(w.grad.data * LR)\n",
    "    \n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "    \n",
    "    test_y_preds, test_y_true = utils.get_class_output(net, test_loader)\n",
    "    correct, total = compute_accuracy(test_y_preds, test_y_true)\n",
    "    acc = float(correct) / total\n",
    "    print('Epoch {}: test accuracy = {} ({}/{})'.format(epoch + 1, acc,\n",
    "                                                   correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST bin classification digits 0/1 (sigmoid+BCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[1] Loss = 45.23631286621094\n",
      "Epoch 1: test accuracy = 0.9990543735224586 (2113/2115)\n",
      "2[1] Loss = 0.13379809260368347\n",
      "Epoch 2: test accuracy = 0.9990543735224586 (2113/2115)\n",
      "3[1] Loss = 0.0598868690431118\n",
      "Epoch 3: test accuracy = 0.9990543735224586 (2113/2115)\n",
      "4[1] Loss = 0.3059484362602234\n",
      "Epoch 4: test accuracy = 0.9990543735224586 (2113/2115)\n",
      "5[1] Loss = 0.026982322335243225\n",
      "Epoch 5: test accuracy = 0.9990543735224586 (2113/2115)\n"
     ]
    }
   ],
   "source": [
    "IN_SIZE = 28 * 28\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 256\n",
    "\n",
    "NEPOCHS = 5\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(IN_SIZE , HIDDEN1_SIZE)\n",
    "        self.l2 = torch.nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)\n",
    "        self.l3 = torch.nn.Linear(HIDDEN2_SIZE, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IN_SIZE)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        y_logits = self.l3(x).view(-1)\n",
    "        return y_logits\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    \n",
    "for epoch in range(NEPOCHS):\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader_01):\n",
    "        y_logits = net(X)\n",
    "        loss = criterion(y_logits, y.type(torch.float32))\n",
    "    \n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        for w in net.parameters():\n",
    "            w.data.sub_(w.grad.data * LR)\n",
    "    \n",
    "        if batch_idx % 300 == 0:\n",
    "            print('{}[{}] Loss = {}'.format(epoch + 1, \n",
    "                                            batch_idx+1, loss.data))\n",
    "            \n",
    "    test_y_preds, test_y_true = utils.get_class_output(net, test_loader_01,\n",
    "                                            act=torch.sigmoid)\n",
    "    correct, total = compute_accuracy(test_y_preds, test_y_true)\n",
    "    acc = float(correct) / total\n",
    "    print('Epoch {}: test accuracy = {} ({}/{})'.format(epoch + 1, acc,\n",
    "                                                    correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used as output units for binary clasification problems with BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x \\in \\mathbb{R},  \\space \\sigma(x) \\in \\mathbb{R}$$\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "$$(\\sigma(x))' = \\sigma(x)(1 - \\sigma(x)), \\space x \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used as output units for multinomia; clasification problems with Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y \\in \\mathbb{R}^{p}$$\n",
    "$$ y_j = \\text{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{k=1}^p e^{x_k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be extended to matrices, by applying softmax on each row.\n",
    "\n",
    "$$x, y \\in \\mathbb{R}^{n*p}$$\n",
    "$$ y_{ij} = \\text{softmax}(x)_{ij} = \\frac{e^{x_{ij}}}{\\sum_{k=1}^p e^{x_{ik}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Let } S_i = \\text{softmax}(x)_i$$\n",
    "$$\\frac{\\partial S_i}{\\partial x_j} = S_i (1 - S_j) \\space (i = j)$$\n",
    "$$\\frac{\\partial S_i}{\\partial x_j} = -S_i * S_j \\space (i \\neq j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_e = np.exp(x)\n",
    "    return x_e / np.sum(x_e, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost linear, very good hidden unit.  \n",
    "Not diferentiable at $0$, in pratice not an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y \\in \\mathbb{R}$$\n",
    "$$ y = \\text{relu}(x) = max(0, x)$$\n",
    "$$(\\text{relu}(x))' = 1 \\space (x > 0)$$\n",
    "$$(\\text{relu}(x))' = 0 \\space (x \\leq 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return (x > 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y, \\alpha \\in \\mathbb{R}$$\n",
    "$$ y = \\text{leaky_relu}(x) = max(0, x) + \\alpha min(0, x)$$\n",
    "$$(\\text{leaky_relu}(x))' = 1 \\space (x > 0)$$1\n",
    "$$(\\text{leaky_relu}(x))' = \\alpha \\space (x \\leq 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha):\n",
    "    return np.maximum(0, x) + alpha * np.minimum(0, x)\n",
    "\n",
    "def leaky_relu_prime(x, alpha):\n",
    "    return (x > 0).astype(x.dtype) + alpha * (x <= 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU (Exponential Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y, \\alpha \\in \\mathbb{R}$$\n",
    "$$ y = \\text{elu}(x) = max(0, x) + min(0, \\alpha(e^x-1))$$\n",
    "$$(\\text{elu}(x))' = 1 \\space (x > 0)$$\n",
    "$$(\\text{elu}(x))' = \\alpha e^x \\space (x \\leq 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha):\n",
    "    return np.maximum(0, x) + np.minimum(0, alpha * (np.exp(x)-1))\n",
    "\n",
    "def elu_prime(x, alpha):\n",
    "    return (x > 0).astype(x.dtype) + alpha * np.exp(x) * (x <= 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric RELU is the same than Leaky RELU, except that $\\alpha$ is a lernable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to sigmoid, but closer to identity function (linear) near $0$.  \n",
    "Output range is between $-1$ and $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y \\in \\mathbb{R}$$\n",
    "$$ y = tanh(x)$$\n",
    "$$tanh(x) = 2\\sigma(2z)-1$$\n",
    "$$(tanh(x))' = 1 - tanh(x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x, y, \\beta \\in \\mathbb{R}$$\n",
    "$$ y = \\text{softplus}(x) = \\frac{1}{\\beta} log(1 + e^{\\beta x})$$\n",
    "$$(\\text{softplus}(x))' = \\sigma(\\beta x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x, beta = 1):\n",
    "    return (1 / beta) * np.log(1 + np.exp(beta * x))\n",
    "\n",
    "def softplus_prime(x, beta=1):\n",
    "    return sigmoid(beta * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear (No Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used as output unit for regression problem with MSE.  \n",
    "No gradients problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cost function take the predictions and true values for a whole batch, and return a unique number, the error, that must be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (Mean Squarer Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are denoted $\\hat{y} \\in \\mathbb{R}^n$.  \n",
    "The true values are denoted $y \\in \\mathbb{R}^n$.\n",
    "\n",
    "$$MSE(W) = \\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple target versions: $y, \\hat{y} \\in \\mathbb{R}^{n*m}$\n",
    "$$MSE(W) = \\frac{1}{n*m} \\sum_{i=1}^n||y_i - \\hat{y}_i||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial MSE(W)}{\\partial \\hat{y}} = \\frac{2}{n}(\\hat{y} - y)$$\n",
    "$$\\frac{\\partial MSE(W)}{\\partial y} = \\frac{2}{n}(y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any dimensions, MSE can be applied by just reshaping $y$ and $\\hat{y}$ into 1D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    n = len(y_true)\n",
    "    return (1 / n) * np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def mse_dy_pred(dout, y_pred, y_true):\n",
    "    n = len(y_true)\n",
    "    return dout * (2 / n) * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCE (Binary Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE is used to solve binary classification problems.\n",
    "Usualy it's used with an activation function for the output layer that scales values between $0$ and $1$, like sigmoid.  \n",
    "The predictions $\\hat{y} \\in \\mathbb{R}^n$ are probabilities to belong to the class.  \n",
    "The true values are $ y\\in \\mathbb{R}^n$ are either 0 or 1 (1 if belongs to the class). \n",
    "$$J(W) = - \\sum_{i=1}^n (y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(W)}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$\n",
    "Let $\\hat y = \\sigma(o)$, $o \\in \\mathbb{R}^n$ (final activation is sigmoid).\n",
    "$$\\frac{\\partial J(W)}{\\partial o} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_pred, y_true):\n",
    "    return - np.sum(y_true*np.log(y_pred)+(1-y_true) * np.log(1-y_pred))\n",
    "    \n",
    "def bce_dy_out(dout, y_out, y_true):\n",
    "    y_pred = sigmoid(y_out)\n",
    "    return dout * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy is used to solve binary classification problems.\n",
    "Usualy it's used with an activation function for the output layer that gives probabilities suming to $1$ in each row, like softmax.  \n",
    "The predictions $\\hat{y} \\in \\mathbb{R}^{n*k}$ are probabilities to belong to each of the k classes.  \n",
    "The true values $y\\in \\mathbb{R}^{n*k}$ are one-hot vectors, with a $1$ entry in the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W) = - \\sum_{i=1}^n \\sum_{j=1}^k [y_{ij} log(\\hat{y_{ij}})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(W)}{\\partial \\hat{y}} =  - \\frac{y}{\\hat{y}}$$\n",
    "Let $\\hat y = $softmax$(o)$, $o \\in \\mathbb{R}^{n*k}$ (final activation is softmax).\n",
    "$$\\frac{\\partial J(W)}{\\partial o} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    return - np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def cross_entropy_dy_out(dout, y_out, y_true):\n",
    "    y_pred = softmax(y_out)\n",
    "    return dout * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE (Mean Absolute Error) or L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are denoted $\\hat{y} \\in \\mathbb{R}^n$.  \n",
    "The true values are denoted $y \\in \\mathbb{R}^n$.\n",
    "\n",
    "$$J(W) = \\frac{1}{n} \\sum_{i=1}^n|y_i - \\hat{y}_i|$$\n",
    "$$\\frac{\\partial J(W)}{\\partial \\hat{y}} = \\frac{1}{n} sign(\\hat{y} - y)$$\n",
    "$$\\frac{\\partial J(W)}{\\partial y} = \\frac{1}{n} sign(y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_pred, y_true):\n",
    "    n = len(y_true)\n",
    "    return (1 / n) * np.sum(np.abs(y_true - y_pred))\n",
    "\n",
    "def mae_dy_pred(dout, y_pred, y_true):\n",
    "    n = len(y_true)\n",
    "    return dout * (1 / n) * np.sign(y_pred - y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
