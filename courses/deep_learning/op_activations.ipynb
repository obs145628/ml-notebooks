{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "Outputs a number between $0$ and $1$. Can be used as output activation for a binary classification problem.\n",
    "\n",
    "## Foward pass\n",
    "\n",
    "Take a tensor as input, and applies the sigmoid elementwise\n",
    "\n",
    "$$\\sigma(x): \\mathbb{R} \\to \\mathbb{R}$$\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$(\\sigma(x))' = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.143906e-08\n",
      "8.9251614e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "def sigmoid_dx(x, dout):\n",
    "    return sigmoid_prime(x) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "y = sigmoid(x)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = sigmoid_dx(x, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = torch.sigmoid(tx)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Ouputs a vector of probabilities (between 0 and 1, sum to 1). Can be used as output activation for a classification problem,\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "$$Y = \\text{softmax(X)}, \\space X, Y \\in \\mathbb{R}^{N*K}$$\n",
    "\n",
    "$$x, y \\in \\mathbb{R}^{p}$$\n",
    "$$ Y_{ij} = \\frac{\\exp(x_{ij})}{\\sum_{k=1}^K \\exp(x_{ik})}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$\\text{Let } S_{ci} = \\text{softmax}(x)_{ci}$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{ci}}{\\partial x_{cj}} =\n",
    "\\begin{cases}\n",
    "    S_{ci} (1 - S_{cj}) & \\text{if } i = j\\\\\n",
    "    -S_{ci} * S_{cj} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial E}{X_{cj}} = \\sum_{i=1}^K \\frac{\\partial E}{\\partial S_{ci}} * \\frac{\\partial S_{ci}}{\\partial X_{cj}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5499829e-07\n",
      "7.847853192507804e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True)\n",
    "    x_e = np.exp(x)\n",
    "    return x_e / np.sum(x_e, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_dx(x, dout):\n",
    "    s = softmax(x)\n",
    "    N, p = x.shape\n",
    "    \n",
    "    prim = s.reshape(N, 1, p) * (np.eye(p).reshape(1, p, p)\n",
    "                                 - s.reshape(N, p, 1))\n",
    "    dx = np.sum(dout.reshape(N, p, 1) * prim, axis=1)\n",
    "    return dx\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "x = np.random.randn(27, 13).astype(np.float32)\n",
    "y = softmax(x)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = softmax_dx(x, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = torch.softmax(tx, 1)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "\n",
    "Almost linear, very good hidden unit.  \n",
    "Not diferentiable at $0$, in pratice not an issue.\n",
    "\n",
    "## Foward pass\n",
    "\n",
    "Take a tensor as input, and applies the ReLU elementwise\n",
    "\n",
    "$$\\text{ReLU}: \\mathbb{R} \\to \\mathbb{R}$$\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$(\\text{ReLU}(x))' =  1(x > 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "def relu_dx(x, dout):\n",
    "    return relu_prime(x) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "y = relu(x)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = relu_dx(x, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = torch.relu(tx)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU\n",
    "\n",
    "A modified version of ReLU to fix the non-differentiable part of ReLU\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "$$ y = \\text{leaky_relu}(x) = \\max(0, x) + \\alpha \\min(0, x), \\space x, y, \\alpha \\in \\mathbb{R}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$\n",
    "\\text{leaky_relu}(x))' =\n",
    "\\begin{cases}\n",
    "     1 & \\text{if } x > 0\\\\\n",
    "    \\alpha & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def leaky_relu(x, alpha):\n",
    "    return np.maximum(0, x) + alpha * np.minimum(0, x)\n",
    "\n",
    "def leaky_relu_px(x, alpha):\n",
    "    return (x > 0).astype(x.dtype) + alpha * (x <= 0).astype(x.dtype)\n",
    "\n",
    "def leaky_relu_dx(x, alpha, dout):\n",
    "    return leaky_relu_px(x, alpha) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "alpha=0.2\n",
    "y = leaky_relu(x, alpha)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = leaky_relu_dx(x, alpha, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = F.leaky_relu(tx, alpha)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU\n",
    "\n",
    "A modified version of ReLU to fix the non-differentiable part of ReLU\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "$$ y = \\text{ELU}(x) = \\max(0, x) + \\min(0, \\alpha(e^x-1)), \\space x, y, \\alpha \\in \\mathbb{R}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x))' =\n",
    "\\begin{cases}\n",
    "     1 & \\text{if } x > 0\\\\\n",
    "    \\alpha e^x & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "4.679838e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def elu(x, alpha):\n",
    "    return np.maximum(0, x) + np.minimum(0, alpha * (np.exp(x)-1))\n",
    "\n",
    "def elu_px(x, alpha):\n",
    "    return ((x > 0).astype(x.dtype) \n",
    "            + alpha * np.exp(x) * (x <= 0).astype(x.dtype))\n",
    "\n",
    "def elu_dx(x, alpha, dout):\n",
    "    return elu_px(x, alpha) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "alpha=0.2\n",
    "y = elu(x, alpha)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = elu_dx(x, alpha, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = F.elu(tx, alpha)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbolic Tangent\n",
    "\n",
    "Very similar to sigmoid, but closer to identity function (linear) near $0$.  \n",
    "Output range is between $-1$ and $1$\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "Take a tensor as input, and applies $\\tanh$ elementwise.\n",
    "\n",
    "$$\\tanh(x) : \\mathbb{R} \\to \\mathbb{R}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$(\\tanh(x))' = 1 - \\tanh(x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5879527e-07\n",
      "2.485637e-07\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def tanh_dx(x, dout):\n",
    "    return tanh_prime(x) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "y = tanh(x)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = tanh_dx(x, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = torch.tanh(tx)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softplus\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "Take a tensor as input, and applies softplus elementwise.\n",
    "\n",
    "$$\\text{softplus}(x) = \\frac{1}{\\beta} log(1 + e^{\\beta x}), \\space x, \\beta \\in \\mathbb{R}$$\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "$$(\\text{softplus}(x))' = \\sigma(\\beta x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5359043e-06\n",
      "2.2238205e-06\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "def softplus(x, beta):\n",
    "    return (1 / beta) * np.log(1 + np.exp(beta * x))\n",
    "\n",
    "def softplus_prime(x, beta):\n",
    "    return sigmoid(beta * x)\n",
    "\n",
    "def softplus_dx(x, beta, dout):\n",
    "    return softplus_prime(x, beta) * dout\n",
    "\n",
    "x = np.random.randn(4, 6, 2).astype(np.float32)\n",
    "beta = 0.24\n",
    "y = softplus(x, beta)\n",
    "loss = np.sum(y**2)\n",
    "dy = 2*y\n",
    "dx = softplus_dx(x, beta, dy)\n",
    "\n",
    "tx = torch.from_numpy(x).requires_grad_(True)\n",
    "ty = F.softplus(tx, beta)\n",
    "tloss = torch.sum(ty**2)\n",
    "tloss.backward()\n",
    "tdx = tx.grad\n",
    "\n",
    "print(metrics.tdist(y, ty.data.numpy()))\n",
    "print(metrics.tdist(dx, tdx.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
