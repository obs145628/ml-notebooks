{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: input tensor of size $(N, C_X, H_X, W_X)$  \n",
    "$K$: kernel tensor of size $(C_Y, C_X, H_K, W_K)$  \n",
    "$Y$: output tensor of size $(N, C_Y, H_Y, W_Y)$\n",
    "\n",
    "$S_H$: Stride on the y-axis.  \n",
    "$S_W$: Stride on the x-axis.\n",
    "\n",
    "$$Y = \\text{Conv2D}(X, K, S_H, S_W)$$\n",
    "\n",
    "$$H_Y = \\lfloor \\frac{H_X - H_K}{S_H} \\rfloor + 1$$  \n",
    "$$W_Y = \\lfloor \\frac{W_X - W_K}{S_W} \\rfloor + 1$$\n",
    "\n",
    "$$Y_{n,f,y,x} = \\sum_{c=0}^{C_X} \\sum_{i=0}^{H_K} \\sum_{j=0}^{W_K} X_{n,c,y*S_H+i,x*S_W+j} K_{f,c,i,j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val(X, K, n, f, y, x):\n",
    "    res = 0\n",
    "    for c in range(K.shape[1]):\n",
    "        for i in range(K.shape[2]):\n",
    "            for j in range(K.shape[3]):\n",
    "                res += X[n, c, y+i, x+j] * K[f, c, i, j]\n",
    "    return res\n",
    "\n",
    "def conv2d(X, K, sh, sw):\n",
    "    h_y = int((X.shape[2] - K.shape[2]) / sh + 1)\n",
    "    w_y = int((X.shape[3] - K.shape[3]) / sw + 1)\n",
    "    \n",
    "    Y = np.empty((X.shape[0], K.shape[0], h_y, w_y))\n",
    "    \n",
    "    for n in range(X.shape[0]):\n",
    "        for f in range(K.shape[0]):\n",
    "            for y in range(h_y):\n",
    "                for x in range(w_y):\n",
    "                    Y[n, f, y, x] = compute_val(X, K, n, f, y*sh, x*sw)\n",
    "                    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 13, 16)\n",
      "torch.Size([2, 4, 13, 16])\n",
      "8.76177517283893e-05\n",
      "[12.27027611 -1.85225329 -5.01949694 -8.9430606   8.00193804 21.22102025\n",
      " -9.40190511 11.04296152 -0.60304749  5.9764414 ]\n",
      "[12.270277  -1.8522543 -5.019496  -8.943059   8.001938  21.221014\n",
      " -9.401903  11.04296   -0.6030485  5.9764423]\n"
     ]
    }
   ],
   "source": [
    "#Test stride = 1\n",
    "\n",
    "X = np.random.randn(2, 3, 17, 23)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d(X, K, 1, 1)\n",
    "tX = torch.Tensor(X)\n",
    "tK = torch.Tensor(K)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(1, 1))\n",
    "\n",
    "print(Y.shape)\n",
    "print(tY.shape)\n",
    "print(metrics.tdist(tY.data.numpy(), Y))\n",
    "\n",
    "print(Y.ravel()[:10])\n",
    "print(tY.data.numpy().ravel()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 5, 5)\n",
      "torch.Size([2, 4, 5, 5])\n",
      "3.068909616939018e-05\n",
      "[-36.17271351  16.92498969   7.10963938  12.88219436  -8.32642641\n",
      "  12.03880214  15.00547853  -7.8010264   25.09452249   0.91864666]\n",
      "[-36.17271     16.924992     7.109639    12.882193    -8.326427\n",
      "  12.0388      15.005477    -7.8010254   25.094528     0.91864663]\n"
     ]
    }
   ],
   "source": [
    "#Test strided complete\n",
    "\n",
    "X = np.random.randn(2, 3, 17, 24)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d(X, K, 3, 4)\n",
    "tX = torch.Tensor(X)\n",
    "tK = torch.Tensor(K)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(3, 4))\n",
    "\n",
    "print(Y.shape)\n",
    "print(tY.shape)\n",
    "print(metrics.tdist(tY.data.numpy(), Y))\n",
    "\n",
    "print(Y.ravel()[:10])\n",
    "print(tY.data.numpy().ravel()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 5, 5)\n",
      "torch.Size([2, 4, 5, 5])\n",
      "2.944308609853652e-05\n",
      "[ 5.87410606 -2.94072337 -5.19944722  3.12111597  6.34410141 -7.72689006\n",
      " -0.64374334  4.17813088 16.82332271 -6.28946122]\n",
      "[ 5.8741064 -2.9407241 -5.1994486  3.121117   6.3440995 -7.7268906\n",
      " -0.6437428  4.17813   16.823324  -6.28946  ]\n"
     ]
    }
   ],
   "source": [
    "#Test strided partial\n",
    "\n",
    "X = np.random.randn(2, 3, 19, 26)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d(X, K, 3, 4)\n",
    "tX = torch.Tensor(X)\n",
    "tK = torch.Tensor(K)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(3, 4))\n",
    "\n",
    "print(Y.shape)\n",
    "print(tY.shape)\n",
    "print(metrics.tdist(tY.data.numpy(), Y))\n",
    "\n",
    "print(Y.ravel()[:10])\n",
    "print(tY.data.numpy().ravel()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padded Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the output of the convolution to be of a specific size, we usually pad the input in both x and y axis of 0 values, for both sides.\n",
    "\n",
    "$X$: input tensor of size $(N, C_X, H_X, W_X)$  \n",
    "$K$: kernel tensor of size $(C_Y, C_X, H_K, W_K)$  \n",
    "$Y$: output tensor of size $(N, C_Y, H_Y, W_Y)$\n",
    "\n",
    "$S_H$: Stride on the y-axis.  \n",
    "$S_W$: Stride on the x-axis.  \n",
    "$P_H$: Padding on the x-axis.  \n",
    "$P_W$: Padding on the x-axis.\n",
    "\n",
    "$$Y = \\text{Conv2D}(X, K, S_H, S_W, P_H, P_W)$$\n",
    "\n",
    "$$H_Y = \\lfloor \\frac{H_X - H_K + 2P_H}{S_H} \\rfloor + 1$$\n",
    "$$W_Y = \\lfloor \\frac{W_X - W_K + 2P_W}{S_W} \\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(X, ptop, pbot, pleft, pright):\n",
    "    Y = np.zeros((X.shape[0], X.shape[1], X.shape[2] + ptop + pbot,\n",
    "                  X.shape[3] + pleft + pright))\n",
    "    Y[:, :, ptop:ptop+X.shape[2], pleft:pleft+X.shape[3]] = X\n",
    "    return Y\n",
    "\n",
    "def conv2d_padded(X, K, sh, sw, ph, pw):\n",
    "    return conv2d(pad_tensor(X, ph, ph, pw, pw), K, sh, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 5, 5)\n",
      "torch.Size([2, 4, 5, 5])\n",
      "3.297999823558242e-05\n",
      "[  2.75597412   1.76427656  -1.32040894 -14.56122892  -3.84040831\n",
      " -31.07545898  -4.92394088  -5.23159529  11.75772608  22.49719166]\n",
      "[  2.755973    1.7642767  -1.3204103 -14.561233   -3.8404062 -31.075459\n",
      "  -4.923938   -5.2315955  11.757724   22.497198 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 19, 26)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d_padded(X, K, 3, 4, 0, 0)\n",
    "tX = torch.Tensor(X)\n",
    "tK = torch.Tensor(K)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(3, 4))\n",
    "\n",
    "print(Y.shape)\n",
    "print(tY.shape)\n",
    "print(metrics.tdist(tY.data.numpy(), Y))\n",
    "\n",
    "print(Y.ravel()[:10])\n",
    "print(tY.data.numpy().ravel()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 10, 10)\n",
      "torch.Size([2, 4, 10, 10])\n",
      "3.447534606364203e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 19, 26)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d_padded(X, K, 3, 4, 7, 10)\n",
    "tX = torch.Tensor(X)\n",
    "tK = torch.Tensor(K)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(3, 4), padding=(7, 10))\n",
    "\n",
    "print(Y.shape)\n",
    "print(tY.shape)\n",
    "print(metrics.tdist(tY.data.numpy(), Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\text{tr}(X, d1, d2, d3, d4)$$\n",
    "\n",
    "Operation to transpose a 4D tensor, such that first dimension becomes $d1$, second becomes $d2$, and so on.  \n",
    "\n",
    "In the convolution, the last 2 dimensions represent the x and y-axises for both the kernel, input, and output tensors, we never need to change them.  \n",
    "The only needed transpose is to swap the 2 axes.  \n",
    "Let's define $X^T = \\text{tr}(X, 1, 0, 2, 3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\text{stride0}(X, h, w)$$\n",
    "\n",
    "Extend the 2 last axes of the tensor $X$ by adding $h$ rows of $0$ between the original rows, and $w$ columns of $0$ between the original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride0(X, h, w):\n",
    "    \n",
    "    Y = np.zeros((X.shape[0], X.shape[1],\n",
    "                 1 + (h + 1) * (X.shape[2] - 1), \n",
    "                 1 + (w + 1) * (X.shape[3] - 1)))\n",
    "    \n",
    "    for i1 in range(X.shape[0]):\n",
    "        for i2 in range(X.shape[1]):\n",
    "            for i3 in range(X.shape[2]):\n",
    "                for i4 in range(X.shape[3]):\n",
    "                    Y[i1, i2, i3 * (h+1), i4 * (w+1)] = X[i1, i2, i3, i4]\n",
    "                \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\text{rot180}(X)$$\n",
    "\n",
    "Make a 180 degree rotations on the 2 last axes of the tensor $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot180(X):\n",
    "    \n",
    "    Y = np.empty(X.shape)\n",
    "    for i1 in range(X.shape[0]):\n",
    "        for i2 in range(X.shape[1]):\n",
    "            for i3 in range(X.shape[2]):\n",
    "                for i4 in range(X.shape[3]):\n",
    "                    Y[i1, i2, i3, i4] = X[i1, i2, \n",
    "                                        X.shape[2] - i3 - 1,\n",
    "                                        X.shape[3] - i4 - 1]\n",
    "                    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E}{\\partial K} = \\text{conv2d}(\\text{pad}(X^T, P_H, P_W), \\text{stride0}(\\nabla_Y E^T, S_H-1, S_W-1))^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_padded_dk(X, dY, sh, sw, ph, pw):\n",
    "    Xtr = np.transpose(pad_tensor(X, ph, ph, pw, pw), (1, 0, 2, 3))\n",
    "    f_dY = np.transpose(dY, (1,0,2,3))\n",
    "    f_dY = stride0(f_dY, sh - 1, sw - 1)\n",
    "    o_dK = conv2d(Xtr, f_dY, 1, 1)\n",
    "    return np.transpose(o_dK, (1,0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E}{\\partial X} = \\text{conv2d}(\\text{pad}(\\text{stride0}(\\nabla_Y E, S_H - 1, S_W - 1), H_K - 1, W_K - 1), \\text{rot180}(K^T))_{:,:,P_H:H_X+P_H,P_W:W_X+P_W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_padded_dx(K, dY, sh, sw, ph, pw):\n",
    "    pdY = pad_tensor(stride0(dY, sh - 1, sw - 1), \n",
    "                     K.shape[2] - 1, K.shape[2] - 1,\n",
    "                     K.shape[3] - 1, K.shape[3] - 1)\n",
    "    K180 = np.transpose(rot180(K), (1, 0, 2, 3))\n",
    "    dX_full = conv2d(pdY, K180, 1, 1)\n",
    "    dX = dX_full[:,:,ph:dX_full.shape[2]-ph, pw:dX_full.shape[3]-pw]\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008505403649264535\n",
      "[   4.2319527 -294.97983    -62.27544    -36.04886    -44.115356\n",
      "  135.40556     99.77838     74.368034   333.33154   -204.06589  ]\n",
      "[   4.23194802 -294.97981392  -62.27546192  -36.04880602  -44.11533931\n",
      "  135.40561936   99.77840562   74.36800223  333.33151138 -204.06588999]\n",
      "0.0007010260194984067\n",
      "[-13.444337  -18.262794  125.35315   -45.83321   -22.022026   33.528214\n",
      " -25.425465   10.127198   64.602066    2.9770412]\n",
      "[-13.44432102 -18.26277261 125.35316172 -45.83321508 -22.02201179\n",
      "  33.52820121 -25.42547419  10.12720413  64.60206918   2.97704919]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 17, 24).astype(np.float32)\n",
    "K = np.random.randn(4, 3, 5, 8).astype(np.float32)\n",
    "Y = conv2d_padded(X, K, 3, 4, 6, 8)\n",
    "Yf = np.reshape(Y, (-1,))\n",
    "e = Yf @ Yf\n",
    "\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tK = torch.tensor(K, requires_grad=True)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, stride=(3, 4), padding=(6, 8))\n",
    "tY = tY.view(-1)\n",
    "te = torch.dot(tY, tY)\n",
    "te.backward()\n",
    "\n",
    "dK = conv2d_padded_dk(X, 2 * Y, 3, 4, 6, 8)\n",
    "dX = conv2d_padded_dx(K, 2 * Y, 3, 4, 6, 8)\n",
    "\n",
    "dK_sol = tK.grad.data.numpy()\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(metrics.tdist(dK_sol, dK))\n",
    "print(dK_sol.ravel()[:10])\n",
    "print(dK.ravel()[:10])\n",
    "\n",
    "print(metrics.tdist(dX_sol, dX))\n",
    "print(dX_sol.ravel()[:10])\n",
    "print(dX.ravel()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution transpose  (deconvolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also called fractionally-strided convolution, it's an operation to upsample 2D data.  \n",
    "It's operation is the same as to compute the gradient of the convoluton relative to $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: input tensor of size $(N, C_X, H_X, W_X)$  \n",
    "$K$: kernel tensor of size $(C_Y, C_X, H_K, W_K)$  \n",
    "$Y$: output tensor of size $(N, C_Y, H_Y, W_Y)$\n",
    "\n",
    "$S_H$: Stride on the y-axis.  \n",
    "$S_W$: Stride on the x-axis.  \n",
    "$P_H$: Padding on the x-axis (corresponding to pading for conv input).  \n",
    "$P_W$: Padding on the x-axis (corresponding to padding for conv input).\n",
    "\n",
    "$$Y = \\text{Conv2D_transpose}(X, K, S_H, S_W, P_H, P_W)$$\n",
    "\n",
    "$$H_Y = (H_X - 1) S_H - 2 * P_H + H_K$$\n",
    "$$W_Y = (W_X - 1) S_W - 2 * P_W + W_K$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 24\n",
      "(2, 3, 17, 24)\n"
     ]
    }
   ],
   "source": [
    "HX = (Y.shape[2] - 1) * 3 - 2 * 6 + K.shape[2]\n",
    "WX = (Y.shape[3] - 1) * 4 - 2 * 8 + K.shape[3]\n",
    "\n",
    "print(HX, WX)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a conv transpose wth arguments $S_H$, $S_W$, $P_H$ and $P_W$ gives exactly the gradient operation of the conv2d with the same arguments.  \n",
    "The same way, the gradient with respect to the input of the convolution transpose is the actual convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35486.62502238485\n",
      "35486.62502238486\n",
      "3.111882376880343e-14\n",
      "7.669622831608581e-13\n",
      "1.0476004925131232e-12\n"
     ]
    }
   ],
   "source": [
    "X2 = np.random.randn(*Y.shape)\n",
    "K2 = np.random.randn(*K.shape)\n",
    "Y2 = conv2d_padded_dx(K2, X2, 3, 4, 6, 8)\n",
    "\n",
    "Y2f = Y2.reshape(-1)\n",
    "e = Y2f @ Y2f\n",
    "dX2 = conv2d_padded(Y2 * 2, K2, 3, 4, 6, 8)\n",
    "dK2 = conv2d_padded_dk(Y2 * 2, X2, 3, 4, 6, 8)\n",
    "\n",
    "tX2 = torch.tensor(X2, requires_grad=True)\n",
    "tK2 = torch.tensor(K2, requires_grad=True)\n",
    "tY2 = torch.nn.functional.conv_transpose2d(tX2, tK2, stride=(3, 4), \n",
    "                                           padding=(6, 8))\n",
    "tY2f = tY2.view(-1)\n",
    "te = torch.dot(tY2f, tY2f)\n",
    "te.backward()\n",
    "\n",
    "print(e)\n",
    "print(te.data.numpy())\n",
    "print(metrics.tdist(tY2.data.numpy(), Y2))\n",
    "print(metrics.tdist(tX2.grad.data.numpy(), dX2))\n",
    "print(metrics.tdist(tK2.grad.data.numpy(), dK2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution AddBias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to add a vector $b$ of size $C_Y$ to the output of the convolution.  \n",
    "The operation is broadcasted to every position in the input.  \n",
    "\n",
    "$$Y = X + b, \\space X, Y \\in \\mathbb{R}^{N, C, H, W}, \\space b \\in \\mathbb{R}^C$$ \n",
    "$$Y_{n,c,i,j} = X_{n,c,i,j} + b_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y}$$\n",
    "$$\\frac{\\partial E}{\\partial b_c} = \\sum_{n=0}^N \\sum_{i=0}^H \\sum_{J=0}^W   \\frac{\\partial E}{\\partial Y_{ncij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 8, 9)\n"
     ]
    }
   ],
   "source": [
    "def conv2d_bias_add(X, b):\n",
    "    return np.transpose(np.transpose(X, (0, 3, 2, 1)) + b, (0, 3, 2, 1))\n",
    "\n",
    "X = np.random.randn(4, 6, 8, 9)\n",
    "b = np.random.randn(6)\n",
    "\n",
    "print(conv2d_bias_add(X, b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31180.443713126795\n",
      "31180.445\n",
      "3.9100816682472546e-05\n",
      "0.0008785277254920673\n",
      "[ 129.81822   -22.769493  108.71103   139.54231    51.764565 -167.12312\n",
      "    2.09487    62.73004   285.674    -295.93918 ]\n",
      "[ 129.81824269  -22.76947692  108.71101132  139.54227146   51.76462611\n",
      " -167.12318224    2.09485557   62.73000509  285.67403302 -295.93914631]\n",
      "0.0008570048285231473\n",
      "[ -15.412226   53.548717   28.9291     92.12505   -20.475239  -42.663155\n",
      " -104.22132    12.306933   20.25196   -14.48221 ]\n",
      "[ -15.41222737   53.54872246   28.92910638   92.1250647   -20.47525047\n",
      "  -42.66316142 -104.22131218   12.30692726   20.25195988  -14.48219793]\n",
      "[-183.75158732 -159.68335086 -311.66476433  101.68080914]\n",
      "[-183.7516  -159.6833  -311.6647   101.68082]\n",
      "7.877045010856777e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 17, 24).astype(np.float32)\n",
    "K = np.random.randn(4, 3, 5, 8).astype(np.float32)\n",
    "b = np.random.randn(4).astype(np.float32)\n",
    "Y = conv2d_padded(X, K, 3, 4, 6, 8)\n",
    "Y = conv2d_bias_add(Y, b)\n",
    "Yf = np.reshape(Y, (-1,))\n",
    "e = Yf @ Yf\n",
    "\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tK = torch.tensor(K, requires_grad=True)\n",
    "tb = torch.tensor(b, requires_grad=True)\n",
    "tY = torch.nn.functional.conv2d(tX, tK, bias=tb,\n",
    "                                stride=(3, 4), padding=(6, 8))\n",
    "tYf = tY.view(-1)\n",
    "te = torch.dot(tYf, tYf)\n",
    "te.backward()\n",
    "\n",
    "dY = 2 * Y\n",
    "db = np.sum(dY, axis=(0, 2, 3))\n",
    "dK = conv2d_padded_dk(X, dY, 3, 4, 6, 8)\n",
    "dX = conv2d_padded_dx(K, dY, 3, 4, 6, 8)\n",
    "\n",
    "dK_sol = tK.grad.data.numpy()\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(e)\n",
    "print(te.data.numpy())\n",
    "print(metrics.tdist(Y, tY.data.numpy()))\n",
    "print(metrics.tdist(dK_sol, dK))\n",
    "print(dK_sol.ravel()[:10])\n",
    "print(dK.ravel()[:10])\n",
    "print(metrics.tdist(dX_sol, dX))\n",
    "print(dX_sol.ravel()[:10])\n",
    "print(dX.ravel()[:10])\n",
    "print(db)\n",
    "print(tb.grad.data.numpy())\n",
    "print(metrics.tdist(tb.grad.data.numpy(), db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation can be applied after a convolution transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39335.24326852916\n",
      "39335.24326852916\n",
      "3.1822902218206984e-14\n",
      "7.888666479021034e-13\n",
      "9.046082080092663e-13\n",
      "2.2737367544323206e-13\n"
     ]
    }
   ],
   "source": [
    "X2 = np.random.randn(*Y.shape)\n",
    "K2 = np.random.randn(*K.shape)\n",
    "b2 = np.random.randn(3)\n",
    "Y2 = conv2d_padded_dx(K2, X2, 3, 4, 6, 8)\n",
    "Y2 = conv2d_bias_add(Y2, b2)\n",
    "Y2f = Y2.reshape(-1)\n",
    "e = Y2f @ Y2f\n",
    "dY2 = Y2 * 2\n",
    "dX2 = conv2d_padded(dY2, K2, 3, 4, 6, 8)\n",
    "dK2 = conv2d_padded_dk(dY2, X2, 3, 4, 6, 8)\n",
    "db2 = np.sum(dY2, axis=(0, 2, 3))\n",
    "\n",
    "tX2 = torch.tensor(X2, requires_grad=True)\n",
    "tK2 = torch.tensor(K2, requires_grad=True)\n",
    "tb2 = torch.tensor(b2, requires_grad=True)\n",
    "tY2 = torch.nn.functional.conv_transpose2d(tX2, tK2, bias=tb2, \n",
    "                                           stride=(3, 4), \n",
    "                                           padding=(6, 8))\n",
    "tY2f = tY2.view(-1)\n",
    "te = torch.dot(tY2f, tY2f)\n",
    "te.backward()\n",
    "\n",
    "print(e)\n",
    "print(te.data.numpy())\n",
    "print(metrics.tdist(tY2.data.numpy(), Y2))\n",
    "print(metrics.tdist(tX2.grad.data.numpy(), dX2))\n",
    "print(metrics.tdist(tK2.grad.data.numpy(), dK2))\n",
    "print(metrics.tdist(tb2.grad.data.numpy(), db2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution by matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: input tensor of size $(N, C_X, H_X, W_X)$  \n",
    "$K$: kernel tensor of size $(C_Y, C_X, H_K, W_K)$  \n",
    "$Y$: output tensor of size $(N, C_Y, H_Y, W_Y)$\n",
    "\n",
    "$S_H$: Stride on the y-axis.    \n",
    "$S_W$: Stride on the x-axis.  \n",
    "\n",
    "Can be transformed into matrix multiplication: $M_K$ $M_X$.\n",
    "\n",
    "$M_K$ is just $K$ reshaped to size $(C_Y, C_X * H_K * W_K)$. Each row contains the ordered values for the dot product for each feature map of the kernel.\n",
    "\n",
    "$M_X$ is of size $(C_X * H_K * W_K, N * H_Y * W_Y)$. Each column contains the ordered values for the dot produt with the kernel, for each image and position in the output.  \n",
    "This matrix contains a lot of duplicates and can be quite big.  \n",
    "\n",
    "Finally the result is reshaped to a 4D-tensor of size $(C_Y, N, H_Y, W_Y)$, and then transposed to get the same order as $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8507597872912736e-14\n"
     ]
    }
   ],
   "source": [
    "def x2mat(X, K, sh, sw):\n",
    "    hy = int((X.shape[2] - K.shape[2]) / sh + 1)\n",
    "    wy = int((X.shape[3] - K.shape[3]) / sw + 1)\n",
    "    \n",
    "    mx = np.empty((X.shape[0], hy, wy,\n",
    "                   X.shape[1] * K.shape[2] * K.shape[3]))\n",
    "    \n",
    "    for n in range(X.shape[0]):\n",
    "        for i in range(hy):\n",
    "            for j in range(wy):\n",
    "                v = X[n, :, i*sh:i*sh+K.shape[2], j*sw:j*sw+K.shape[3]]\n",
    "                mx[n, i, j] = v.reshape(-1)\n",
    "     \n",
    "    return mx.reshape(-1, mx.shape[3])\n",
    "\n",
    "def conv2d_matmul(X, K, sh, sw):\n",
    "    \n",
    "    hy = int((X.shape[2] - K.shape[2]) / sh + 1)\n",
    "    wy = int((X.shape[3] - K.shape[3]) / sw + 1)\n",
    "    \n",
    "    mX = x2mat(X, K, sh, sw)\n",
    "    mK = K.reshape(K.shape[0], -1)\n",
    "    mY = mK @ mX.T\n",
    "    Y = mY.reshape(K.shape[0], X.shape[0], hy, wy)\n",
    "    Y = np.transpose(Y, (1, 0, 2, 3))\n",
    "    return Y\n",
    "\n",
    "X = np.random.randn(2, 3, 17, 24)\n",
    "K = np.random.randn(4, 3, 5, 8)\n",
    "Y = conv2d(X, K, 3, 4)\n",
    "Y2 = conv2d_matmul(X, K, 3, 4)\n",
    "\n",
    "print(metrics.tdist(Y, Y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: input tensor of size $(N, C, H_X, W_X)$  \n",
    "$Y$: output tensor of size $(N, C, H_Y, W_Y)$\n",
    "\n",
    "$K_H$: Kernel height.  \n",
    "$K_W$ Kernel width.  \n",
    "$S_H$: Stride on the y-axis.    \n",
    "$S_W$: Stride on the x-axis.\n",
    "\n",
    "$$H_Y = \\lfloor \\frac{H_X - K_H}{S_H} \\rfloor + 1$$  \n",
    "$$W_Y = \\lfloor \\frac{W_X - K_W}{S_W} \\rfloor + 1$$\n",
    "\n",
    "$$Y_{n,c,i,j} = \\max(X_{n,c,i*S_H:i*S_H+K_H,j*S_W:j*S_W+K_W})$$\n",
    "The kernel returns the maximum value in the applied region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(X, kh, kw, sh, sw):\n",
    "    \n",
    "    hy = int((X.shape[2] - kh) / sh) + 1\n",
    "    wy = int((X.shape[3] - kw) / sw) + 1\n",
    "    \n",
    "    Y = np.empty((X.shape[0], X.shape[1], hy, wy))\n",
    "    \n",
    "    for n in range(X.shape[0]):\n",
    "        for c in range(X.shape[1]):\n",
    "            for i in range(hy):\n",
    "                for j in range(wy):\n",
    "                    Y[n, c, i, j] = np.max(X[n, c, i*sh:i*sh+kh,\n",
    "                                             j*sw:j*sw+kw])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5, 3])\n",
      "(2, 3, 5, 3)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 18, 17).astype(np.float32)\n",
    "Y = maxpool(X, 2, 3, 4, 5)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tY = torch.nn.functional.max_pool2d(tX, (2, 3), (4, 5))\n",
    "\n",
    "print(tY.shape)\n",
    "print(Y.shape)\n",
    "print(metrics.tdist(Y, tY.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E}{\\partial X_I} = \\sum_{Y_J \\in \\text{Succ}(X_I)} \\frac{\\partial E}{\\partial Y_J} \\mathbb{1}_{X_I = Y_J}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_dk(X, Y, dout, kh, kw, sh, sw):\n",
    "    hy = int((X.shape[2] - kh) / sh) + 1\n",
    "    wy = int((X.shape[3] - kw) / sw) + 1\n",
    "    \n",
    "    dX = np.zeros(X.shape)\n",
    "    \n",
    "    for n in range(Y.shape[0]):\n",
    "        for c in range(Y.shape[1]):\n",
    "            for i in range(Y.shape[2]):\n",
    "                for j in range(Y.shape[3]):\n",
    "                    m = Y[n, c, i, j]\n",
    "                    for xi in range(i*sh,i*sh+kh):\n",
    "                        for xj in range(j*sw,j*sw+kw):\n",
    "                            if X[n, c, xi, xj] == m:\n",
    "                                dX[n, c, xi, xj] += dout[n, c, i, j]\n",
    "                                \n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(1, 1, 6, 6)\n",
    "Y = maxpool(X, 2, 2, 2, 2)\n",
    "dX = maxpool_dk(X, Y, 2*Y, 2, 2, 2, 2)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tY = torch.nn.functional.max_pool2d(tX, (2, 2), (2, 2))\n",
    "tYf = tY.view(-1)\n",
    "te = torch.dot(tYf, tYf)\n",
    "te.backward()\n",
    "\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(metrics.tdist(Y, tY.data.numpy()))\n",
    "print(metrics.tdist(dX, dX_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(1, 1, 5, 5)\n",
    "Y = maxpool(X, 2, 2, 1, 1)\n",
    "dX = maxpool_dk(X, Y, 2*Y, 2, 2, 1, 1)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tY = torch.nn.functional.max_pool2d(tX, (2, 2), (1, 1))\n",
    "tYf = tY.view(-1)\n",
    "te = torch.dot(tYf, tYf)\n",
    "te.backward()\n",
    "\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(metrics.tdist(Y, tY.data.numpy()))\n",
    "print(metrics.tdist(dX, dX_sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3, 9, 7)\n",
    "Y = maxpool(X, 3, 2, 2, 1)\n",
    "dX = maxpool_dk(X, Y, 2*Y, 3, 2, 2, 1)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "tY = torch.nn.functional.max_pool2d(tX, (3, 2), (2, 1))\n",
    "tYf = tY.view(-1)\n",
    "te = torch.dot(tYf, tYf)\n",
    "te.backward()\n",
    "\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(metrics.tdist(Y, tY.data.numpy()))\n",
    "print(metrics.tdist(dX, dX_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Response Normalization\n",
    "\n",
    "$$b^i_{x,y} = a^i_{x,y} \\left( k + \\alpha \\sum_{j=\\max(0,i-n/2)}^{\\min(N-1,i+n/2)} (a^j_{x,y})^2 \\right) ^{-\\beta}$$\n",
    "\n",
    "This operation takes a 4D tensor as input, it is applied right after a 2D Convolution, and helps for generalization.  \n",
    "It implements a form of lateral inhibition, and creates a form a competion with the biggest activity among neurons of different kernels.  \n",
    "$k$, $n$, $\\alpha$ and $\\beta$ are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.087681048627601e-16\n"
     ]
    }
   ],
   "source": [
    "def local_response_norm(n, alpha, beta, k, X):\n",
    "    \n",
    "    Y = np.empty(X.shape)\n",
    "    for e in range(X.shape[0]):\n",
    "        for i in range(X.shape[1]):\n",
    "            for x in range(X.shape[2]):\n",
    "                for y in range(X.shape[3]):\n",
    "                    val = X[e, i, x, y]\n",
    "                    ss = 0\n",
    "                    for j in range(max(0, i-n//2), \n",
    "                                   min(X.shape[1], i + n//2)):\n",
    "                        ss += X[e, j, x, y]**2\n",
    "                        \n",
    "                    den = (k + alpha * ss) ** beta\n",
    "                    Y[e, i, x, y] = val / den\n",
    "    \n",
    "    return Y\n",
    "\n",
    "def local_response_norm_dX(n, alpha, beta, k, X, dY):\n",
    "    \n",
    "    dX = np.empty(dY.shape)\n",
    "    #TODO\n",
    "    return dX\n",
    "    \n",
    "    \n",
    "\n",
    "X = np.random.randn(2, 14, 9, 7)\n",
    "Y = local_response_norm(6, 0.0001, 0.75, 1, X)\n",
    "\n",
    "tX = torch.tensor(X, requires_grad=True)\n",
    "lrn = torch.nn.LocalResponseNorm(6, 0.0001 * 6, 0.75, 1)\n",
    "tY = lrn(tX)\n",
    "tYf = tY.view(-1)\n",
    "te = torch.dot(tYf, tYf)\n",
    "te.backward()\n",
    "dX_sol = tX.grad.data.numpy()\n",
    "\n",
    "print(metrics.tdist(Y, tY.data.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
