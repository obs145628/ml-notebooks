{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A structured probabilistic (or graphical) model is graph which describes the direct interraction between random variables.  \n",
    "\n",
    "Use a graph to describe a probability distribution allows us to evercome many challenges, but finding which graph structure is most suited for a problem is not easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge of Unstructured Modeling\n",
    "\n",
    "The goal od DL is to understand high-dimensional data with rich structures, such as photos or speech records.  \n",
    "Classification algorithms can take a high-dimensional ata and give it a label.  \n",
    "\n",
    "Probalistic models need to do more complex tasks:\n",
    "- Density estimation: Estime the data generating distribution $p(x)$.\n",
    "- Denoising: Gin a damaged $\\tilde{x}$, find the original correct $x$.\n",
    "- Missing value imputation: Return estimates or a probability distribution over the missing elements of $x$.\n",
    "- Sampling: Generate new samples from $p(x)$.\n",
    "\n",
    "Modeling a rich distribution over millions of random variables is a computationally and statistically challenging task.  \n",
    "If we wish to model a distribution of $n$ discrete variables, taking each $k$ values, the naive approach of representing $P(x)$ by a lookup table would requires $k^n$ parameters. This is not feasible because:\n",
    "- Memory: can't store the whole representation\n",
    "- Statistical efficienty: Because they an huge nomber of parameters, a huge number of training examples is need.\n",
    "- Inference cost: If we want to compute the marginal distribution given we lernt a joint distribution for example, we would need to sum over the whole table.\n",
    "- Sampling cost: We would need to go through the whole table in the worst case.  \n",
    "\n",
    "The table based model every interracton between every variables, but in real life most variables influence each other only indirectly.  \n",
    "Graphical models only model direct interactions. The model has less parameters so it need less data and computations are faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Graphs to Describe Model Structure\n",
    "\n",
    "Each node represent a random variable, and each edge a direct interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Models\n",
    "\n",
    "The Directed graphical model is also called a belief network or Bayesian network.  \n",
    "They are directed edges. An edge from a to b means the distribution over b depends on a.  \n",
    "\n",
    "A directed acyclic graph $G$ defines a set of local conditional probabilisty distributions:\n",
    "$$p(x_i | P_{a_G}(x_i))$$\n",
    "\n",
    "with $P_{a_G}(x_i)$ the parents of node $x_i$ in $G$.  \n",
    "The joint distribution is given by:\n",
    "$$p(x) = \\prod_{x_i} p(x_i | P_{a_G}(x_i))$$\n",
    "\n",
    "Let $m$ the maximum number of parents any node. The complexity of a full table is $O(k^m)$, wich can reduce a lot with $m \\ll n$.  \n",
    "Other restrictions (such that the graph is the tree) assure faster computations.  \n",
    "\n",
    "Directed models are applicable when we know the causality, and it only flows in one direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undirected models\n",
    "\n",
    "Undirected models are also called Markov random fields (MRF) or Markov networks.  \n",
    "They are used when the interaction between variables have not a clear direction, or operate in both.  \n",
    "The value associated with the edges is not a condition probability distribution.  \n",
    "\n",
    "An undirected graph $G$ is complosed of several cliques $\\mathcal{C}$, each with a non-negative factor $\\phi(C)$ called clique potential, that mesures the affinity of $x_i \\in \\mathcal{C}$ for being in each of their possible joint states.  \n",
    "They define un unormalized probability distribution:\n",
    "$$\\tilde{p}(x) = \\prod_{\\mathcal{C} \\in G} \\phi(\\mathcal{C})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Partition Function\n",
    "\n",
    "We need to normalize to get a valid probability distribution:\n",
    "$$p(x) = \\frac{1}{Z} \\tilde{p}(x)$$\n",
    "\n",
    "with $Z$ the partition function, a sum or integral over all point joint assignments of $x$:\n",
    "$$Z = \\int \\tilde{p}(x)dx$$\n",
    "\n",
    "Usually $Z$ is intractable, and in DL we often use approximations.  \n",
    "\n",
    "The domain of each variable has a huge impact and the resulting probability distribution.  \n",
    "For example, $x$ n-dimensional random vector parametrized by a vector of biases $b$.  \n",
    "We have one clique per element, such as $\\phi^{(i)}(x_i) = \\exp (b_i x_i)$.  \n",
    "If $x \\in \\mathbb{R}^n$, $Z$ diverges and there is no probability distribution.  \n",
    "If $x \\in {0, 1}^{n}$, $p(x)$ factorizes into $n$ independent distributions $p(x_i = 1) = \\text{sigmoid}(b_i)$.  \n",
    "If $x \\in \\{ [1, 0, \\text{...}, 0], [0, 1, 0, \\text{...}, 0], \\text{...}, [0, 0, \\text{...}, 1] \\}$, then $p(x) = \\text{softmax}(b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy-Based Models\n",
    "\n",
    "An energy-based model is the speacial kind of undirected model:\n",
    "\n",
    "$$\\tilde{p}(x) = \\exp(-E(x))$$\n",
    "\n",
    "with $E(x)$ the energy function.  \n",
    "We can learn $E(x)$, and we are always sure to have $\\tilde{p}(x) \\geq 0$.\n",
    "\n",
    "Many energy-based models are called Boltzmann machines.  \n",
    "\n",
    "Because $exp(a)=exp(b)=ab$, Different cliques in the graph are ust different terms of the energy function.  \n",
    "\n",
    "Energy-based models can be seen as a product of experts. Each term in $E$ is an expert that determines if a particular soft contrainst is satisfied. Multiple experts together enforce higher dimensional constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation and D-Separation\n",
    "\n",
    "We can use the graph to known wich subsets of variables are independent from each other given the value of ther subsets of variables. This is called separation.  \n",
    "A set of variables $A$ is separated from another set $B$ given a third set of variables $S$ if $A$ is independant from $B$ given $S$.  \n",
    "\n",
    "Two variables $a$ and $b$ rae independant given an observered set $S$ if there is no path between $a$ and $b$ that do not pass by an observed variable.  \n",
    "Paths going through unobserved variables are said active, those through observed variables are said passive.  \n",
    "\n",
    "For directed graphs, there is the same concept, but called D-Separation.  \n",
    "\n",
    "Seperation tell us only about conditional independances implied by the graph, but there is no garanty gat it represent all independencies.  \n",
    "\n",
    "Context-specific independences are inpendance present only when a variable has a specific value, it can't be represented by a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting between Undirect and Directed Graphs\n",
    "\n",
    "No probabilistic model is undirected or directed, just some are more easlisy represented as one than as another.  \n",
    "They both have pros and cons, we should choose depending on the task.  \n",
    "\n",
    "We can choose depending on which approaches captures the most independencies (uses the fewest edges).  \n",
    "We may also switch while using the same model. It's often straightforward to sample from directed models, and undirect models are can approximate inference procedures.  \n",
    "\n",
    "Any distribution can be represented by a directed or undirected model, or a complete graph.  \n",
    "\n",
    "In a directed graph, when nodes $a$ and $b$ are parent of $c$, and there is no edge between $a$ and $b$, this structure is called an immorality. Undirected models can't represent it perfectly.  \n",
    "\n",
    "To convert a direct graph $D$ into an undirected graph $U$, for every pair $(x,y)$, there is an edge if $D$ contains an edge $x \\to y$ or $y \\to x$, or if $x$ and $y$ are both parent of any $z$.  \n",
    "We call $U$ a moralized graph.  \n",
    "\n",
    "Likewise, some structure of undirected models can't be represented by directed ones.  \n",
    "If $U$ has a loop of length $> 3$ we no chord, we need to add any chord first before converting it.  \n",
    "A chord is an edge between two non-consecutive variables in the loop.  \n",
    "The grap with added chords is called a chordal or triangulated graph (all loops are of size 3).  \n",
    "To convert $U$ to $D$, we must add direction to every edges, in a way that doesn't create any cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Graphs\n",
    "\n",
    "In an undirected graph, every $\\phi$ function is a subset of some clique in the graph. But it may have one factor over the whole clique, or several factors over different parts of the clique.  \n",
    "\n",
    "Factor graphs explicitly represent the scope of each $\\phi$ function. The undirect model also contains squares, corresponding to factors. There is an edge between a variable and a factor if the variable is an argument in the factor function. There edges between variables are not represented anymore.  \n",
    "\n",
    "Representation, inference and learning are cheaper in factor graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from Graphical Models\n",
    "\n",
    "Sampling from directed models is easy with ancestral sampling.  \n",
    "The variables are sorted by topological order, the the variables are sampled in that order.  \n",
    "It's easy to sample from $p(x)$ as long as it's easy to sample from each conditional distribution $p(x_i | P_{a_G}(x_i))$.\n",
    "\n",
    "To sample from undirect models, we couple convert it to direct ones, but it would requires solving intractable inference problems.  \n",
    "\n",
    "Every variable interacts with every other, so there is no clear beginning point for the sampling process.  \n",
    "Sample from an undirected model is an expensive multipass process.  \n",
    "\n",
    "One approach is Gibbs sampling.  \n",
    "We sample for each variable $x_i$ conditioned on all others other variables (just the neighbors of $x_i$ thanks to the graph structure).  \n",
    "After sampling from all $x_i$s, we have an inaccurate sample from $p(x)$.  \n",
    "By repeating the process and sampling several times using the updated values, it converges to a sample of $p(x)$.  \n",
    "But it's difficult to know when the sample reached a sufficiently accurate approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Structured Modeling\n",
    "\n",
    "Representating, learning, inference and sampling costs are reduced. This is possible because we decide not to model certain interractions.  \n",
    "\n",
    "They also allows us to separate knowdledge from learning / inference.  \n",
    "We can develop algorithms from broad classes of graph, and we can independantly design models that represent best the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about Dependencies\n",
    "\n",
    "A generative models to learn the distribution about the observable variables $v$ that are higly dependent. The DL solution to capture these dependencies is to introduce hidden latent variables $h$. The model can capture dependencies between $v_i$ and $v_j$ indirectly by direct dependecies between $(v_i, h)$ and $(v_j, h)$.  \n",
    "This results in a much smaller graph than without $h$, directly modelling interactions $(v_i, v_j)$.  \n",
    "\n",
    "Structure learning tries to find the structure of the graph to only connect tightly coupled variables.  \n",
    "It's usually a Greedy-Search procedure, with starts with an initial structure, evaluate it, and move to another structure with a few edges added / removed, that should increase the score.  \n",
    "\n",
    "Using latent variables instead of structure learning avoid doing many rounds of training. Using parameter learing we can learn a model with a fixed structure that finds the right marginal $p(v)$.  \n",
    "\n",
    "Another advantage is that $h$ provide an alternative representation for $v$. For example $h$ can be used for classification.  \n",
    "Many approaches of feature learning use latent variables. $\\mathbb{E}[h|v]$ is a good feature mapping for $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Approximate Inference\n",
    "\n",
    "We often train the model wiht maximum likelihood:\n",
    "$$\\log p(v) = \\mathbb{E}_{h \\sim p(h|v)}[\\log p(h,v) - \\log p(h|v)]$$\n",
    "\n",
    "We need to compute $p(h|v)$.  \n",
    "\n",
    "Inference problems trie to predicth the value of the probability distribution over some variables the value of other variables.  \n",
    "For DL most inference problems are intractable.  \n",
    "Instead we can use variational inference: approximate $p(h|v)$ by learning an approximate tractable distribution $q(h|v)$ as close to the true one as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DL Approach to Structured Probabilistic Models\n",
    "\n",
    "Most DL models have no or only one layer of latent variables, but use deep  computation graphs to define the conditional distributions inside the model.  \n",
    "DL models use distributed representation, they often have only one large layer of latient variables.  \n",
    "\n",
    "For distributed representations, all $v_i$ sis connected to many $h_j$, yelding a graph not sparse enough for tradional algorithms.  \n",
    "Most DL algorithms are designed to make Gibbs samping or variatonal inference efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Restricted Botlzmann Machines (RBM)\n",
    "\n",
    "\n",
    "602"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
