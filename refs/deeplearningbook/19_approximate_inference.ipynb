{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, we usually have a set of visible variables $v$, and hiden $h$.  \n",
    "Inference is usually to compute $p(h|v)$, or it's expectation. There are often necessary for maximum likelihood learning.  \n",
    "\n",
    "Many deep models (many latent layers) have intractable posterior distribution. This is due to interractions between latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference as Optimization\n",
    "\n",
    "Inference can be described as an optimization problem, and we can find an approximate solution to it.  \n",
    "We want to compute $\\log p(v;\\theta)$, mut it may be too costly to marginalize out $h$.  \n",
    "We can compute instead the evidence lower bound (ELBO):\n",
    "$$\\mathcal{L}(v, \\theta, q) = \\log p(v;\\theta) - D_\\text{KL}(q(h|v)||p(h|v;\\theta))$$\n",
    "with $q$ arbitrary distribution.  \n",
    "$L \\leq \\log p(v;\\theta)$, they are equal if $q$ is the same distribution as $p(h|v)$.  \n",
    "$\\mathcal{L}$ can be a lot more easy to compute for some $q$. We can rewrite it as:\n",
    "$$\\mathcal{L}(v, \\theta, q) = \\mathbb{E}_{h \\sim q}[\\log p(h,v)] + H(q)$$\n",
    "$$H(q) = - \\mathbb{E}_{h \\sim q}[\\log q(h|v)]$$\n",
    "\n",
    "We can think of inference as finding $q$ to maximizes $\\mathcal{L}$.  \n",
    "Exact inference search over a family of function that contains $p(h|v)$. We can make inference less expensive by:\n",
    "- Search over a restricted family of $q$\n",
    "- Une an imperfect optimization prodecure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization\n",
    "\n",
    "EM is a popular algorithm for training with latent variables. We alternate between 2 steps untils convergence:\n",
    "\n",
    "- E-step: Set $q(h^{(i)}|v) = p(h^{(i)}|v^{(i)};\\theta^{(0)})$ for all training examles. $q$ is set in terms of $\\theta^{(0)}$, and changing $\\theta$ doesn't afect $q$.\n",
    "- M-Step: maximize $\\sum_i \\mathcal{L}(v^{(i)},\\theta,q)$ w.r.t. $\\theta$.  \n",
    "\n",
    "It can be seen as maximizing $\\mathcal{L}$ w.r.t. $q$ on E-step, and $\\theta$ on M-step.  \n",
    "On M-step, we can take one or several gradient steps.  \n",
    "The M-step introduce a gap between $\\mathcal{L}$ and $\\log p(v)$ as $\\theta$ moves further away from $\\theta^{(0)}$. E-step reduces the gap to $0$ each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP Inference and Sparse Coding\n",
    "\n",
    "MAP Inference infer the most likely value:\n",
    "$$h^* = \\arg \\max_h p(h|v)$$\n",
    "\n",
    "MAP inference is a form of approximate inference, with $q$ required to be a Dirac distribution:\n",
    "$$q(h|v) = \\delta(h - \\mu)$$\n",
    "$q$ is entirely controller by $\\mu$. Dropping constants terms of $\\mathcal{L}$, we get:\n",
    "$$\\mu^* = \\arg \\max_\\mu \\log p(h=\\mu, v)$$\n",
    "\n",
    "This can be trained with EM. E-step use MAP inference to infer $h^*$, and M-step updatee $\\theta$ to increase $\\log p(h^*, v)$.  \n",
    "\n",
    "MAP inference is usually used for sparse coding.  \n",
    "We start with a sparse-inducing prior, such as a factorial Laplace:\n",
    "$$p(h_i) = \\frac{\\lambda}{2} \\exp (-\\lambda |h_i|)$$\n",
    "\n",
    "The visible units are generated with a linear transformation and adding Gaussian noise:\n",
    "$$p(x|h) = \\mathcal{N}(Wh + b, \\beta^{-1}I)$$\n",
    "\n",
    "Computing $p(h|v)$ is difficult because the latent variables are connected, and the sparse prior makes these interractions non-Gaussian.  \n",
    "We use MAP inference and learn the parameters by maximizing the ELBO.  \n",
    "\n",
    "Let $H$ matrix of all hidden units, and $V$ matrix of all visible units. The sparse coding process minimizes the following criterion:\n",
    "$$J(H,W) = \\sum_{i,j} |H_{i,j}| + \\sum_{i,j} (V - HW^T)^2_{ij}$$\n",
    "\n",
    "We can minimize $J$ by alternating minimize w.r.t. $H$ (convex) and w.r.t. $W$ (linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference and Learning\n",
    "\n",
    "We can maximize $\\mathcal{L}$ over a restricted family of $q$. This family should be chosen so it's easy to compute $\\mathbb{E}_q \\log p(h,v)$. We can do this by adding assumptions about how $q$ factorizes, for example the mean field approach impose $q$ to be factorial:\n",
    "$$q(h|v) = \\prod_i q(h_i|v)$$\n",
    "This graphical model approach is called stuctured variatonal inference.  \n",
    "\n",
    "We specify how $q$ factorizes, and then determines the optimal distribution.  \n",
    "For discrete $h$, we uasually use a table to store $q$ and optimize use classical optimization techniques.  \n",
    "For continuous $h$, we use calculus of variation to perform optimization over a space of function.  \n",
    "\n",
    "We can think of maximizing $\\mathcal{L}$ w.r.t $q$ as minimizing $D_\\text{KL}(q(h|v)||p(h|v))$, which is fitting $q$ to $p$. But this is usually done in the opposite direction, here we encourage $q$ to have low probability everywhere the posterior has low probability. We choose this direction because the computations involve take the expectation over $q$, which is simple because of the choice of $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Latent Variables\n",
    "\n",
    "Usually $q$ is defined as a lookup table. For example, if $h$ is binary, and we use the mean field assumption, we only need a vector $\\hat{h}$: $q(h_i = 1 | v) = \\hat{h}_i$.  \n",
    "We need ro optimize $q$ using basic optimization techniques. But it should be fast, because the optimization is done in the iner loop of training. A popular choice is to solve fixed-point equations:\n",
    "$$\\frac{\\partial}{\\partial \\hat{h}_i} \\mathcal{L} = 0$$\n",
    "\n",
    "We update iteratively all $\\hat{h}_i$ until we satisfy a convergence criterion.  \n",
    "\n",
    "### Example: Binary sparse coding\n",
    "\n",
    "The input $v \\in \\mathbb{R}^n$ is generated by adding Gaussian noise to the sum of $m$ different components. Each component is switched on or off by the corresponding hidden unit $h \\in \\{ 0, 1 \\} ^m$.\n",
    "$$p(h_i = 1) = \\sigma(b_i)$$\n",
    "$$p(v|h) = \\mathcal{N}(v; Wh, \\beta^{-1})$$\n",
    "with $b$ set of biases, $W$ weight matrix, and $\\beta$ diagonal matrix.  \n",
    "\n",
    "Let's suppose we want to train this model mith maximum likelihood:\n",
    "$$\\frac{\\partial}{\\partial b_i} \\log p(v) = \\mathbb{E}_{h \\sim p(h|v)} = \\frac{\\partial}{\\partial b_i} \\log p(h)$$\n",
    "\n",
    "This requires expectation w.r.t. $p(h|v)$, which is untractable.  \n",
    "\n",
    "We can solve the problem using variational inference, with a mean field assumption:\n",
    "$$q(h|v) = \\prod_i q(h_i|v)$$\n",
    "\n",
    "We represent q using a vector of probabilities: $q(h_i = 1 | v) = \\hat{h}_i$. We add rescrition that $\\hat{h}_i$ is never $0$ or $1$, this donne using an unrestricted vector $z$, and we set $\\hat{h} = \\sigma(z)$.  \n",
    "This gives us a tractable $\\mathcal{L}$.  \n",
    "\n",
    "This is possible to optimize $\\mathcal{L}$ for both $v$ and $h$, but:\n",
    "- We need to store a $\\hat{h}$ for each $v$ (doesn't scale to big data)\n",
    "- We need to extract $\\hat{h}$ very quickly from $v$.  \n",
    "\n",
    "We don't optimize for $\\hat{h}$, instead we rapidly find a local maxium:\n",
    "$$\\nabla_h \\mathcal{L}(v, \\theta, \\hat{h}) = 0$$\n",
    "We iteratively solve until convergence:\n",
    "$$\\frac{\\partial}{\\partial \\hat{h}_i}\\mathcal{L}(v, \\theta, \\hat{h}) = 0$$\n",
    "\n",
    "It can be more advantageous to update several units at once. Some models like RBMs are structured in a way that allow block updates, but not sparse coding.  \n",
    "\n",
    "Another technique is damping: we solve for the individual optimal values, and move all units a little in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus of Varitations\n",
    "\n",
    "Many ML algorithms minimize a funcion $J(\\theta)$ by finding $\\theta \\in \\mathbb{R}^n$ for which $J$ takes on its minimal value.  \n",
    "A functional $J[f]$ is a function of functions. We can take  variational derivaties of a function w.r.t. indivual values $f$ at any specific value of $x$.  \n",
    "The function derivate of $J$ w.r.t. function $f$ at point $x$ is denoted:\n",
    "$$\\frac{\\delta}{\\delta f(x)} J$$\n",
    "\n",
    "We can optimise a functional by solving for the function were the functional derivative at every point is $0$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Find the probabability distribution over $x \\in \\mathbb{R}$ that has maximal differential entropy. We want to optimize:\n",
    "$$H[p] = \\mathbb{E}_x \\log p(x)$$\n",
    "\n",
    "We need to ue Lagrange multipliers for:\n",
    "- make sure $p(x)$ integrates to $1$.\n",
    "- Fix the variance $\\sigma^2$, otherwhise the entropy increases without bonds.\n",
    "- Fix the mean $\\mu$, otherwhise shifting the distriubtion yields another one with same entropy.\n",
    "\n",
    "The lagangrial functional is:\n",
    "$\\mathcal{L}[p] = \\lambda_1(\\int p(x)dx - 1) + \\lambda_2 (\\mathbb{E}[x] - \\mu) + \\lambda_3 (\\mathbb{E}[(x - \\mu)^2] - \\sigma^2) + H[p]$$\n",
    "\n",
    "We need to find $p$ such that:\n",
    "$$\\forall x, \\frac{\\delta}{\\delta p(x)} \\mathcal{L} = 0$$\n",
    "We obtain:\n",
    "$$p(x) = \\exp (\\lambda_1 + \\lambda_2 x + \\lambda_3 (x - \\mu)^2 - 1)$$\n",
    "\n",
    "We also need to choose the $\\lambda$ values to satisfy the constraints. For example, it could be $\\lambda_1 = 1 - \\log \\sigma \\sqrt{2 \\pi}$, $\\lambda_2 = 0$, $\\lambda_3 = - \\frac{1}{2\\sigma^2}$. We obtain:\n",
    "$$p(x) = \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "That's why the Gaussian is the nonimformative prior, a Gaussian has maximum entropy, we impose the least possible amount of structure.  \n",
    "\n",
    "There is no critical points (function) for minimum entropy. As functions place more probability on $x = \\mu \\pm \\sigma$, and less probability on all other points, they lose entropy. There is no single minimal as there is no minimal positive real number.  \n",
    "One solution would be to place zero mass on all but two points, using a mixture of Dirac, but they can't be describe by a single function, and cannot be find by our optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Latent Variables\n",
    "\n",
    "We need to use calculus of variations when maximizing $\\mathcal{L}$ w.r.to $q(h|v)$.  \n",
    "\n",
    "Usually, we just use proposed equations. For example, for the mean field approximation:\n",
    "$$q(h|v) = \\prod_i q(h_i|v)$$\n",
    "The optimal unormalized distribution is:\n",
    "$$\\tilde{q}(h_i|v) = \\exp (\\mathbb{E}_{h_{-i} \\sim q(h_{-i}|v)} \\log \\tilde{p}(v,h))$$\n",
    "\n",
    "We only need to use calculus of variations to develop new forms of variational learning.  \n",
    "\n",
    "The solution is a fixed point equation, we can apply it for every value of $i$ until convergence to get the correct functional form of $q(h|v)$.  \n",
    "But it also tell us the functional form that the optimal solution will take, wheter we use the fixed-point equations or not to arrive there.  \n",
    "The usual technique is to take the functional form, regard some values as parameters, and learn them using usual optimization alorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between Learning and Inference\n",
    "\n",
    "Training using approximate inference tends to adapt the model in a way that makes the approximating assumptions more true.  \n",
    "\n",
    "Variatonal leanring increases $\\mathbb{E}_{h \\sim q} \\log p(v, h)$. This behavior cause our approximating assumptions to become self-fulfilling. Train a model with a unimoal approximate posterior yields a model with a posterior far closer to unimodal than with exact inference.  \n",
    "\n",
    "It's difficult to estimate the amount of harm caused by variational approximation. After trainin, we can estimate $\\log p(v;\\theta)$ and find the gap with $\\mathcal{L}$ for a specific value of $\\theta$. It doesn't mean that this is accurate for other values of $\\theta$, or that we found a godd $\\theta$.  \n",
    "We would need to know $\\theta^* = \\arg \\max_\\theta \\log p(v;\\theta)$.  \n",
    "If $\\max_q \\mathcal{L}(v, \\theta^*, q) \\ll p(v; \\theta^*)$, it might be that $\\theta^*$ can't be captured by our $q$ family, and learning will never approac $\\theta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Approximate Inference\n",
    "\n",
    "We can think of the optimization process as a function $f$ that maps input $v$ to a distributions $q^* = \\arg \\max_q \\mathcal{L}(v, q)$.  \n",
    "We can approximate this using a neural network $\\hat{f}(v, \\theta)$\n",
    "\n",
    "## Wake-Sleep\n",
    "\n",
    "When training to infer $h$ from $v$ we don't have a supervised training set.  \n",
    "The Wake-sleep algorithm draw samples of both $v$ and $h$ from the model.  \n",
    "In a directed model, we can use ancestral simpling to go from $h$ and to $v$, and we can learn the reverse mapping $v$ to $h$. But this approach learns on inputs that have high probability according to the model, not the training data.\n",
    "\n",
    "## Other Forms of Learned Infeence\n",
    "\n",
    "A single pass in a learned inference network yield faster inference than iterating the mean field point equation. It's trained by running the network, improving the estimate with one step of mean-field, then update network to refine its estimate.  \n",
    "\n",
    "Learned approximate inference is often used for generative modeling, with variational autoencoder. The inference network define $\\mathcal{L}$, and the parameters are adapter to increase $\\mathcal{L}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
