{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Definition\n",
    "\n",
    "A definition of machine leanring might be:  \n",
    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measures P, if its performance at task T, as muesured by P, improves by experience E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task T\n",
    "\n",
    "- Classification: Learning wich of k classes an input belong to, and predict the class of unseen data. It may output a class or a probability distribution over the classes.  \n",
    "\n",
    "\n",
    "- Classification with missing inputs: Some of the input data may have missing features. One strategy is to learn several models, depending on the missing inputs.  \n",
    "\n",
    "\n",
    "- Regression: The program learns to predict a real number given the input. \n",
    "\n",
    "\n",
    "- Transcription: Transform a relatively unstructed representation of some kind of data into discrete, textitual form. Eg: optical character recognition, speech recignition.  \n",
    "\n",
    "\n",
    "- Machine translation: Convert a sequence of symbols from one language to another language.\n",
    "\n",
    "\n",
    "- Structured output: Any task where the output is multiple data with relationships between the elements. Eg:Pixel-wise segmentation, assign a label to every pixel in an image.  \n",
    "\n",
    "\n",
    "- Anomaly detection: Recognize anormal examples from a set of inputs. Eg: Credit card fraud detection.  \n",
    "\n",
    "\n",
    "- Synthesis and sampling: Generate new examples similar to those in the training data.\n",
    "\n",
    "\n",
    "- Imputation of missing values: The input data has some missing features, and the algorithm must predict their values.\n",
    "\n",
    "\n",
    "- Denoising: The input is a corrupted example generated by a corruption process from a clean example, and the algorithm must retrieve the clean example.\n",
    "\n",
    "\n",
    "- Density estimation or probability mass function estimation: Learning the probability density function $p(x)$ of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Performance measure P\n",
    "\n",
    "Common performance measure are the accuracy or the error rate. Usually they are measured on a test set, unseen during training, in order to evaluate how the algorithm generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Experience E\n",
    "\n",
    "Most algorithims experience a dataset. There are several kind of algorithms dependenting of the type of dataset:\n",
    "\n",
    "- Unsupervised learning: The dataset contains many features, and we try to  learn the structure of the data. They usually some form or implicit or explicit density estimation or clustering.  \n",
    "\n",
    "\n",
    "- Supervised learning: Each example has many features, and the associated label. The algorithm learn to predict the label of new, unlabelled data.\n",
    "\n",
    "\n",
    "- Semisupervised learning: Some examples has labels, some examples do not.\n",
    "\n",
    "\n",
    "- Reinforcment learning: The learning agent interacts with an environement, and the experience is changing over time. The goal is to maximize the expected reward when interacting wih the environment.  \n",
    "\n",
    "The limit between them is blurry, some algorithms may both use unsupervised and supervised components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacity, Overfitting and Underfitting\n",
    "\n",
    "The challenge is generalization: perfom well on new, previously unseen data.  \n",
    "When training a model, we recude the error on the training set. What's diffferent from optimization is that we also want to recude the generalization error. This is the expected error on new data.  \n",
    "The generalization error is often estimated by measuring the error on a test set.  \n",
    "\n",
    "The datasets are generated from a probability distribution. We make some assumptions called the i.i.d. assumptions:\n",
    "- each example is independant\n",
    "- the train and test set are identically distributed.  \n",
    "\n",
    "We suppose both the train and test set are i.i.d. samples of $p_\\text{data}$.  \n",
    "A ML algorithm performs well if:\n",
    "1. The training error is small\n",
    "2. The gap between training and test error is small\n",
    "\n",
    "These leads to 2 problems when training a model:\n",
    "- Underfitting: the training eror is too large\n",
    "- Overfitting: the training error is low but the test error is too large.\n",
    "\n",
    "The underfitting / overfitting of a model is controlled by it's capacity or complexity: it's ability to fit a wide variety of functions.  \n",
    "It is controlled by the hyothesis space, the set of functions that a model can learn.   \n",
    "Eg: for lineare regression, it is all linear functions. By adding polynomial terms, we can increase the model capacity.  \n",
    "\n",
    "We need to find the right model capacity depending on the task. Insufficient capacity leads to underfitting, and too much capacity leads to overfitting.  \n",
    "\n",
    "Some models can have a different capacity depending on they parameters, this is the representational capacity of the model.  \n",
    "\n",
    "Occam's razor: Among hypotheses that are equally weel, one should choose the simplest one.  \n",
    "\n",
    "the Vanpik-Chervonenkis dimension (VC dimension) can measure the capacity of a binary classifier.  \n",
    "It is very difficult to measure exactly the capacity of a deep learning models.  \n",
    "\n",
    "Non parametric models have their capacity a function of the dataset.  \n",
    "\n",
    "The Bayes error is a error made by a theorical model that knows the true data distribution. There is still some error becuse of the noise in the distribution.  \n",
    "\n",
    "Mode data usually yields better generalization.  \n",
    "\n",
    "No Free Lunch theorem: averaged over all possible data distributions, every classifier has the same error when classyfying unseen data. A DL model has the same performance that a random classifier.  \n",
    "This assumption doesn't hold when we make assumptions about the data distributions, for example in real-life applications.  \n",
    "\n",
    "The ML goal is not to find an universal model, but to understand the data distributions relevant to the real world applications, and which algorithms perform well on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization \n",
    "\n",
    "Removing or adding functions from the hypothesis space is not the only way to control the model capacity. We can also change the learning algorithm to give him preferences to specific functions in the hypothesis space.  \n",
    "For example, we can add weight-decay to models with a cost function:\n",
    "\n",
    "$$J(w)  = \\text{MSE}_\\text{train} + \\lambda w^Tw$$  \n",
    "\n",
    "The chosen weights are a trade-off between fitting the training data and having small weights. This alternate the model capacity and helps reduce overfitting.\n",
    "\n",
    "They are many other ways to express preferences for different solutions, both implicitly and explicitly.  \n",
    "\n",
    "Regularization is any modification we make to a leanring algorithm that is intended to reduce its generalization error but not its training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Validation Sets\n",
    "\n",
    "Hyperparameters control the behavior of the model, they are fixed before and not learned by te algorithm.  \n",
    "Examples: the degree of a polynomial regressor, or the weight decay coefficient for regularization.  \n",
    "\n",
    "Usually these are parameters that are not appropiate or easy to learn from the training set. This applies to all parameters that controls the model capacity, if learn on the training set they would always choose the one with the biggest capacity and overfit.\n",
    "\n",
    "In order to choose the hyperparameters, we take a small part of the training set, that we call the validation set. The validation is used to estimate the generalization error during training and update the hyperparameters accordingly. Usually we split 80% training 20% validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "When the dataset is small, dividing it into three parts create a really small test part, making it a poor approximation of the generalization error.  \n",
    "\n",
    "We can split the dataset into $k$ subsets, and train $k$ different models.  For each model, one different subset is used as a test set, and the others as the training set. The generalization error is estimated by taking the average test error over the $k$ models.  \n",
    "Cross-Validation should not be used if the dataset is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators, Bias and Variance\n",
    "\n",
    "## Point Estimation\n",
    "\n",
    "Point estimation try to provide the single best prediction of some quantity of interest. It's an general a point estimate $\\hat{\\theta}$ of a vector of parameters $\\theta$.  \n",
    "A point estimator is any function of the data. A good estimators returns an estimate $\\hat{\\theta}$ as close as possible to the true $\\theta$.  \n",
    "\n",
    "\n",
    "In function estimation, we assume there is a function $f(x)$ that describes the relation between $x$ and $y$, and we are trying to find an estimate $\\hat{f}$, this is a point estimator in a function space.\n",
    "\n",
    "\n",
    "## Bias\n",
    "\n",
    "The bias of an estimaor is defined as:\n",
    "\n",
    "$$\\text{bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$$\n",
    "\n",
    "where the expectation is over the training data.\n",
    "An estimator is unbiased if $\\text{bias}(\\hat{\\theta}) = 0$.\n",
    "\n",
    "We prefer an estimate with low bias.\n",
    "\n",
    "## Variance and Standard Error\n",
    "\n",
    "The variance of an estimator is simply:\n",
    "$$\\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] = \\mathbb{E}[\\hat{\\theta}^2] - \\mathbb{E}^2[\\hat{\\theta}] $$\n",
    "\n",
    "The square root of the variance is the standard error, denoted $\\text{SE}(\\hat{\\theta})$.\n",
    "\n",
    "The variance tells us how much the estimate we compute vary as we resample the dataset.  \n",
    "We also prefer an estimate with low variance.\n",
    "\n",
    "The standard eror of the mean is:\n",
    "$$\\text{SE}(\\hat{\\mu}) = \\sqrt{\\text{Var} \\left[ \\frac{1}{m} \\sum_{i=1}^m x^{(i)} \\right]} = \\frac{\\sigma}{\\sqrt{m}}$$\n",
    "\n",
    "$\\sigma$ is often remplaced by the sample variance or the unbasied sample variance, even though they tend to underestimate the true standard deviation. For large $m$, the approximation is quite reasonable.\n",
    "\n",
    "Sample variance:\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\hat{\\mu})^2$$\n",
    "\n",
    "Unbiased sample variance:\n",
    "$$\\tilde{\\sigma}^2 = \\frac{1}{m-1} \\sum_{i=1}^m (x^{(i)} - \\hat{\\mu})^2$$\n",
    "\n",
    "with $\\mu$ the sample mean:\n",
    "$$\\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}$$\n",
    "\n",
    "We can estimate the generalization error with the sample mean of the error on the test set.  \n",
    "The central limit theorem tells us that the mean will be approximatley distritbuted with a normal distribution. We can build a 95% confidence interval on the generalization error:\n",
    "$$(\\hat{\\mu} - 1.96 \\text{SE}(\\hat{\\mu}), \\hat{\\mu} + 1.96 \\text{SE}(\\hat{\\mu}))$$\n",
    "\n",
    "We say that algrithm A is better than algorithm B if the upper bound of the 95% interval for the error of algorithm A is less than the lower bound of the 95% interval for the error algorithm B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading off Bias and Variance to minimize MSE\n",
    "\n",
    "Bias measure the expected deviation from the true value, and variance measure the deviation from the estimator value that any particular sampling data is likely to clause, these are 2 differents sources of errors.  \n",
    "\n",
    "To chose between 2 models, we can compae the MSE of the estimates:\n",
    "$$\\text{MSE} = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Bias}(\\hat{\\theta})^2+ \\text{Var}(\\hat{\\theta})$$\n",
    "\n",
    "We want an estimator with small MSE.  \n",
    "Increasing capacity (ovefitting) tends to increase variance and decrease bias.  \n",
    "On the other hand, decreasing capacity (underfitting) tends to decrease variance and increase bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "\n",
    "(Weak) Consistency is that has the number of exemples increase, our point estimate converges to the true value:\n",
    "$$\\text{plim}_{m \\to \\infty} \\hat{\\theta}_m = \\theta$$  \n",
    "\n",
    "Strong consistency refers to almost sure convergence:\n",
    "$$P(\\lim_{m \\to \\infty} \\hat{\\theta}_m = \\theta) = 1$$  \n",
    "\n",
    "Consistency ensures that the bias diminishes as the number of examples grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Consider a set of $m$ examples: $X = \\{ x^{(1)}, \\text{...}, x^{(m)} \\}$ iid. samples of the unknown $p_\\text{data}(x)$.\n",
    "Let $p_\\text{model}(x, \\theta)$ a parametric model that estimates $p_\\text{data}(x)$.\n",
    "\n",
    "The maximum likelihhod estimator for $\\theta$ is defined as:\n",
    "$$\\theta_\\text{ML} = \\arg \\max_{\\theta} p_\\text{model}(X;\\theta)$$  \n",
    "\n",
    "For simpler computions, it can be rewritten as:\n",
    "$$\\theta_\\text{ML} = \\arg \\max_{\\theta} \\sum_{i=1}^m \\log p_\\text{model}(x^{(i)};\\theta)$$ \n",
    "\n",
    "Let $\\hat{p}_\\text{data}$ the empricial data distribution of the data, we get:\n",
    "\n",
    "$$\\theta_\\text{ML} = \\arg \\max_{\\theta} \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\log p_\\text{model}(x;\\theta)$$   \n",
    "\n",
    "We can also minimize the the dissimilarity between $\\hat{p}_\\text{data}(x)$ and $p_\\text{model}(x;\\theta)$ with the KL divergence:\n",
    "$$D_\\text{KL}(\\hat{p}_\\text{data}||p_\\text{model}) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} [\\log \\hat{p}_\\text{data(x)} - \\log p_\\text{model}(x;\\theta)]$$\n",
    "\n",
    "The term on the left is a constant, and we get the same result as the maximum likelihood.  \n",
    "\n",
    "We can see maximum likelihood as an attempt to match the empirical data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional log-likelihood\n",
    "\n",
    "If we want to predict $y$ given $x$, we can estimate a conditional probabiliy:\n",
    "$$P(y|x;\\theta)$$\n",
    "\n",
    "We get the following maximum likelihood estimator:\n",
    "\n",
    "$$\\theta_\\text{ML} = \\arg \\max_\\theta \\sum_{i=1}^m \\log P(y^{(i)}|x^{(i)};\\theta)$$\n",
    "\n",
    "### Connections to MSE\n",
    "\n",
    "The model tries to estimate the conditional distribution $p(y|x)$:\n",
    "$$p(y|x;\\theta) = \\mathcal{N}(\\hat{f}(x;\\theta), \\sigma^2)$$\n",
    "\n",
    "with $\\hat{f}(x;\\theta)$ a function parametrized by $\\theta$ that predicts the mean of the Gaussian.  \n",
    "\n",
    "The log-likelihood estimate is:\n",
    "$$\\hat{\\theta} = \\arg \\max \\sum_{i=1}^m \\log p(y^{(i)}|x^{(i)};\\theta)$$\n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\max -m \\log \\theta - \\frac{m}{2}\\log (2\\pi) - \\sum_{i=1}^m \\frac{||\\hat{f}(x^{(i)},\\theta) - y^{(i)}||^2}{2\\sigma^2}$$\n",
    "\n",
    "Removing the constant terms, maximizing the log-likelihood is equivalent to minimizing the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML estimator is the best estimator as $m \\to \\infty$.  \n",
    "The estimator is consistent under the following criterions:\n",
    "- $p_\\text{data}$ lie within the model family $p_\\text{model}(x;\\theta)$.\n",
    "- $p_\\text{data}$ must correspond to exactly one value of $\\theta$ (no more), otherwhise the model can't determine which $\\theta$ was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "\n",
    "In the frequentist perspective, the true $\\theta$ is fixed but unknown, and the point estimate $\\hat{\\theta)$ is a function of the random dataset.  \n",
    "\n",
    "In the Bayesian perspective, the dataset is not random but directly observed, and $\\theta$ is uncertain and represented with a probability distribution.  \n",
    "\n",
    "Let $p(\\theta)$ the prior, the knowledge of $\\theta$ before seeing the data. We can take one with high entropy to represent uncertainty (eg: Gaussian is a non-informative prior). Or we can take a prior to reflect a preference into specific solutions (eg: smaller magnitude, near constant). \n",
    "\n",
    "Let the dataset $X = \\{ x^{(1)}, \\text{...}, x^{(m)} \\}$. We call $p(X|\\theta)$ the data likelihood. We can use Bayes rules to estimate our belief of $\\theta$ after seeing $X$:\n",
    "$$P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}$$\n",
    "\n",
    "The predicted distribution of a new sample is given by:\n",
    "$$p(x_\\text{new}|X) = \\int p(x_\\text{new}|\\theta)p(\\theta|X)d\\theta$$\n",
    "\n",
    "After observing $X$, if we are still uncertain about $\\theta$, this uncertainty is incorporated directly into the new predictions.\n",
    "\n",
    "The prior has in fluence by shifting probability mass density towards regions that are prefered apriori. The prior is a subjective human choice and may impact the predictions.  \n",
    "Bayesian methods usually generalize much better with limited data, but suffer for high computatioal cost with too much data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "Let $x \\in \\mathbb{R}^n$, $y \\in \\mathbb{R}$. The prediction is parametrized by $w \\in \\mathbb{R}^n$:\n",
    "$$\\hat{y} = w^Tx$$\n",
    "We have $m$ training examples:\n",
    "$$\\hat{y} = Xw$$\n",
    "\n",
    "The data likelihood is expressed a a Gaussian:\n",
    "$$p(y|X,w) = \\mathcal{N}(Xw, I)$$\n",
    "\n",
    "We define a Gaussian prior:\n",
    "$$p(w) = \\mathcal{N}(\\mu_0, \\Lambda_0)$$\n",
    "\n",
    "Using Bayes Theorem, we have:\n",
    "$$p(w|X,y) \\propto p(y|X,w)p(w)$$\n",
    "\n",
    "Let's define $\\Lambda_m = (X^TX + \\Lambda_0^{-1})^{-1}$ and $\\mu_m = \\Lambda_m (X^Ty + \\Lambda_0^{-1} \\mu_0)$.\n",
    "\n",
    "With these new variables, the posterior can be written as a Gaussian:\n",
    "$$p(w|X,y) = \\mathcal{N}(\\mu_m, \\Lambda_m)$$\n",
    "\n",
    "In most cases, we set $\\mu_0=\\vec{0}$. If we set $\\Lambda_0 = \\frac{1}{\\alpha} I$, we get the following posterior:\n",
    "\n",
    "$$p(w|X,y) = \\mathcal{N}((X^TX + \\alpha I)^{-1}Xy, (X^TX + \\alpha I)^{-1})$$ \n",
    "\n",
    "We get the same estimate for $\\mu_m$ as for ridge regression, and we also have a covariance matrix that gives us uncertainty information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posterior (MAP) Estimation\n",
    "\n",
    "Make predictions with the full Bayesian posterior is computationally expensive or intractable.  \n",
    "Another approach is to get only a point estimate, and we still have some benefits of the Bayesian approach such as the choice of a prior.  \n",
    "\n",
    "One solution is to choose MAP point estimate, this is the point of maximal posterior probability:\n",
    "\n",
    "$$\\theta_\\text{MAP} = \\arg \\max_\\theta p(\\theta|x) = \\arg \\max_\\theta \\log p(x|\\theta) + \\log p(\\theta)$$\n",
    "\n",
    "$\\log p(x|\\theta)$ is the log-likelihood term (MLE)\n",
    "\n",
    "For example, linear regression with a Gaussian prior $\\mathcal{N}(0, \\frac{1}{\\lambda}I^2)$ add the term $\\lambda w^TW$, which gives us the same estimate as ridge regression.  \n",
    "\n",
    "MAP estimates uses information brough by the prior, this helps reduce variance, but tends to increse the bias.  \n",
    "\n",
    "MAP Bayesian inference provides a straightforward way to design complicated yet interpretable regularization terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Algorithms\n",
    "\n",
    "## Probabilistic Supervised Learning\n",
    "\n",
    "Most supervised algorithms estimate a distribution $p(y|x)$. It's usually done by maximum likelihood estimation to find the best $\\theta$ of parametric model distribution.  \n",
    "For linear regression, we have:\n",
    "$$p(y|x;\\theta) = \\mathcal{N}(\\theta^Tx, I)$$\n",
    "\n",
    "For binary classification, we can use logistic regression, based on the sigmoid function:\n",
    "$$p(y=1|x;\\theta) = \\sigma(\\theta^Tx)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "SVM is used for binary classification. It predicts positive class when $x^Tx + b > 0$, negative otherwhise.  \n",
    "\n",
    "SVM can use the kernel trick. A model can use the kernel trick when all computations can be done exclusively by using dot-products between the examples. For SVM, we get:\n",
    "$$w^Tx + b = b + \\sum_{i=1}^m \\alpha_i x^Tx^{(i)}$$\n",
    "with $\\alpha$ vector of coefficients.  \n",
    "\n",
    "We can replace $x$ by the output of a given feature function $\\phi(x)$, and the dot-product with $k(x, x') = \\phi(x)^T\\phi(x')$. \n",
    "\n",
    "The predictions are non-linear with respect to $x$, but linear with respect to $\\phi(x)$. This is equivalent to learning a linear model with $\\phi(x)$ as observations.  \n",
    "Kernel tricks has 2 advantages:\n",
    "- We can learn non-linear models using convex optimization techniques.\n",
    "- Using the kernel trick is much more efficient than computing $\\phi(x)$ than applying the model on the transformed inputs. We can even use infinite $\\phi(x)$ with this method.  \n",
    "\n",
    "The most commonly used kernel is the Gaussian or Radial Basis Function (RBF) kernel:\n",
    "$$k(u, v) = \\mathcal{N}(u - v; 0, \\sigma^2I)$$\n",
    "\n",
    "We can think of Gaussian kernel as template matching. When a test point $x'$ is near to an observation $x$ (Euclidian distance), $k(x,x')$ is large, and the model put a large weight on the corresponding training label $y$. The prediction is an average of all training labels weighted by the similarity of the corresponding examples.  \n",
    "\n",
    "A major drawback is that the prediction cost is linear in the number of training examples. SVM can mitigeate this by learning a sparse $\\alpha$. Training examples corresponding to non-zero $\\alpha_i$ are called support vectors.  \n",
    "Training is also very long with a large dataset. And they also have difficulties to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Simple Supervised Learning Algorithms\n",
    "\n",
    "k-nrearest neighbors is a nonparametric model for regression and classification. It does not have any parameters or training phase, it just uses the training data. For prediciton, it fin the k closest $x$ in the dataset, and return the average of the corresponding $y$.  \n",
    "\n",
    "knn has high capacity, and can have high accuracy with a big training set, but the predictions is linear in the number of training examples.  \n",
    "\n",
    "It may generalizes badly with small datasets, and cannot learn than one feature is more important than others.  \n",
    "\n",
    "Decision tree also breaks the input space into regions, with separate parameter for each. Each node of the tree is associated to a region, and they are formed by axis-aligned cuts. Usually, each leaf node map the whole to the same output.  \n",
    "\n",
    "This is a non-parametric model can can learn a tree of arbitrary size, but they often regalurized with a lot of size contraints, and they are more like parametric models in practice.  \n",
    "Trees might struggle to solve some basic problems because of their axis-aligned cuts. When the decision boundary is not axis-aligned, the tree can try to approximate it with many cuts, implementing a kind of step function.  \n",
    "\n",
    "KNN and decision trees have many limitations, but they can be useful with limited computational ressources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Algorithms\n",
    "\n",
    "Unsupervised learning refers to methods to extract information from data not annotated by a human. It cover several domains:\n",
    "- density estimation\n",
    "- sample from a distribution\n",
    "- denoising\n",
    "- find a manifold of the data\n",
    "- cluster the data\n",
    "\n",
    "A classic task if to find a representation of the data that preverses as much information as possible while obeying some constraint to have a simple or more usefull representation than $x$ itself.  \n",
    "A simpler representation is often one of these:\n",
    "- A low-dimensional representation: compress $x$.\n",
    "- A sparse representation: increase the number of features, but they are mostly zeroes.  \n",
    "- An independant representation: disentangle the source of variation underlying the data distribution.\n",
    "\n",
    "Representation is one of the cental themes of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "PCA learns a representation with lower dimensionality, and with linearly decorrelated elements. This is a first step towards statistical independence. Full dependence must also remove the non-linear dependencies between variables.  \n",
    "\n",
    "PCA learns an orthogonal, linear transformation of $x$ to $z$.  \n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n*m}$ the data matrix of mean $0$.  \n",
    "\n",
    "The unbiased sample covariance matrix associated with $X$ is:\n",
    "$$\\text{Var}(x) = \\frac{1}{m - 1} X^TX$$\n",
    "\n",
    "PCA find $z=x^tW$ where $\\text{Var}(z)$ is diagonal.\n",
    "\n",
    "$X$ have the SVD decomposition $X=U\\Sigma W^T$,with $W$ the right singular vectors.  We get the original diagonalizaton:\n",
    "$$X^TX = W \\Sigma^2 W^T$$.\n",
    "\n",
    "We can express the variance of $X$ as:\n",
    "$$\\text{Var}(x) = \\frac{1}{m-1} W \\Sigma^2 W^T$$\n",
    "\n",
    "As required, the covariance of $Z$ is diagonal:\n",
    "$$\\text{Var}(z) = \\frac{1}{m-1} \\Sigma^2$$\n",
    "\n",
    "PCA is a representation that attemps to disentangle the underlying factors of variation of the data.  \n",
    "PCA find a rotation of the input space (described by $W$) that aligns the principal axes of variance with the basis of the new representation $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering\n",
    "\n",
    "$k$-means clustering divides the training set into $k$ clusters containing nearby points.  \n",
    "It can be considerer as providing a hone-hot representation ($h_i=1$ for current cluster), this is a sparse representation.  \n",
    "\n",
    "$k$-means intialize $k$ different centroids $\\mu^{(i)}$, and alternate between 2 steps until convergence:\n",
    "- Each training example is assigned to the nearest centroid $\\mu^{(i)}$\n",
    "- Each centroid $\\mu^{(i)}$ is updated to the mean of the training examples assigned to it.\n",
    "\n",
    "One difficulty is there is no single criterion to optimize. We can mesure the average euclidian distance between every point and it's assigned cluster, but it may not be a good property for real world tasks.  \n",
    "\n",
    "We usually prefer distributed representation to one-hot representation. Distributed representation can learn several attributes of the data. We don't know which attributes is usefull, and having several reduce the burden to know which one we care about, and we can measure similary between objects byy comparing many attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "When training DL models, the criterion is often a sum over trainining examples of a loss function:\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m L(x^{(i)}, y^{(i)}, \\theta)$$\n",
    "\n",
    "And the gradient is:\n",
    "$$\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\nabla_\\theta L(x^{(i)}, y^{(i)}, \\theta)$$\n",
    "\n",
    "With huge datasets, a single gradient step is really long.\n",
    "$J(\\theta)$ is an expectation over the training data. It can be approximated using a minibatch $B$ of size $m'$, with $m'$ sually small (1 to 100-200).  \n",
    "We get an estimate of the gradient:\n",
    "$$g = \\frac{1}{m'} \\sum_{i=1}^{m'} \\nabla_\\theta L(x^{(i)}, y^{(i)}, \\theta)$$\n",
    "\n",
    "SGD algorithms on non-convex problems convergs to a local minimu, but with DL models we often arrive at a low value of the cost function quickly enough to be usefull.  \n",
    "\n",
    "Prior to DL, non-linear models were usually trained with the kernel trick, build a $m * m$ matrix $G_{ij} = k(x^{(i)}, x^{(i')}$. $G$ was too large for use datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Machine Learning Algorithm\n",
    "\n",
    "A ML algorithm usually has the following blocks:\n",
    "- a dataset\n",
    "- a cost function\n",
    "- an optimization procedure\n",
    "- a model\n",
    "\n",
    "For example, for linear regresion we have:\n",
    "- a dataset: $X$ and $y\n",
    "- a cost function:\n",
    "\n",
    "$$J(w, b) = - \\mathbb{E}_{x, y \\sim \\tilde{p}_\\text{data}} \\log p_\\text{model}(y|x)$$\n",
    "    \n",
    "- optimization procedure: normal equations to solve for when the gradient is zero.\n",
    "- a model:\n",
    "$$p_\\text{model}(y|x) = \\mathcal{N}(x^Tw + b, 1)$$\n",
    "\n",
    "We can replace any components almost independantly of the others.  \n",
    "\n",
    "The most common cost function is negative log-liklihood.  \n",
    "We can also had regularization terms.  \n",
    "Sometimes we cannot compute the cost function, and we need algorithms that computes and iteratively optimize an estimate.  \n",
    "\n",
    "Most ML algorithm can be designed this way. Some special models (eg decision trees) have just a special optimizer (their cost function have flat regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges Motivating Deep Learning\n",
    "\n",
    "The simple ML algorithims can't solve real life AI problems.  \n",
    "DL try to overcome high dimensional data, and generalization when learning really complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "The number of possible configurations of a vector increases exponentially wiht the vector size.  \n",
    "Statical challenge: there is much more possible configurations of $x$ than training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Consistancy and Smoothness Regularization\n",
    "\n",
    "ML algorithms use priors about the function they should learn in order to generalize well.  \n",
    "\n",
    "Priors can be by defined as:\n",
    "- choising a specific kind of prior distribution of the parameters\n",
    "- algorithms biased toward chosing a class of functions over another\n",
    "\n",
    "Smoothness or local consistancy prior: The function we learn should not change very much within a small region:\n",
    "$$f(x) \\approx f(x + \\epsilon)$$\n",
    "But this prior is often not enough to generalize well to real AI tasks.  \n",
    "\n",
    "An extrample example of local consistancy is knn, the predictor is constant over regions of data with the ame neighbors.  \n",
    "Other kernel methods interpolate between the closest training examples. For example, local kernel $k(u,v)$ is large when $u$ and $v$ are really close, and decreases at they grow apart.  \n",
    "\n",
    "Decision trees or knn (k=1) suffers limitations of exclusively smoothness-based learning. The output is constant in specific regions, and to build $n$ accurate regions they need at least $n$ training examples.  \n",
    "\n",
    "Assuming only the smoothness of $f$ is not enough to represent functions that distinguish many more regions than the number of training examples.\n",
    "\n",
    "It's possible to learn a complicated function such chat it generalize well to new inputs. A very large number of regions can be defined an exponentially smaller number of examples by introducing some dependencies via assumptions about the underlying data distribution, allowing to generalize non-locally.  \n",
    "\n",
    "We asumme that the data was generated by the composition of factors of features, possibly in a multi-levels hierarchy. Using deep distributed representation counter the exponential challenges of the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning\n",
    "\n",
    "A manifold is a set of points, with a neighborhood around each point. They are transfromation to move from one point inside the manifold to a neighboring one.  \n",
    "In ML, this is a connected set of points that can be approximated by a small number of dimensions embedded in a higher-dimensional space. Each dimension corresponds to a local direction of variation.  \n",
    "\n",
    "In manifold learning we suppose most of $\\mathbb{R}^n$ is invalid input, and our models should only work well for a collections of manifolds in the data space.  \n",
    "\n",
    "The assumption that the data lies along a low-dimensional manifold is often correct, especially for images, sound or text.  \n",
    "\n",
    "When the data lies on a low-dimensional manifold, ML algorithms work better with  data in terms of the coordinates in the manifold, bit this is a challenging task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
