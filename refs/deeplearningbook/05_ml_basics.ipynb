{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Definition\n",
    "\n",
    "A definition of machine leanring might be:  \n",
    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measures P, if its performance at task T, as muesured by P, improves by experience E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task T\n",
    "\n",
    "- Classification: Learning wich of k classes an input belong to, and predict the class of unseen data. It may output a class or a probability distribution over the classes.  \n",
    "\n",
    "\n",
    "- Classification with missing inputs: Some of the input data may have missing features. One strategy is to learn several models, depending on the missing inputs.  \n",
    "\n",
    "\n",
    "- Regression: The program learns to predict a real number given the input. \n",
    "\n",
    "\n",
    "- Transcription: Transform a relatively unstructed representation of some kind of data into discrete, textitual form. Eg: optical character recognition, speech recignition.  \n",
    "\n",
    "\n",
    "- Machine translation: Convert a sequence of symbols from one language to another language.\n",
    "\n",
    "\n",
    "- Structured output: Any task where the output is multiple data with relationships between the elements. Eg:Pixel-wise segmentation, assign a label to every pixel in an image.  \n",
    "\n",
    "\n",
    "- Anomaly detection: Recognize anormal examples from a set of inputs. Eg: Credit card fraud detection.  \n",
    "\n",
    "\n",
    "- Synthesis and sampling: Generate new examples similar to those in the training data.\n",
    "\n",
    "\n",
    "- Imputation of missing values: The input data has some missing features, and the algorithm must predict their values.\n",
    "\n",
    "\n",
    "- Denoising: The input is a corrupted example generated by a corruption process from a clean example, and the algorithm must retrieve the clean example.\n",
    "\n",
    "\n",
    "- Density estimation or probability mass function estimation: Learning the probability density function $p(x)$ of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Performance measure P\n",
    "\n",
    "Common performance measure are the accuracy or the error rate. Usually they are measured on a test set, unseen during training, in order to evaluate how the algorithm generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Experience E\n",
    "\n",
    "Most algorithims experience a dataset. There are several kind of algorithms dependenting of the type of dataset:\n",
    "\n",
    "- Unsupervised learning: The dataset contains many features, and we try to  learn the structure of the data. They usually some form or implicit or explicit density estimation or clustering.  \n",
    "\n",
    "\n",
    "- Supervised learning: Each example has many features, and the associated label. The algorithm learn to predict the label of new, unlabelled data.\n",
    "\n",
    "\n",
    "- Semisupervised learning: Some examples has labels, some examples do not.\n",
    "\n",
    "\n",
    "- Reinforcment learning: The learning agent interacts with an environement, and the experience is changing over time. The goal is to maximize the expected reward when interacting wih the environment.  \n",
    "\n",
    "The limit between them is blurry, some algorithms may both use unsupervised and supervised components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacity, Overfitting and Underfitting\n",
    "\n",
    "The challenge is generalization: perfom well on new, previously unseen data.  \n",
    "When training a model, we recude the error on the training set. What's diffferent from optimization is that we also want to recude the generalization error. This is the expected error on new data.  \n",
    "The generalization error is often estimated by measuring the error on a test set.  \n",
    "\n",
    "The datasets are generated from a probability distribution. We make some assumptions called the i.i.d. assumptions:\n",
    "- each example is independant\n",
    "- the train and test set are identically distributed.  \n",
    "\n",
    "We suppose both the train and test set are i.i.d. samples of $p_\\text{data}$.  \n",
    "A ML algorithm performs well if:\n",
    "1. The training error is small\n",
    "2. The gap between training and test error is small\n",
    "\n",
    "These leads to 2 problems when training a model:\n",
    "- Underfitting: the training eror is too large\n",
    "- Overfitting: the training error is low but the test error is too large.\n",
    "\n",
    "The underfitting / overfitting of a model is controlled by it's capacity or complexity: it's ability to fit a wide variety of functions.  \n",
    "It is controlled by the hyothesis space, the set of functions that a model can learn.   \n",
    "Eg: for lineare regression, it is all linear functions. By adding polynomial terms, we can increase the model capacity.  \n",
    "\n",
    "We need to find the right model capacity depending on the task. Insufficient capacity leads to underfitting, and too much capacity leads to overfitting.  \n",
    "\n",
    "Some models can have a different capacity depending on they parameters, this is the representational capacity of the model.  \n",
    "\n",
    "Occam's razor: Among hypotheses that are equally weel, one should choose the simplest one.  \n",
    "\n",
    "the Vanpik-Chervonenkis dimension (VC dimension) can measure the capacity of a binary classifier.  \n",
    "It is very difficult to measure exactly the capacity of a deep learning models.  \n",
    "\n",
    "Non parametric models have their capacity a function of the dataset.  \n",
    "\n",
    "The Bayes error is a error made by a theorical model that knows the true data distribution. There is still some error becuse of the noise in the distribution.  \n",
    "\n",
    "Mode data usually yields better generalization.  \n",
    "\n",
    "No Free Lunch theorem: averaged over all possible data distributions, every classifier has the same error when classyfying unseen data. A DL model has the same performance that a random classifier.  \n",
    "This assumption doesn't hold when we make assumptions about the data distributions, for example in real-life applications.  \n",
    "\n",
    "The ML goal is not to find an universal model, but to understand the data distributions relevant to the real world applications, and which algorithms perform well on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization \n",
    "\n",
    "Removing or adding functions from the hypothesis space is not the only way to control the model capacity. We can also change the learning algorithm to give him preferences to specific functions in the hypothesis space.  \n",
    "For example, we can add weight-decay to models with a cost function:\n",
    "\n",
    "$$J(w)  = \\text{MSE}_\\text{train} + \\lambda w^Tw$$  \n",
    "\n",
    "The chosen weights are a trade-off between fitting the training data and having small weights. This alternate the model capacity and helps reduce overfitting.\n",
    "\n",
    "They are many other ways to express preferences for different solutions, both implicitly and explicitly.  \n",
    "\n",
    "Regularization is any modification we make to a leanring algorithm that is intended to reduce its generalization error but not its training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Validation Sets\n",
    "\n",
    "Hyperparameters control the behavior of the model, they are fixed before and not learned by te algorithm.  \n",
    "Examples: the degree of a polynomial regressor, or the weight decay coefficient for regularization.  \n",
    "\n",
    "Usually these are parameters that are not appropiate or easy to learn from the training set. This applies to all parameters that controls the model capacity, if learn on the training set they would always choose the one with the biggest capacity and overfit.\n",
    "\n",
    "In order to choose the hyperparameters, we take a small part of the training set, that we call the validation set. The validation is used to estimate the generalization error during training and update the hyperparameters accordingly. Usually we split 80% training 20% validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "When the dataset is small, dividing it into three parts create a really small test part, making it a poor approximation of the generalization error.  \n",
    "\n",
    "We can split the dataset into $k$ subsets, and train $k$ different models.  For each model, one different subset is used as a test set, and the others as the training set. The generalization error is estimated by taking the average test error over the $k$ models.  \n",
    "Cross-Validation should not be used if the dataset is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators, Bias and Variance\n",
    "\n",
    "## Point Estimation\n",
    "\n",
    "Point estimation try to provide the single best prediction of some quantity of interest. It's an general a point estimate $\\hat{\\theta}$ of a vector of parameters $\\theta$.  \n",
    "A point estimator is any function of the data. A good estimators returns an estimate $\\hat{\\theta}$ as close as possible to the true $\\theta$.  \n",
    "\n",
    "\n",
    "In function estimation, we assume there is a function $f(x)$ that describes the relation between $x$ and $y$, and we are trying to find an estimate $\\hat{f}$, this is a point estimator in a function space.\n",
    "\n",
    "\n",
    "## Bias\n",
    "\n",
    "The bias of an estimaor is defined as:\n",
    "\n",
    "$$\\text{bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$$\n",
    "\n",
    "where the expectation is over the training data.\n",
    "An estimator is unbiased if $\\text{bias}(\\hat{\\theta}) = 0$.\n",
    "\n",
    "We prefer an estimate with low bias.\n",
    "\n",
    "## Variance and Standard Error\n",
    "\n",
    "The variance of an estimator is simply:\n",
    "$$\\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] = \\mathbb{E}[\\hat{\\theta}^2] - \\mathbb{E}^2[\\hat{\\theta}] $$\n",
    "\n",
    "The square root of the variance is the standard error, denoted $\\text{SE}(\\hat{\\theta})$.\n",
    "\n",
    "The variance tells us how much the estimate we compute vary as we resample the dataset.  \n",
    "We also prefer an estimate with low variance.\n",
    "\n",
    "The standard eror of the mean is:\n",
    "$$\\text{SE}(\\hat{\\mu}) = \\sqrt{\\text{Var} \\left[ \\frac{1}{m} \\sum_{i=1}^m x^{(i)} \\right]} = \\frac{\\sigma}{\\sqrt{m}}$$\n",
    "\n",
    "$\\sigma$ is often remplaced by the sample variance or the unbasied sample variance, even though they tend to underestimate the true standard deviation. For large $m$, the approximation is quite reasonable.\n",
    "\n",
    "Sample variance:\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\hat{\\mu})^2$$\n",
    "\n",
    "Unbiased sample variance:\n",
    "$$\\tilde{\\sigma}^2 = \\frac{1}{m-1} \\sum_{i=1}^m (x^{(i)} - \\hat{\\mu})^2$$\n",
    "\n",
    "with $\\mu$ the sample mean:\n",
    "$$\\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}$$\n",
    "\n",
    "We can estimate the generalization error with the sample mean of the error on the test set.  \n",
    "The central limit theorem tells us that the mean will be approximatley distritbuted with a normal distribution. We can build a 95% confidence interval on the generalization error:\n",
    "$$(\\hat{\\mu} - 1.96 \\text{SE}(\\hat{\\mu}), \\hat{\\mu} + 1.96 \\text{SE}(\\hat{\\mu}))$$\n",
    "\n",
    "We say that algrithm A is better than algorithm B if the upper bound of the 95% interval for the error of algorithm A is less than the lower bound of the 95% interval for the error algorithm B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading off Bias and Variance to minimize MSE\n",
    "\n",
    "144"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
