{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a neural network trained to learn the identity function. It has a hidden layer $h$ that describes a code representing the input.  \n",
    "The networkk is divided into an encoder $h=f(x)$ and a decoder $r=g(h)$.  \n",
    "\n",
    "Autoencoders are restricted in a way that allow them to do an approximative reconstruction, and only for inputs ressembling the training  data, they often learn useful properties of the data.  \n",
    "\n",
    "They can be generalized to stochastic mappings $p_\\text{encoder}(h|x)$ and $p_\\text{decoder}(x|h)$.  \n",
    "\n",
    "They are used for dimensionality reduction, feature extraction, and generative modeling.  \n",
    "They can be trained like any other neural network, or using a different algorithm called recirculation, it compares the activations of the input to the activations of the reconstructed input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undercomplete Autoencoders\n",
    "\n",
    "We hope for $h$ to have usefull properties. One way is to constrain $h$ to have smaller dimmension than $x$, this is called un undercomplete autoencoder.  \n",
    "\n",
    "We minimize the loss function:\n",
    "$$L(x, g(f(x))$$\n",
    "\n",
    "When $L$ is the mean squared error and the decoder is learn, $h$ span the same subspace as PCA.  \n",
    "With non-linear functions, we can learn a more powerful representation than PCA.  \n",
    "If the network has too much capacity, it could learn to reconstruct the data without extracting useful information in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Autoencoders\n",
    "\n",
    "Rather than limiting the capacity by keeping the model shallow, or the code size small, we can use specific loss functions to regularize and control the capacity of the model. It forces the model to have other properties, such as:\n",
    "- sparsiy of $h$\n",
    "- smallness of the derivative of $h$\n",
    "- robustness to noise / missing inputs\n",
    "\n",
    "Nearly an generative model with latents variable and an inference procedure can be considered as an autoencoder. For example the variational autoencoder and the generative stochastic network.  \n",
    "They learn a useful overcomplete code without regularization, because they were trained to maximize the probability of the training data instead of copying the input to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoders\n",
    "\n",
    "A sparse autoencoder is trained as a normal autoencoder, with the addition of a sparsity penalty $\\Omega(h)$ to the reconstruction error.  \n",
    "They are typically used to learn features for a classifier.  \n",
    "\n",
    "We can think of a sparse autoencoder as approximating maximum likelihood training of a generative model with latent variables.\n",
    "\n",
    "Suppose our model has visible variables $x$ and latent variables $h$, with explicit joint distribution:\n",
    "$$p_\\text{model}(x,h) = p_\\text{model}(h) p_\\text{model}(x\\|h)$$\n",
    "with $ p_\\text{model}(h)$ the model prior distribution over $h$, before seeing $x$.  \n",
    "The log-likelihood is:\n",
    "$$\\log  p_\\text{model}(x) = \\log \\sum_h  p_\\text{model}(h,x)$$\n",
    "\n",
    "We can think of the autoencoder as maximizing this sum with a point estimate $h$. With this chosen $h$, we are maximizing:\n",
    "$$\\log p_\\text{model}(h,x) = \\log p_\\text{model}(h) + \\log p_\\text{model}(x|h)$$\n",
    "\n",
    "If the priop is sparcity-inducing, with a Laplace, we obtain:\n",
    "$$-\\log p_\\text{model}(h) = \\Omega(h) + \\text{const}$$\n",
    "$$\\Omega(h) = \\lambda \\sum_{i} |h_i|$$\n",
    "\n",
    "We can see the sparsity penalty not as a regularization term, but as a consequence of the model distribution over its latent variables.  \n",
    "Training an autoencoder can be seen as training a generative model, and the learned code is usefull because it describe the latent variables that explain the input.  \n",
    "\n",
    "Other work proposed a connection between the sparsity penalty and the $\\log Z$ (normalizer) when using maximum likelihood to an undirected probabilistic model. Minimizing $\\log Z$ prevents the undirected model from having high probability everywhere, and imposing sparsty on $h$ prevents the autoencoder from having low reconstruction error everywhere.\n",
    "\n",
    "We can make $h$ parse with ReLU activation for the code layer, and the parse penalty, that can control the sparsity of $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders\n",
    "\n",
    "A denoising autoencoder (DAE) minimizes:\n",
    "$$L(x, g(f(\\tilde{x}))$$\n",
    "with $\\tilde{x}$ is a copy of $x$ corrupted by some noise. DAE must undo this corruption.  \n",
    "Denoising forces implicitely the model to learn the structure of $p_\\text{data}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing by Penalizing Derivatives\n",
    "\n",
    "The contractive autoencoder (CAE) use another form of regularization by penalty:\n",
    "$$L(x, g(f(x)) + \\Omega(h, x)$$\n",
    "$$\\Omega(h, x) = \\lambda \\sum_i ||\\nabla_x h_i||^2$$\n",
    "\n",
    "This forces the model to learn a code that doesn't change much when $x$ changes slightly.\n",
    "523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Power, Layer Size and Depth\n",
    "\n",
    "Like any feedforward networks, they are many advantages to a deep autoencoder. It can more easily enforce arbitrary constraints. Depth exponentially reduce the computational cost of representing some functions, and the amount of data needed.  \n",
    "\n",
    "Deep autoencoders can be pretrained by trainining a stack of shallow autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Encoders and Decoders\n",
    "\n",
    "Given code $h$, We may think of the decoder as a conditional distribution $p_\\text{decoder}(x|h)$, and the decoder is trained by minimizing the negative log-likelihood $-\\log p_\\text{decoder}(x|h)$.  \n",
    "The output are treated as being contidionally independent given $h$ so the probability distribution is not computationally expensive.  \n",
    "\n",
    "We can go further and make an encoding distribution $p_\\text{encoder}(h|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders\n",
    "\n",
    "The DAE autoencoder receives a corrupted input and tries to reconstruct the original input. The corruption process is a condition distribution $C(\\tilde{x}|x)$.\n",
    "\n",
    "The autoencoder learns a reconstruction distribution $p_\\text{reconstruct}(x|\\tilde{x}) = p_\\text{decoder}(x|h)$ with $h$ output of the encoder: $f(\\tilde{x})$ and $p_\\text{decoder}$ defined by a decoder $g(h)$.  \n",
    "\n",
    "The autoencoder is trained to minimize the negative log-likelihhod using stochastic gradient descent:\n",
    "$$-\\mathbb{E}_{x \\sim p_\\text{data}} \\mathbb{E}_{\\tilde{x} \\sim C(\\tilde{x}|x)} \\log p_\\text{decoder}(x|h=f(\\tilde{x}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Score\n",
    "\n",
    "The training criterion of DAE (with conditionally Gaussian $p(x|h)$) is $||g(f(\\tilde{x})) - x||^2$. The vector $g(f(\\tilde{x})) - \\tilde{x}$ points toward the nearest point on the data manifold. The autoencoder thus lears a vector field $g(f(x)) - x$.  \n",
    "This vector field in an estimate of what is called a score gradient field:\n",
    "$$\\nabla_x \\log p(x)$$\n",
    "\n",
    "\n",
    "Score matching provides a consistent estimator of a probability distribution. It encourages the model to have the same score as the data distribution at every training point.\n",
    "\n",
    "Training a DAE with sigmoidal hidde units, linear reconstruction units, Gaussian noise and MSE is equivalent to training an RBM with denoising score matching.  \n",
    "An RMB is a model that tries to learn an explicit $p_\\text{model}(x,\\theta)$.  \n",
    "Training an RBM with contrastive divergence yields a cost function identical to the cost function of the CAE.  \n",
    "\n",
    "A gneric autoencoder architecture may estimate the score using the MSE criterion:\n",
    "$$||g(f(\\tilde{x})) - x||^2$$\n",
    "and a gaussian corruption:\n",
    "$$C(\\tilde{x}|x) = \\mathcal{N}(x,\\sigma^2I)$$  \n",
    "\n",
    "\n",
    "DAE are just MLP trained to denoise, but they refer to models that learn an internal representation of the data as a side effect. The learned representation can be used to pretrain a deeper network or a supervised network. Denoising is just a technique to train very high-capacity models witout learning a simple identify function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Manifolds with Autoencoders\n",
    "\n",
    "Many ML algorithms epxloits the idea that data concentrates around a low-dimensional manifold or a set of manifolds. They learn a function that behaves correctly only with data on the manifold.  \n",
    "Autoencoders go further and aim to learn the structure of the manifold.  \n",
    "\n",
    "We can characterize a manifold by the set of its tangent plane. At every point $x$ on the $d$-dimensional manifold, the tangent plane is given by $d$ basic spanning the local directions of variational allowed in the manifold. One can change $x$ infinitesimaly while staying in the manifold.  \n",
    "\n",
    "An autoencoder involves the compromise between:\n",
    "1. Learning a code $h$ that can approximately allow to recover $x$, only for points draw from the data distribution.\n",
    "2. Satisfy the penalty, their prefer solution less sentitive to the input.\n",
    "\n",
    "Because of this, the autoencoder can afford to only represent the variations that are needed to reconstruct the training examples. If the data concentrates near a low-dimensional manifold, this yields a representation that implicitely capture a local coordinate system for this manifold.  \n",
    "It learns a mapping only sensible to changes along the manifold directions, but insensitive to changes orthogonal to it.  \n",
    "\n",
    "We call embedding the representation of a datpoint on (or near) the manifold. It is given by a vector with lower dimension than the original data space.  \n",
    "Some algorithms learn directly an embedding for each training example, while others learn a mapping from data space to its embedding.  \n",
    "\n",
    "Most methods to learn a nonlinear manifolds are unsupervised, based o the rearest-neigbor graph: one node per training examples, connect near neighbors to each other.  \n",
    "We associate ech node with a tangent plane that spans the direction towards its neighbors, and be can obtain a ew coordinate system through optimization.  \n",
    "One drawback is if the manifold is not very smooth, we need a very large number of training examples, and it may not generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractive Autoencoders\n",
    "\n",
    "CAE introduce a penalty on the code $h = f(x)$:\n",
    "$$\\Omega(h) = \\lambda ||\\frac{\\partial f(x)}{\\partial x}||^2_F$$\n",
    "\n",
    "\n",
    "CAE reconstruction error is equivalent DAE in the limit of small Gaussian noise. DAE make the autoencoder resist small but finit-sized perturbations of $X$, and the CAE make the autoencoder resist infinitesimal perturbations of $x$.  \n",
    "\n",
    "The name contractive because, locally, $f(x)$ maps a neighborhood of inputs points to a smaller neighborhood of outputs points.  \n",
    "\n",
    "A linear operator $J$ is said to be contractive if $|Jx|| < 1$ for all unit-norm $x$.  \n",
    "We can think of CAE as penalizing the norm of the local linear approximation of $f(x)$ in order to encourage the local linear operators at every training point to be a contraction.\n",
    "\n",
    "Directions of $x$ with large $Jx$ rapidly change $h$, so these are likely to be directions which approximate the tangent planes of the manifold.  \n",
    "The directions of the largest singular values of $J$ are interpreted as the learned tangent directions.  \n",
    "\n",
    "One issue is that computing the penalty is really expensive for deep autoencoders. One strategy is to separately train a serie of single-layer CAE, each trained to reconstruct the code of the previous CAE. The combination of those form a deep CAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Sparse Decomposition (PSD)\n",
    "\n",
    "PSD combines a parametric autoencoder and sparse coding. It is used as unsupervised feature learning for object recognition.  \n",
    "Let $f(x)$ the encoder, $g(h)$ the decoder, and $h$ the code controlled by optimization. The criterion is:\n",
    "$$||x - g(h)||^2 + \\lambda |h|_1 + \\gamma ||h - f(x)||^2.  \n",
    "\n",
    "Training alternates between minimization with respect to $h$ and with respect to the model parameters, with gradient descent.  \n",
    "\n",
    "PSD is an example of learned approximate inference.  \n",
    "Computing $h$ by iterative optimization is only during training, for evaluation we compute $h$ via $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Autoencoders\n",
    "\n",
    "Autoencoders has been used for dimensionality reduction. Training a stack of RBM to initialize the weights of a deep autoencoder gives a reconstruction error smaller than PCA.  \n",
    "Dimensionality reduction can improve performance on many tasks such as classficiation. It consumes less memory and runtime, and the learning code may help for generalization.  \n",
    "\n",
    "Autoencoders also help for information retrivial: Finding entries in a database that resemble a query entry.  \n",
    "Semantic hashing learn a binary low-dimensional code, and store all the entries in a hash table. We can very efficiently return all database entries that have the same binary code as the query, or flip individual bits to return similar entries.  \n",
    "\n",
    "The encoding final layer uses sigmoid activation that are trained to saturate to nearly $0$ or $1$ for all inputs. We inject noise just before the sigmoid, with increasing magnitude over time. To fight that noise and preverse information, the network increase the magnitude of the inputs until satturation occurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
