{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a model of the input:\n",
    "$$p_\\text{model}(x)$$\n",
    "\n",
    "The model may also have a latent variable $h$:\n",
    "$$p_\\text{model}(x) = \\mathbb{E}_h p_\\text{model}(x|h)$$\n",
    "$h$ is another way to represent the data.\n",
    "\n",
    "A linear factor model use a stochastic, linear decoder that generatex $x$ by adding noise to a linear transformation of $h$.  \n",
    "We sample the explanatory factos $h$ from a distribution:\n",
    "$$h \\sim p(h)$$\n",
    "$$p(h) = \\prod_i p(h_i)$$\n",
    "Next we use the decoder to generate $x$:\n",
    "$$x = WH + b + \\text{noise}$$\n",
    "Usually the noise is Gaussian and diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic PCA and Factor analysis\n",
    "\n",
    "In factor analysis, the latent variable prior is a unit Gaussian:\n",
    "$$h \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "The noise is draw from a diogonal distribution, with covariance matrix:\n",
    "$$\\psi = \\text{diag}(\\sigma^2_1,\\text{...},\\sigma^2_n)$$\n",
    "\n",
    "The role of the latent variables is to capture the depencies between the $x_i$. $x$ is a multivariate normal random variable:\n",
    "$$x \\sim \\mathcal{N}(b, WW^T + \\psi)$$\n",
    "\n",
    "For probabilistic PCA, we use the same variance $\\sigma^2$ for every variable:\n",
    "$$X = Wh + b + \\sigma z$$\n",
    "$$z \\sim \\mathcal{N}(0,I)$$\n",
    "$$x \\sim \\mathcal{N}(b, WW^T + \\sigma^2 I)$$\n",
    "\n",
    "An iterative EM algorithm can estimates the parameters $W$ and $\\sigma^2$.\n",
    "\n",
    "Probabilistic PCA supposes that most variation in the data can be captured  by the latent variables $h$, up to the reconstruction error $\\sigma^2$. As $\\sigma \\to 0$, Probabilistic PBA becames PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independant Component Analysis (ICA)\n",
    "\n",
    "\n",
    "ICA is an approach to model linear factors that separates an observed signal into many underlying signals that are intended to be independant (not just decorrelated).  \n",
    "\n",
    "We train a fully parametric generative model.  \n",
    "We fix the prior $p(h)$, and define the generating process:\n",
    "$$x = Wh$$\n",
    "\n",
    "Maximum likelihood is used to determine $p(x)$ and learn the model.  \n",
    "By choosing an independant prior, we recover underlying factors that are as close to independant as possible.  \n",
    "\n",
    "This is used to recover signals that have been mixed together. For example, if $n$ people speaks simultaneuously, recorded with $n$ microphones, ICA can separate them and. Each example is a moment in time, with each featre $x_i$ corresponding to one microphone. Each $h_i$ will contain only one person speaking clearly.  \n",
    "It's also used in neuroscience, to separate brain signals and eletric signals measured with electrodes.  \n",
    "\n",
    "They are many approaches to the problem, most aim to make the elements of $h = W^{-1}x$ independants from each other.  \n",
    "All requires $p(h)$ to be non-Gaussian, otherwhise $W$ is not identifiable.  \n",
    "Typical choices of these distributions have larger peaks near $0$, so most implementations of ICA are learning sparse features.\n",
    "\n",
    "Many variants of ICA are not generative models, they do not represent $p(h)$ or $p(x)$, they only find a way to transform between $x$ and $h$.  \n",
    "For example, they aim to increase the sample kurtosis of $h=W^{-1}x$, leading to $p(h)$ non-Gaussan.  \n",
    "\n",
    "ICA can be genelizariated to a nonlinear model, where we use a nonlinear function $f$ to generate the data.  \n",
    "Another approach, Nonlinear Inpedendant Components Estimation (NICE) stacks a series of invertible encoders. They transforn the data into a space where it has a factorized marginal distribution.  \n",
    "It is more likely to suceed because the encoders are nonlinear.  \n",
    "\n",
    "Another generalization of ICA learn groups of features with dependences within a group, but discouraged between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Feature Analysis\n",
    "\n",
    "SFA is a regularization technique that can be applied to any differentiable model to learn features that changes slowly over time.  \n",
    "We add a regularization term to the cost function:\n",
    "$$\\lambda \\sum_t L(f(x^{(t+1)}, f(x^{(t)}))$$\n",
    "\n",
    "with $f$ the feature extractor, and $L$ a loss functon measuring the distance between the features, usually MSE.  \n",
    "\n",
    "With a linear feature extractor $f(x; \\theta)$, we can get a closed form solution.  \n",
    "The problem is defined as:\n",
    "$$\\min_\\theta \\mathbb{E}_t[(f(x^{(t+1)})_i - f(x^{(t)})_i)^2]$$\n",
    "$$\\text{s.t. } \\mathbb{E}_t[f(x^{(t)})_i] = 0$$\n",
    "$$\\text{s.t. } \\mathbb{E}_t[f(x^{(t)})_i^2] = 0$$\n",
    "\n",
    "Constraining the features to have $0$ mean ensure me have a unique solution, and constraining them to have unit variance prevent features from all collapsing to $0$.  \n",
    "\n",
    "To learn multiples features, we must had the constraints that the features are decorelatted from each other's, otherwhise all features would be the same lowest signal:\n",
    "$$\\forall i < j, \\space \\mathbb{E}_t[f(x^{(t)})_i f(x^{(t)})_j] = 0$$\n",
    "\n",
    "SFA can learn nonlinear features by applying nonlinear basis expansion first. We can campose several SFA modules, applying non-linear basis expansion to the outputs of the previous one, then leanrning SFA on top on it, recursively. It can learn deep nonlinear slow features.\n",
    "\n",
    "Deep SFA has been used to learn features for object recognition and pose estimation, but it did not made any state of the start applications.  \n",
    "Maybe the slowness prior is too strong, it encourages the model to ignore position of objects with high velocity. It would be better to impose a prior that features should be easy to predict from one time step to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Coding\n",
    "\n",
    "Sparse coding is a linear factor model that performs unsipervised features learning.\n",
    "\n",
    "$$p(x|h) = \\mathcal{N}(Wh + b, \\frac{1}{\\beta}I)$$\n",
    "\n",
    "The prior $p(h)$ is chosen to have sharp peaks near $0$.  \n",
    "It can be a Laplace:\n",
    "\n",
    "$$p(h_i) = \\text{Laplace}(0,\\frac{2}{\\lambda}) = \\frac{\\lambda}{4} \\exp (-\\frac{1}{2} \\lambda |h_i|)$$\n",
    "\n",
    "Or a Student-t:\n",
    "\n",
    "$$p(h_i) \\propto \\frac{1}{(1 + \\frac{h_i^2}{\\nu})^{\\frac{\\nu + 1}{2}}}$$\n",
    "\n",
    "Maximum likelihood is intractable. Instead, we use an iterative 2-steps process.  \n",
    "The first step is an optimization problem that finds the single most likely code value for $h$:\n",
    "$$h^* = \\arg \\max_h p(h|x)$$\n",
    "\n",
    "With a Laplace prior, it simplies to:\n",
    "\n",
    "$$h^* = \\arg \\min_h \\lambda ||h||_1 + \\beta ||x - Wh||_2^2$$\n",
    "\n",
    "with $W$ fixed. \n",
    "\n",
    "The second step update $W$ to minize the reconstruction error.\n",
    "\n",
    "The whole procedure yields a sparse $h^*$.\n",
    "\n",
    "Sparse coding with a non-parametric encoder can minimize the reconstruction error and log-prior better than a parametric model.  \n",
    "Another advantage is there is no generealization error in the encoder, compared to parametrics model. The problem is convex and leads to an optimal code. When used as a feature extractor for a classifier, it may lead to better generalization than a parametric encoder.  \n",
    "\n",
    "The disadvantage is that computing $h$ given $x$ is an iterative algorithm that takes a lot more time. \n",
    "What's more backpropagate through the encoder is not easy, which make it complicate to finetune it with labelled data.  \n",
    "\n",
    "Sparse coding, like other linear factor models, generate poor samples, even when the model reconstruct the data well. It's because the prior of $h$ is factorial, resulting in the model including random subsets of all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Interpretation of PCA\n",
    "\n",
    "PCA and factor analysis can be seen as learning a manifold. It defines a Gaussian distribution that is very narrow along somes axes and elongated along others. PCA aligns it with a linear manifold in a higher dimensional space.  \n",
    "\n",
    "The encoder compute a low-dimensionial representation $h$:\n",
    "$$h = f(x) = W^T(x - \\mu)$$\n",
    "The decoder reconsructs $x$:\n",
    "$$\\hat{x} = g(h) = Vh + b$$\n",
    "\n",
    "The goal is to minimise the reconstruction error:\n",
    "$$\\mathbb{E}[||x - \\hat{x}||^2]$$\n",
    "\n",
    "The optimal encoder and decoder corresponds to $V = W$, $\\mu = b = \\mathbb{E}[x]$, and the columns of $W$ an orthonormal basis which spans the same subspace as the principal eigenvectors of the covariance matrix:\n",
    "$$C = \\mathbb{E}[(x - \\mu)(x - \\mu)^T]$$\n",
    "\n",
    "The eigenvalue $\\lambda_i$ of $C$ corresponding to the variance of $x$ in the direction of $v^{(i)}$.\n",
    "\n",
    "If $x \\in \\mathbb{R}^D$ and $h \\in \\mathbb{R}^d$, the reconstruction error is:\n",
    "$$\\sum_{i=d+1}^D \\lambda_i$$\n",
    "\n",
    "Hence if the covariance has rank $d$, the reconstruction error is $0$.  \n",
    "\n",
    "Maximizing the variance of the elements of $h$, with $W$ constrained to be orthogonal, yields the same solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
