{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNNL Design\n",
    "\n",
    "## Primitives\n",
    "\n",
    "Functor object that encapsulate an operation (eg conv, gemm), or a fused operator (eg: gemm + Relu).  \n",
    "Immutable state: operation params (eg tensor shape), or precomputation.  \n",
    "Mutable state: memory buffer, temporary storage used during execution.\n",
    "\n",
    "Primitive creation is an expensive operation (precomputation), it should be created once and used many times.\n",
    "\n",
    "## Engine\n",
    "\n",
    "Computational device\n",
    "\n",
    "## Streams\n",
    "\n",
    "Encapsulate execution context (eg: OpenCL command queues)\n",
    "\n",
    "## Memory objects\n",
    "\n",
    "Handles to memory allocated on a specific engine, memory format, and tensor indices mapping to offset.\n",
    "\n",
    "## Memory descriptors\n",
    "\n",
    "Tensor dimensions, datatype, and memory format.\n",
    "\n",
    "## Operation descriptors\n",
    "\n",
    "Describe an operation (eg: Convolution) and it's properties (eg: shape of input tensors, forward or backward operation).\n",
    "\n",
    "## Primitive descriptors\n",
    "\n",
    "Abstraction between primitive and Operator descriptors, give details about primitive implementation and memory format.\n",
    "\n",
    "## Basic API Usage\n",
    "\n",
    "1. Init engine and stream\n",
    "2. Prepare data input (eg store to std::vector)\n",
    "3. Wrapping data into memory objects\n",
    "4. Creating the operations primitives\n",
    "5. Executing the primitives\n",
    "6. Load output data from memory objects\n",
    "\n",
    "## Memory format propagation\n",
    "\n",
    "The placeholder memory format let the implementation choose the actual format.  \n",
    "For convolutions, the best choice is based on hardware and ops params (eg filter size), so it's best to keep the placeholder format.  \n",
    "\n",
    "Other primitives such as elemwhise, batchnorm, should use the same format as previous layer, this is called format propagation.\n",
    "\n",
    "## Propagation Kinds\n",
    "\n",
    "- Forward Inference: during inference, input data fed into model, produce result\n",
    "- Forward Traning: during training, some difference with inference for performance (need to keep data for backward phase)\n",
    "- Backward data: propagation error with respect to the input data\n",
    "- Backward weights: propagation error with respect to the model weights\n",
    "\n",
    "For forward inference:\n",
    "- do as many in-place operations as possible\n",
    "- some primitives can be chained / fused together\n",
    "\n",
    "## Workspace\n",
    "\n",
    "A workspace is an additional tensor, required for computations for some operands.  \n",
    "For example, some primitives fills it during forward pass, and use it during backward pass.  \n",
    "Workspace is different from scratchpad (only needed during primitive executing, no need to be preserved during calls).\n",
    "\n",
    "## Quantization\n",
    "\n",
    "We can do Int8 inference using quantization: Convert 32fp to int8, using a scaling factor.  \n",
    "They are some int8 primitives for computation\n",
    "\n",
    "## API\n",
    "\n",
    "The library API is written in C.\n",
    "A header-based wrapper around the C API is available for C++. It makes code much more convenient to use.  \n",
    "Some features aren't available in the C++ API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
