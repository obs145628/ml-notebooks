{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 measure factors:\n",
    "- Throughput\n",
    "- Efficiency\n",
    "- Latency\n",
    "- ACcuracy\n",
    "- Memory Usage\n",
    "\n",
    "TensorRT Developer guide p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Export model to format parsable by TensorRT: ONNX, Caffe, UFF.\n",
    "2. Choose optimization options\n",
    "3. TensorRT create an optimized inference runtime engine.\n",
    "4. The engine can be serialized / loaded again.\n",
    "5. The engine can run inference\n",
    "\n",
    "TensorRT devloper guide p7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT takes network def, perform optis, generate runtime engine.\n",
    "\n",
    "Optis are:\n",
    "- Eliminator of unused layers and no-ops\n",
    "- Layer & tensor fusion\n",
    "- Kernel Auto-tuning\n",
    "- Modify precision of weights\n",
    "\n",
    "TensorRT developer guide p8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT core interface:\n",
    "- Network Definition\n",
    "- Builder\n",
    "- Engine\n",
    "\n",
    "TensorRT developer guide p9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C++ API steps\n",
    "\n",
    "1. Define a network\n",
    "    - Manually with API\n",
    "    - Using a parser of file ONNX, Caffe or UFF\n",
    "\n",
    "2. Build an engine\n",
    "    - From the network def, optimize it\n",
    "    - From a file, desarialize the engine\n",
    "\n",
    "3. (Optional) Serialize the engine\n",
    "\n",
    "4. Perform Inference  \n",
    "    4.1. Create execution context  \n",
    "    4.2. Prepare input and output buffers  \n",
    "    4.3. enquele kernels on a CUDA stream  \n",
    "    4.4. use sync methods such as other CUDA stream to get results\n",
    "    \n",
    "TensorRT developer guide chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom layers:\n",
    "    \n",
    "Extend tensorRT functionality: creating a custom layer with implem.\n",
    "\n",
    "1. Create class that herits from a plugin base class\n",
    "2. Create class that herits from IPluginCreator\n",
    "1. Use the plugin registery to get the plugin creator\n",
    "1. Create plugin using plugin name and metada\n",
    "1. Add the plugin into the current network definition\n",
    "\n",
    "TensorRT developer guide chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision\n",
    "    \n",
    "Types: fp32, fp16, quantized int8.  \n",
    "You can set the precision for any layer. It actually set the prefered types for input and outputs.  \n",
    "It may insert reformat ops if necessary.  \n",
    "These are only preferences, TensorRT chose it only if it leads to higher performance (some layers may be faster in higher precision). It may also not exist such a layer implem for this precision.  \n",
    "You can also set the prefered type for the whole network.  \n",
    "\n",
    "For int8 quanrization, TensorRT need a scale factor.  \n",
    "Only supports Symmetric quantization: scale calculated with absolute maximum dynamic range values.  \n",
    "The dyanmic range can be set manually for each tensor, or it can be computed using calibration dataset.  \n",
    "\n",
    "Explicit precision network: precision of all tensors and layers known, usefull to export pre-quantized models with scaling already established.\n",
    "\n",
    "TensorRT developer guide chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization profile: describe range of dimensions for each network input, and the dimensions the autotuner should use.  \n",
    "Usefull when having network with dynamic shapes (only resolved at run-time).\n",
    "\n",
    "TensorRT developer guide section 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops\n",
    "\n",
    "Loops are defined wih 4 layer kinds:\n",
    "    \n",
    "- ITripLimitLayer: number of iterations\n",
    "- IIteratorLayer: iterate over a tensor\n",
    "- IRecurrenceLayer: recurrent definition\n",
    "- ILoopOutputLayer: Output value of the loop\n",
    "\n",
    "The loop body can only have a restricted kind of layers, and nested loops.\n",
    "\n",
    "TensorRT developer guide chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization\n",
    "\n",
    "TensorRT has quantization and dequantization operations.\n",
    "1. Train a model using QAT (Quantized Aware Training) with TensorFlow\n",
    "2. Export the model to quantized ONNX\n",
    "3. Export the mort to TensorRT using explicit precision mode\n",
    "\n",
    "TensorRT developer guide chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph surgeon API:\n",
    "\n",
    "Manipulate TF grapg. Search, Add, Edit, Remove nodes.  \n",
    "You can mark some nodes as plugins for custom layers implementations.\n",
    "\n",
    "TensorRT developer guide section 12.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
