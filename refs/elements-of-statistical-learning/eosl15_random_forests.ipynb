{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Random Forests\n",
    "\n",
    "Bagging average many noisy but unbiased models to reduce the variance. This is done by training $B$ models, each with a different sample of size $N$ from the original dataset, and average the models predictions.  \n",
    "\n",
    "Random forests improve the variance reduction by reducing the correlation between the trees. This is done by a modification of a tree growing process: To split a nodes, only select $m \\leq p$ on the input variables at random as candidates for splitting.  \n",
    "Reduce $m$ reduces the correlation between the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details of Random Forests\n",
    "\n",
    "Recommandations:\n",
    "- For classification, set $m = \\sqrt{p}$ and minimum node size $1$\n",
    "- For regression, set $m = p/3$ and minimum node size $5$.\n",
    "\n",
    "## Out of Bag Samples\n",
    "\n",
    "The OOB error estimate predicts for each $x_i$  the corresponding $y_i$ by averaging only the predictions of the trees that were not trained with $x_i$.  \n",
    "The error is almost identical to that obtained by $N$-fold cross-validation.\n",
    "\n",
    "## Variable importance\n",
    "\n",
    "As for tree boosting, we can the compute the importance of each predictor.  \n",
    "With random forests, we can compute a different variable-importance measure, using the OOB samples.\n",
    "\n",
    "\n",
    "## Proximity Plots\n",
    "\n",
    "We compute a $N*N$ proximity matrices. For every tree, any pair or OOB observations sharing a terminal node has their proximity increased by one.  \n",
    "We use multidimensional scaling to reduce it to two dimensions and plot it. It gives us an indication of which observations are close together for the random forest.  \n",
    "Proximity plots often look very similar for any dataset, having a star shape, which casts doubt on their utility.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "When $p$ is large, with a lot of noisy variables, a small $m$ results in a poor model, because at each split the chance to get a relevant variable is low.  \n",
    "With a large number of relevant variables, random forest is robust to an increase in the number of noisy variables.  \n",
    "\n",
    "Another claim is that random forest doesn't overfit. Increasing $B$ does not cause overfitting. But an average of fully grown trees can result in a too rich of a model, with unecessary variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
