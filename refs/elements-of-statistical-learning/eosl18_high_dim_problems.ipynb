{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When $p$ is much bigger than $N$\n",
    "\n",
    "High variance and overfitting are a major concern in this setting.  \n",
    "Simple, highly regularized models are often used.  \n",
    "\n",
    "Let's suppose we are trying to predict a linear model.  \n",
    "With $p<<N$, we can identify as many coefficients as we want without shrinkage.  \n",
    "With $p=N$, we can identify some non-zero coefficients with moderate shrinkage.\n",
    "With $p>>N$, even though they are many non-zero coefficients, we don't have a hope to find them, we need to shrink a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal LDA and Nearest Shrunken Centroids\n",
    "\n",
    "The simplest form of regularization assumes that the features are independant within each class (the within class covariance matris is diagonal).  \n",
    "It greatly reduces the number of parameters and often result in an effective and interpretable classifier.  \n",
    "\n",
    "The discriminant score for class $k$ is:\n",
    "$$\\theta_k(x) = - \\sum_{j=1}^p \\frac{(x_j - \\bar{x}_{kj})^2}{s_j^2} + 2 \\log \\pi_k$$\n",
    "with $s_j$ the within-class standard deriavtion for feature $j$, and:\n",
    "$$\\bar{x}_{kj} = \\frac{1}{N_k} \\sum_{i \\in C_k} x_{ij}$$  \n",
    "\n",
    "We call $\\bar{x}_k$ the centroid of class $k$. Diagonal LDA can be seen as a nearest centroid classifier with appropriate standardization.  \n",
    "\n",
    "To regularize in order to drop out features, we can shrink the classwise mean toward the overall mean for each feature separately. This method is called Nearest Shrunken Procedure (NSC).  \n",
    "\n",
    "Let \n",
    "$$d_{jk} = \\frac{\\bar{x}_{kj} - \\bar{x}_j}{m_k(s_j + s_0)}$$\n",
    "\n",
    "with $m_k^2 = 1/N_k - 1/N$ and $s_0$ a small positive constant.  \n",
    "\n",
    "We can shrink $d_{kj}$ toward zero using soft thresholding:\n",
    "$$d'_{kj} = \\text{sign}(d_{kj})(|d_{kj}| - \\Delta)_+$$\n",
    "with $\\Delta$ a parameter to be determined.  \n",
    "\n",
    "The shruken centroids are obtained by:\n",
    "$$\\bar{x}'_{kj} = \\bar{x}_j + m_k(s_j + s_0)d'_{kj}$$\n",
    "We use the shrunken centroids $\\bar{x}'_{kj}$ instead of the original $\\bar{x}_{kj}$ in the discriminant score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers with Quadratic Regularization\n",
    "\n",
    "## Regularized Discriminant Analysis\n",
    "\n",
    "LDA involves the inversion of a $p*p$ within-covariance matrix $\\Sigma$. When $p > n$, the matrix is singular.  \n",
    "RDA solves the issue by shrinking $\\Sigma$ towards its diagonal:\n",
    "$$\\hat{\\Sigma}(\\gamma) = \\gamma \\hat{\\Sigma} + (1 - \\gamma) \\text{diag}(\\hat{\\Sigma})$$\n",
    "\n",
    "## Logistic Regression with Quadratic Regularization\n",
    "\n",
    "The multiclass logistic model is expressed as:\n",
    "$$P(G=k|X=x) \\frac{\\exp (\\beta_{k0} + x^T \\beta_k)}{\\sum_{l=1}^K \\exp (\\beta_{l0} + x^T \\beta_l)}$$\n",
    "\n",
    "This has $K$ coefficients vecors $\\beta_k$. We regalurize the fitting by maximizing the penalized log-likelihhood:\n",
    "$$\\max_{ \\{ \\beta_{0k}, \\beta_k \\}_1^K} \\left[ \\sum_{i=1}^N \\log P(g_i|x_i) - \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\beta_k||_2^2 \\right]$$\n",
    "\n",
    "## The Support Vector Classifier\n",
    "\n",
    "When $p > N$, the classes are perfectly separable, unless there are identical feature vectors in different classes.  \n",
    "Surprisingly, the unregularized SVC often works about as well as the best regularized version.\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "RDA, regularized logistic regression and SVC shrinks weights toward zero, but they keep all features.  \n",
    "Recursive feature elimination remove feature with small weights, and retrain the classifier.  \n",
    "\n",
    "All 3 approches can be modified using kernels, to increase model complixity. With $p > N$ overfitting is always a danger, and yet using kernel may sometimes give better results.\n",
    "\n",
    "## Computational shorcuts when $p \\gg N$\n",
    "\n",
    "Instead of working with $X \\in \\mathbb{R}^{N*p}$ matrix, we can work with a matrix of size $N*N$, using the SVD:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X & = UDV^T \\\\\n",
    "& = RV^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $R \\in \\mathbb{R}^{N*N}$:\n",
    "$$R = UD$$\n",
    "\n",
    "We can usually work with $R$ instead of $X$.  \n",
    "For example, let's consider the estimates from a ridge regression:\n",
    "$$\\hat{\\beta} = (X^TX + \\delta I)^{-1}X^ty$$\n",
    "\n",
    "We can instead get $\\hat{\\theta}$ the ridge regression estimate using $(r_i, y_i)$. And then we get $\\hat{\\beta} = V \\hat{\\theta}$.  \n",
    "This idea can be generalized to any linear models with a quadratic penalty on the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifiers with $L_1$ Regularization\n",
    "\n",
    "The lasso for linear regression is:\n",
    "$$\\min_{\\beta} \\frac{1}{2} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right) ^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "$L_1$ penalty causes a subset of the $\\hat{\\beta}_j$ to be exactly zero for a sufficiently large value of $\\lambda$, and hence performs feature selection.  \n",
    "\n",
    "When $p > N$, as $\\lambda \\to 0$ the model fits perfectly the dataset.  \n",
    "When $p > N$ the number of non-zero coefficients is at most $N$ for any values of $\\lambda$.  \n",
    "\n",
    "Linear regression can be applied for two-class clasification using $\\pm 1$ as labels, and using sign for the predictions.  \n",
    "\n",
    "A more natural approach is to use the lasso penalty on logistic regression. We can use a symmetric multinomial logistric regression model, and maximixe the penalized log-likelihood:\n",
    "$$\\max_{ \\{ \\beta_{0k}, \\beta_k \\}_1^K} \\left[ \\sum_{i=1}^N log P(g_i|x_i) - \\lambda \\sum_{k=1}^K \\sum_{j=1}^p |\\beta_{kj}|  \\right]$$\n",
    "\n",
    "The lasso tends to encourage a sparse solution, and ridge tends to shrink the oefficients of correlated variables toward each other.  \n",
    "The elastic net penalty is a compromise:\n",
    "$$\\sum_{j=1}^p (\\alpha |\\beta|_j + (1 - \\alpha) \\beta_j^2)$$\n",
    "with $\\alpha \\in [0,1]$ parameter that determines the mix of the penalties.\n",
    "\n",
    "The logistic regression problem above with the elastic net penalty becomes:\n",
    "$$\\max_{ \\{ \\beta_{0k}, \\beta_k \\}_1^K} \\left[ \\sum_{i=1}^N log P(g_i|x_i) - \\lambda \\sum_{k=1}^K \\sum_{j=1}^p (\\alpha|\\beta_{kj}| + (1 - \\alpha) \\beta_{kj}^2)  \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fused Lasso\n",
    "\n",
    "The Fused Lasso is a method that tend to smooth the coefficients uniformly. We add a penalty to take into account the ordering of the features:\n",
    "\n",
    "$$\\min_{\\beta} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right) ^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p-1} |\\beta_{j-1} - \\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification When Features are Unavailable\n",
    "\n",
    "Instead of working with features, we can instead work with an $N*N$ proximity matrix, and we can interpret the proximities as inner-products.\n",
    "\n",
    "For example, it can be considerer as the matrix kernel $K$, and can be used with kernel methods, SVM.\n",
    "\n",
    "## Classition and other methods using Kernels\n",
    "\n",
    "They are a number of other classifier, besides SVM, that can be implemented using only inner-product matrices. This also implies they can be kernelized like the SVM.  \n",
    "\n",
    "For nearest-neigbor, we can transform inner-products to distances:\n",
    "$$||x_i - x_{i'}||^2 = \\langle x_i, x_i \\rangle + \\langle x_{i'}, x_{i'} \\rangle - \\langle x_i, x_{i'} \\rangle$$\n",
    "\n",
    "For nearest-centroid classification, with training pairs $(x_i, g_i)$, and class  centroids $\\bar{x}_k$, we can compute the distance of the test point to each centroid:\n",
    "$$||x_0 - \\bar{x}_k||^2 = \\langle x_0, x_0 \\rangle - \\frac{2}{N_k} \\sum_{g_i=k} \\langle x_0, x_i \\rangle + \\frac{1}{N_k^2} \\sum_{g_i=k} \\sum_{g_{i'}=k} \\langle x_i, x_{i'} \\rangle$$\n",
    "\n",
    "We can also perform kernel PCA. Let $X$ centered data matrix, with SVD decomposition:\n",
    "$$X=UDV^T$$\n",
    "We get the matrix of principal components $Z$:\n",
    "$$Z = UD$$\n",
    "When $K=XX^T$, it follow that $K=UD^2U^T$, and hence we can compute $Z$ from the eigeindecomposition of $K$.  \n",
    "If $X$ is not centered, we need to use the double-centered kernel instead:\n",
    "$$\\tilde{K} = (I-M)K(I-M)$$\n",
    "with $M = \\frac{1}{N} 1 1^T$.\n",
    "\n",
    "But they are some things that we cannot do with kernels:\n",
    "- We cannot standardize the variables.\n",
    "- We cannot assess directly the contribution of individual variables (i.e. we cannot use the lasso penalty)\n",
    "- We cannot separate the good variables from the noise: they all get an equal say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Regression: Supervised Principal Components\n",
    "\n",
    "PCA is an effective method to find linear combinations of features that exhibit large variation in the data.  \n",
    "Supervised PCA find linear linear combination with both high variance and significant correlation with the outcome.\n",
    "\n",
    "Supervised PCA can be related Latent-Variable modeling.  \n",
    "Suppose we hase a response variable $Y$ related to an underlying latent variable U by a linear model:\n",
    "$$Y = \\beta_0 + \\beta_1 U + \\sigma$$\n",
    "\n",
    "We have measurements on a set of features $X_j$, $j \\in \\mathcal{P}$:\n",
    "$$X_j = \\alpha_{0j} + \\alpha_{1j}U + \\sigma_j, \\space j \\in \\mathcal{P}$$\n",
    "We also have many additional features $X_k$, $k \\notin \\mathcal{P}$, which are independent of $U$.\n",
    "\n",
    "This is a 3 steps, proccess, similar to supervised PCA:\n",
    "- Estimates the set $\\mathcal{P}$\n",
    "- Given $\\hat{\\mathcal{P}}$, estimates $U$\n",
    "- Perform a regression fit to estimate $\\beta$, $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Assessment and the Multiple-Testing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Assessment asses the significance of each features, it's related to multiple hypothesis testing.  \n",
    "Let's suppose we have a dataset ok $N$ observations, each with $M$ features, separated into $K=2$ groups.  \n",
    "\n",
    "To identify which features are informative to guess the group, we construct a two-sample t-statistic for each feature:\n",
    "$$t_j = \\frac{\\bar{x}_{2j} - \\bar{x}_{1j}}{\\text{se}_j}$$\n",
    "where:\n",
    "$$\\bar{x}_{kj} = \\frac{1}{N_k} \\sum_{i \\in C_k} x_{ij}$$\n",
    "\n",
    "$\\text{se}_j$ is the pooled within-group standard error for feature $j$:\n",
    "$$\\text{se}_j = \\hat{\\sigma}_j \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}$$\n",
    "$$\\hat{\\sigma}_j^2 = \\frac{1}{N_1 + N_2 - 2} \\left( \\sum_{i \\in C_1} (x_{ij} - \\bar{x}_{1j})^2 + \\sum_{i \\in C_2} (x_{ij} - \\bar{x}_{2j})^2 \\right)$$\n",
    "\n",
    "We could consider any value $> 2$ in absoluve value to be significantly large. However, with $M$ large, we would expect many largve values to occur by chance.  \n",
    "We can assess the result for all $M$ using the multiple testing problem.   \n",
    "We can compute the p-value for each feature $p_j$:\n",
    "$$p_j = \\frac{1}{K} \\sum_{k=1}^K I(|t_j^k| > |t_j|)$$\n",
    "\n",
    "where whe take $K$ random sample labels permutations $t_j^k$.\n",
    "\n",
    "Using p-values, we can test the hypotheses:\n",
    "$H_{0j} = $ label has no effect on feature $j$.  \n",
    "$H_{1j} = $ label has an effect on feature $j$.  \n",
    "We reject $H_{0j}$ at level $\\alpha$ if $p_j < \\alpha$.  \n",
    "\n",
    "Let $A_j$ the event that $H_{0j}$ if falsely rejected: $P(A_j) = \\alpha$  \n",
    "The familiy-wise error rate (FWER) is the probability of at least one false rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The False Discovery Rate\n",
    "\n",
    "Possible outcomes from $M$ hypotesis tests:\n",
    "\n",
    "|&nbsp;|Called not significant|Called signification|Total|\n",
    "|---|---|---|---|\n",
    "|$H_0$ True| $U$ | $V$ | $M_0$|\n",
    "|$H_0$ False|$T$|$S$|$M_1$|\n",
    "|Total|$M-R$|$R$|$M$|\n",
    "\n",
    "The false discovery rate is:\n",
    "$$\\text{FDR} = E(V/R)$$\n",
    "\n",
    "The expectation is taken over the sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetric Cutpoints and the SAM Procedure\n",
    "\n",
    "In previous approaches, we used the absolute value of $t_j$, hence applying the same cutpoint to both positive and negative values.  \n",
    "Significance analysis of microrrays (SAM) derive separate cut-point for the two classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
