{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Classic ensemble methods:\n",
    "- Bagging / Random Forests\n",
    "- Boosting\n",
    "- Stacking\n",
    "\n",
    "Ensemble learning divided in 2 task:\n",
    "- Build multiple base learners\n",
    "- Combine them into one predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Regularization Paths\n",
    "\n",
    "## Penalized Regression\n",
    "\n",
    "Let $\\mathcal{T} = \\{ T_k \\}$ the set of all possible regresion trees on the training data. We can construct the following predictor:\n",
    "$$f(x) = \\sum_k \\alpha_k T_k(x)$$\n",
    "\n",
    "The $\\alpha$ can be estimated by least-squares using a form of regularization:\n",
    "\n",
    "$$\\min_\\alpha \\sum_{i=1}^N \\left( y_i - \\sum_k \\alpha_k T_k(x_i) \\right) ^2 + \\lambda J(\\alpha)$$\n",
    "with $J(\\alpha)$ a penalty function (e.g. ridge or lasso).  \n",
    "\n",
    "Using lasso, we get a sparse solution, with only a small fraction of all possible trees in the model.  \n",
    "Given the huge number of $T_k$, solving directly is intractable. A feasible forward startegy exists that approximates lasso:\n",
    "\n",
    "1. Initialize $\\alpha_k = 0$\n",
    "2. For $m=1 \\to M$:\n",
    "    $$(\\beta^*, k^*) \\leftarrow \\arg \\min_{\\beta, k} \\sum_{i=1}^N (y_i - \\sum_l \\alpha_lT_l(x_i) - \\beta T_k(x_i))$$\n",
    "    $$\\alpha_k \\leftarrow \\alpha_k + \\epsilon *\\text{sign}(\\beta^*)$$\n",
    "3. Output $f_M(x) = \\sum_k \\alpha_k T_k(x)$\n",
    "\n",
    "$M$ is inversely related to $\\lambda$\n",
    "\n",
    "## The Bet on Sparsity Principle\n",
    "\n",
    "Boosting with shrinkage approximate a $L_1$ model. The $L_2$ penalty is much faster to compute.  \n",
    "The superior performance of boosting over models such as SVM may be due to the implicit use of $L_1$ versus $L_2$. this may be due because $L_1$ is better suited to parse situations.  \n",
    "\n",
    "Take for example a dataset of $10.000$ data points, with millions of trees.  \n",
    "If the true population coefficients of the trees arose from a Gaussian, ridge regression is the better predictor, this a dense scenario.  \n",
    "If only a few coefficients are nonzero, lasso is better, this a sparse scenario.  \n",
    "With $L_2$, it performs poorly on both scenarios, because there is too litle data to estimate correctly all coefficients in the dense scenario.  \n",
    "With $L_1$, it might performs well on the sparse scenario.  \n",
    "This leads to the bet on sparsity principle: Use a procedure that does well in sparse problems, since no procedure dowes well in dense problems.  \n",
    "\n",
    "\n",
    "Larger training sets allow to estimate coefficients with smaller standard error. In situations with small noise-to-signal ration (NSR), we can identify more non-zero coefficients than in situations with larger NSR.  \n",
    "Increasing the size of the dictionary $\\mathcal{T}$ may lead to a sparser representation, but the search problem becomes more difficult, and may lead to higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Ensembles\n",
    "\n",
    "Importance sampled learning ensembles - Friedman, J. and Popescu, B. (2003) - [PDF](https://pdfs.semanticscholar.org/966f/fe536f84efd15c1379dad9adffe90b20676f.pdf)\n",
    "\n",
    "Let's consider bulding models of the form:\n",
    "$$f(x) = \\alpha_0 + \\sum_{k} \\alpha_k T_k(x)$$\n",
    "with $\\mathcal{T}$ a dictionary of basis functions, typically trees.\n",
    "\n",
    "A specific approach break the process into 2 steps:\n",
    "- Find a finite dictionay $\\mathcal{T}_L = \\{ T_1(x), \\text{...}, T_M(x) \\}$ from the training data\n",
    "- A lasso path is fit to the model with $\\mathcal{T}_L$:\n",
    "    $$\\alpha(\\lambda) = \\arg \\min_{\\alpha} \\sum_{i=1}^N L(y_i, \\alpha_0 + \\sum_{m=1}^M \\alpha_m T_m(x_i)) + \\lambda \\sum_{m=1}^M |\\alpha_m|$$\n",
    "    \n",
    "This approach saves a lot of computation times, both at training and at computation if the number of trees is small\n",
    "\n",
    "## Learning a Good Ensemble\n",
    "\n",
    "\n",
    "In order to select a good $\\mathcal{T}_L$, we can use a measure of lack of relevance:\n",
    "$$Q(\\gamma) = \\min_{c_0, c_1} \\sum_{i=1}^N L(y_i, c_0 + c_1 b(x_i;\\gamma))$$\n",
    "with $\\gamma$ the whose set of parameters of the basis function $b(x;\\gamma)$. For a tree, it would be every splitting variables, split-points, and values in terminal nodes.  \n",
    "If only one basis function where to be selected, it would be $\\gamma^* = \\arg \\min_{\\gamma} Q(\\gamma)$.\n",
    "\n",
    "Introducing randomness in the selection of $\\gamma$ would produce less optimal values $Q(\\gamma) \\geq Q(\\gamma^*)$.\n",
    "$$\\sigma = E[Q(\\gamma) - Q(\\gamma^*)$$\n",
    "- $\\sigma$ too narow suggest $b(x;\\gamma_m)$ look alike and similar to $b(x;\\gamma^*)$.\n",
    "- $\\sigma$ too wide implies poor $b(x;\\gamma_m)$.\n",
    "\n",
    "ISLE Ensenble generation use sub-sampling for introducing randomness:\n",
    "1. Initialize $f_0(x)$:\n",
    "    $$f_0(x) = \\arg \\min_c \\sum_{i=1}^N L(y_i, c)$$\n",
    "2. For $m=1 \\to M$:\n",
    "    $$\\gamma_m \\leftarrow \\arg \\min_\\gamma \\sum_{i \\in S_m(\\mu)} L(y_i, f_{m-1}(x_i) + b(x_i;\\gamma))$$\n",
    "    $$f_m(x) = f_{m-1}(x) + v b(x;\\gamma_m)$$  \n",
    "    \n",
    "with $S_m(\\nu)$ refers to a subsample of size $N * \\mu$ ($\\mu \\in (0,1)$)\n",
    "\n",
    "## Rule Ensembles\n",
    "\n",
    "Predictive learning via rule ensembles - Friedman, J. and Popescu, B. (2008) - [PDF](https://arxiv.org/pdf/0811.1679.pdf)\n",
    "\n",
    "We can convert a tree into a set of rules. Usually somes rules can be removed while still spamming the same tree.  \n",
    "For each tree $T_m$, we construct its mini-ensemble of rules $\\mathcal{T}^M_\\text{RULE}$, and combine them all into a larger ensemble:\n",
    "$$\\mathcal{T}_\\text{RULE} = \\bigcup_{m=1}^M \\mathcal{T}^m_\\text{RULE}$$\n",
    "\n",
    "This is treated as any other ensemble and post-processed via lasso or other procdedures.\n",
    "Advantages:\n",
    "- The space of models is enlarged\n",
    "- Rules are easier to interpret than trees.\n",
    "- $\\mathcal{T}_\\text{RULE}$ can by extended with each $X_j$ for example, allowing to also model linear functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
