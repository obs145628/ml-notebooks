{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bootstrap and Maximum Likelihood Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spline(X, inters, M):\n",
    "\n",
    "    Xe = np.empty((len(X), M + len(inters)))\n",
    "    \n",
    "    for i in range(M):\n",
    "        Xe[:,i] = X[:,0]**i\n",
    "    \n",
    "    for i in range(len(inters)):\n",
    "        Xe[:,M+i] = np.maximum(X[:, 0] - inters[i], 0)**(M-1)\n",
    "    return Xe\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N,1) * 4\n",
    "y = 8.4*X[:,0] + 0.8*X[:,0]**2 +0.4*X[:,0]**4 - 3.5+0.3*np.random.randn(N)\n",
    "\n",
    "q1 = np.percentile(X, 25)\n",
    "q2 = np.percentile(X, 50)\n",
    "q3 = np.percentile(X, 75)\n",
    "H = spline(X, np.array([q1,q2,q3]), 4)\n",
    "\n",
    "beta = np.linalg.inv(H.T @ H) @ H.T @ y\n",
    "preds = H @ beta\n",
    "\n",
    "utils.plot_reg(X, y, preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Inference\n",
    "\n",
    "Let's define $Z$ a continious random variable that represents the data. $z_i = (x_i,y_i)$.  \n",
    "We define a probabality density function for $Z$:\n",
    "$$z_i \\sim g_\\theta(z)$$\n",
    "\n",
    "$\\theta$ represents the parameters governing the distribution of $Z$. For example, if $Z$ is normal we will have:\n",
    "$$\\theta = (\\mu, \\sigma^2)$$\n",
    "$$g_\\theta(z) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{(z - \\mu)^2}{2\\sigma^2}) $$\n",
    "\n",
    "We define the likelihood function as:\n",
    "$$L(\\theta;z) = g_\\theta(z)$$\n",
    "This is the probability of the observed data under the model $g_\\theta$\n",
    "$$L(\\theta;Z) = \\prod_{i=1}^N g_\\theta(z_i)$$\n",
    "$L(\\theta;Z)$ is a function of $\\theta$, with the observed data $Z$ fixed.  \n",
    "\n",
    "We define the log-likelihood:\n",
    "$$l(\\theta;z) = \\log L(\\theta;z)$$\n",
    "$$l(\\theta;Z) = \\sum_{i=1}^N \\log g_\\theta(z_i)$$  \n",
    "\n",
    "The method of maximum likelihood choose the value $\\hat{\\theta}$ to maximize $l(\\theta;Z)$  \n",
    "\n",
    "We define sore function:\n",
    "$$\\dot{l}(\\theta;Z) = \\frac{\\partial l(\\theta;Z)}{\\partial \\theta}$$\n",
    "$$\\dot{l}(\\theta;Z) = \\sum_{i=1}^N \\dot{l}(\\theta;z_i)$$\n",
    "$$\\text{with } \\dot{l}(\\theta;z_i) = \\frac{\\partial l(\\theta;z_i)}{\\partial \\theta}$$\n",
    "\n",
    "We have $\\dot{l}(\\hat{\\theta};Z)=0$.  \n",
    "\n",
    "We define the information matrix as:\n",
    "$$I(\\theta) = -\\sum_{i=1}^N \\frac{\\partial^2 l(\\theta;z_i)}{\\partial \\theta \\partial \\theta^T}$$\n",
    "\n",
    "$I(\\hat{\\theta})$ is called the obsvered information.  \n",
    "The Fisher information is defined:\n",
    "$$i(\\theta) = E_\\theta[I(\\theta)]$$  \n",
    "\n",
    "We denote $\\theta_0$ as the true value of $\\theta$.  \n",
    "As $N \\to \\infty$, the sampling distribution of the matrix likelihood estimator has a limitting normal distribution:\n",
    "$$\\hat{\\theta} \\to \\mathcal{N}(\\theta_0, i(\\theta_0)^{-1})$$  \n",
    "\n",
    "The sampling distribution of $\\hat{\\theta}$ may be approximated by:\n",
    "$$\\mathcal{N}(\\hat{\\theta}, I(\\hat{\\theta})^{-1})$$\n",
    "\n",
    "We can estimate the standard error of $\\hat{\\theta_j}$:\n",
    "$$\\text{se}[\\hat{\\theta_j}] = \\sqrt{I(\\hat{\\theta}_{jj}^{-1}}$$\n",
    "\n",
    "We can have condifence bounds for $\\theta_j$:\n",
    "$$\\hat{\\theta}_j \\pm z^{(1-\\alpha)} \\sqrt{I(\\hat{\\theta}_{jj}^{-1}}$$\n",
    "\n",
    "with $z^{(1 - \\alpha)}$ the $1 - \\alpha$ percentile of the standard normal distribution. For a condifence interval of 95\\% for example, $z^{(1-\\alpha)} = 1.96$ \n",
    "\n",
    "Let's apply the log-likelihood for a gaussian of parameters $\\theta = (\\beta, \\sigma^2)$.  The log-likelihood is:\n",
    "\n",
    "$$l(\\theta) = -\\frac{N}{2} \\log \\sigma^2 2\\pi - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - h(x_i)^T\\beta)^2$$\n",
    "\n",
    "We get the maximum likelihood estimate by solving:\n",
    "\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\beta} = 0$$\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\sigma^2} = 0$$\n",
    "\n",
    "We get the results:\n",
    "\n",
    "$$\\hat{\\beta} = (H^TH)^{-1}X^Ty$$\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{\\mu}(x_i))^2$$\n",
    "\n",
    "They are identical to the least squares estimate.  \n",
    "The part of the information matrix about $\\beta$ is:\n",
    "$$I(\\beta) = (H^TH) \\frac{1}{\\sigma^2}$$\n",
    "\n",
    "So the estimated variance $(H^TH)^{-1}\\hat{\\sigma}^2$ aggrees with least sqares estimate.  \n",
    "\n",
    "So least squares and maximum likelihood with a Gaussian give the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap versus Maximum Likelihood\n",
    "\n",
    "Boostrap can be seen as a computer implementation of the maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Bayesian approach, we define a sampling model $P(Z|\\theta)$ for our data given the parameters, and a prior distribution $P(\\theta)$ for the parameters, refecting our knowledge of $\\theta$ before seeing $Z$.  \n",
    "We then compute $P(\\theta|Z)$, the updated knowledge of $\\theta$ after seeing the data.\n",
    "\n",
    "$$P(\\theta|Z) = \\frac{P(Z|\\theta P(\\theta))}{\\int P(Z|\\theta) P(\\theta) d\\theta}$$\n",
    "\n",
    "The posterior distribution can also be used to predict values of fucture observations, via the predictive distribution:\n",
    "\n",
    "$$P(z^\\text{new}|Z) = \\int P(z^\\text{new}|\\theta) P(\\theta|Z)d\\theta$$  \n",
    "\n",
    "We can use the Bayesian approach to solve the example.  \n",
    "Let's assume $\\sigma^2$ kown, and $x_1,\\text{...},x_N$ constant, the randomnes comes from $y$ varying around $\\mu(x)$.  \n",
    "\n",
    "We choose a Gaussian prior centered at zero:\n",
    "$$\\beta \\sim \\mathcal{N}(0, \\tau \\Sigma)$$\n",
    "\n",
    "with prior correlation matrix $\\Sigma$ and variance $\\tau$\n",
    "\n",
    "Using Bayesian approach, the posterior distribution is also Gaussian, with mean and coravriance:\n",
    "\n",
    "$$E(\\beta|Z) = \\left(H^TH + \\frac{\\sigma^2}{\\tau} \\Sigma^{-1}\\right)^{-1}H^Ty$$\n",
    "$$\\text{cov}(\\beta|Z) = \\left(H^TH + \\frac{\\sigma^2}{\\tau} \\Sigma^{-1}\\right)^{-1}\\sigma^2$$\n",
    "\n",
    "For our problem, $\\mu(x)$ should be smooths, we can make it bo choosing as a prior $\\Sigma=I$.  \n",
    "As $\\tau \\to \\infty$, the posterior distribution and the bootstrap distribution coincide.  \n",
    "When $\\tau \\to \\infty$, the prior is called a noninformative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Components mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.5547557910238999\n",
      "u1 = 4.656549404108161\n",
      "u2 = 1.0837171125336311\n",
      "s1 = 0.8178682729284208\n",
      "s2 = 0.8122733838856797\n"
     ]
    }
   ],
   "source": [
    "def gauss_pdf(mu, s2, x):\n",
    "    num = np.exp(-((x - mu)**2)/(2*s2))\n",
    "    den = np.sqrt(2 * np.pi * s2)\n",
    "    return num/den\n",
    "\n",
    "def em(y, max_iters = 10000):\n",
    "    \n",
    "    #inititalization\n",
    "    u1, u2 = np.random.choice(y, 2, replace=False)\n",
    "    p = 0.5\n",
    "    s1 = s2 = np.mean((y - np.mean(y))**2)\n",
    "    lls = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        ## Expectation Step\n",
    "        res = np.empty(len(y))\n",
    "        for i in range(len(y)):\n",
    "            res[i] = (p*gauss_pdf(u2,s2,y[i])) / (\n",
    "                (1-p)*gauss_pdf(u1,s1,y[i]) + p*gauss_pdf(u2,s2,y[i])\n",
    "            )                                           \n",
    "        \n",
    "        ##maximization step\n",
    "        u1 = np.sum((1 - res) * y) / np.sum(1 - res)\n",
    "        u2 = np.sum(res * y) / np.sum(res)\n",
    "        s1 = np.sum((1 - res) * (y - u1)**2) / np.sum(1 - res)\n",
    "        s2 = np.sum(res * (y - u2)**2) / np.sum(res)\n",
    "        p = np.mean(res)\n",
    "        \n",
    "        ##compute log-likelihood\n",
    "        ll = 0\n",
    "        for i in range(len(y)):\n",
    "            ll += np.log((1-p)*gauss_pdf(u1,s1,y[i])\n",
    "                       + p*gauss_pdf(u2,s2,y[i]))\n",
    "        lls.append(ll)\n",
    "        if len(lls) > 1:\n",
    "            diff = (lls[-1] - lls[-2])**2\n",
    "            if diff < 1e-10:\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return p, u1, u2, s1, s2, lls\n",
    "\n",
    "y = np.array([\n",
    "    -0.39, 0.12, 0.94, 1.67, 1.76, 2.44, 3.72, 4.28, 4.92, 5.53,\n",
    "     0.06, 0.48, 1.01, 1.68, 1.80, 3.25, 4.12, 4.60, 5.28, 6.22\n",
    "])\n",
    "\n",
    "p, u1, u2, s1, s2, lls = em(y)\n",
    "\n",
    "plt.plot(np.arange(1,len(lls)+1), lls)\n",
    "plt.show()\n",
    "\n",
    "print('p =', p)\n",
    "print('u1 =', u1)\n",
    "print('u2 =', u2)\n",
    "print('s1 =', s1)\n",
    "print('s2 =', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation\n",
    "\n",
    "We have a dataset of points $x_1,\\text{...},x_N$, with $x_i \\in \\mathbb{R}^p$.  \n",
    "Density estimation build a model to estimate and fit the density $P(x)$.  \n",
    "\n",
    "It can be used for anomaly detection. We have a dataset of normal events $x_1,\\text{...},x_N$, and we build a model $P(x)$.  \n",
    "For a new event $x$, we compute the probability that $x$ comes from $P(x)$. If this probability is too low, the example is detected as an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "We have the random variable for our dataset $x$.  \n",
    "Let's define a lattent (hidden / unobserved) latent variable $z$.  \n",
    "$(x,z)$ have a joint distribution:\n",
    "$$P(x,z) = P(x|z) P(z)$$\n",
    "\n",
    "with\n",
    "$$Z \\sim \\text{Multinomial}(\\phi)$$\n",
    "$$X|Z=j \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)$$  \n",
    "\n",
    "We need to find $\\phi \\in \\mathbb{R}^{K}$ ($K$ number of gaussians), $\\mu_j \\in \\mathbb{R}^p$, and $\\Sigma_j \\in \\mathbb{R}^{p*p}$ that fit best the dataset $X$.  \n",
    "\n",
    "If the $z_i$ are known, the problem can be easily solved with maximum likelihood estimation:\n",
    "$$l(\\phi,\\mu,\\Sigma) = \\sum_{i=1}^N \\log p(x_i,z_i;\\phi,\\mu,\\Sigma)$$\n",
    "\n",
    "We get the folowing estimators:\n",
    "\n",
    "$$\\phi_j = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}(z_i = j)$$\n",
    "\n",
    "$$\\mu_j = \\frac{\\sum_{i=1}^N \\mathbf{1}(z_i = j)x_i}{\\sum_{i=1}^N \\mathbf{1}(z_i = j)}$$\n",
    "\n",
    "$$\\Sigma_j = \\frac{\\sum_{i=1}^N \\mathbf{1}(z_i = j)(x_i - \\mu_j)(x_i - \\mu_j)^T}{\\sum_{i=1}^N \\mathbf{1}(z_i = j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi[0] = 0.2\n",
      "mu[0] = [ 1.4  6.7 -2.4]\n",
      "sig[0] =\n",
      "[[ 62.96  34.8  -18.38]\n",
      " [ 34.8   20.45 -16.47]\n",
      " [-18.38 -16.47 176.34]]\n",
      "phi[1] = 0.5\n",
      "mu[1] = [ 0.2 -5.2  4.5]\n",
      "sig[1] =\n",
      "[[ 95.89   1.9   -7.27]\n",
      " [  1.9   40.56 -27.96]\n",
      " [ -7.27 -27.96  40.38]]\n",
      "phi[2] = 0.25\n",
      "mu[2] = [-5.7 -5.3 -1.4]\n",
      "sig[2] =\n",
      "[[ 97.02    62.78    73.985 ]\n",
      " [ 62.78    56.24    87.35  ]\n",
      " [ 73.985   87.35   168.8825]]\n",
      "phi[3] = 0.05\n",
      "mu[3] = [3.5 4.6 2.8]\n",
      "sig[3] =\n",
      "[[ 89.94  40.77 -72.09]\n",
      " [ 40.77  50.66 -64.39]\n",
      " [-72.09 -64.39 103.7 ]]\n"
     ]
    }
   ],
   "source": [
    "N = 50#0000\n",
    "p = 3\n",
    "K = 4\n",
    "\n",
    "rphi = [0.2, 0.5, 0.25, 0.05]\n",
    "\n",
    "rmu = [\n",
    "    np.array([1.4, 6.7, -2.4]),\n",
    "    np.array([0.2, -5.2, 4.5]),\n",
    "    np.array([-5.7, -5.3, -1.4]),\n",
    "    np.array([3.5, 4.6, 2.8])\n",
    "]\n",
    "\n",
    "rsig = [\n",
    "    np.array([[1.4,1.6,2.8],[0.4,-0.5,12.5],[7.8,4.2,-3.5]]),\n",
    "    np.array([[1.8,-2.8,5.8],[8.4,3.4,-2.5],[-4.7,4.6,-0.7]]),\n",
    "    np.array([[2.3,3.6,4.2],[7.3,2.2,-1.55],[6.2,6.2,12.2]]),\n",
    "    np.array([[1.7,-4.8,3.9],[2.8,1.9,-6.5],[8.9,4.9,-6.8]])\n",
    "]\n",
    "for i in range(K):\n",
    "    rsig[i] = rsig[i].T @ rsig[i] #generate PSD matrices\n",
    "\n",
    "\n",
    "Zc = np.random.multinomial(N, rphi)\n",
    "Z = []\n",
    "for i in range(K):\n",
    "    Z += [i] * Zc[i]\n",
    "Z = np.array(Z).astype(np.int)\n",
    "np.random.shuffle(Z)\n",
    "\n",
    "X = np.empty((N, p))\n",
    "for i in range(K):\n",
    "    X[Z==i] = np.random.multivariate_normal(rmu[i], rsig[i], \n",
    "                                            size=len(X[Z==i]),\n",
    "                                            check_valid='raise')\n",
    "    \n",
    "for i in range(K):\n",
    "    print('phi[{}] = {}'.format(i, rphi[i]))\n",
    "    print('mu[{}] = {}'.format(i, rmu[i]))\n",
    "    print('sig[{}] ='.format(i))\n",
    "    print(rsig[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi[0] = 0.26\n",
      "mu[0] = [ 5.3766818   8.70469689 -4.10641438]\n",
      "sig[0] =\n",
      "[[21.81082815 10.77820134 -1.65880742]\n",
      " [10.77820134  6.36100409 -4.24129223]\n",
      " [-1.65880742 -4.24129223 34.58774118]]\n",
      "phi[1] = 0.4\n",
      "mu[1] = [ 1.80742846 -8.36999981  8.11121841]\n",
      "sig[1] =\n",
      "[[ 56.98237628   0.61071025  -4.47229418]\n",
      " [  0.61071025  34.8863811  -28.63994442]\n",
      " [ -4.47229418 -28.63994442  62.61604857]]\n",
      "phi[2] = 0.28\n",
      "mu[2] = [-4.99069781 -3.27497304  1.49277116]\n",
      "sig[2] =\n",
      "[[ 97.75648235  75.4999698  121.83048613]\n",
      " [ 75.4999698   72.52674877 126.97800062]\n",
      " [121.83048613 126.97800062 242.42017727]]\n",
      "phi[3] = 0.06\n",
      "mu[3] = [0.43523515 0.86281594 5.12846802]\n",
      "sig[3] =\n",
      "[[ 22.35046223   2.91020802 -17.04590499]\n",
      " [  2.91020802  52.78794955 -59.14160513]\n",
      " [-17.04590499 -59.14160513  74.82410784]]\n"
     ]
    }
   ],
   "source": [
    "def mle_mixture(X, Z, K):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    phi = np.zeros(K)\n",
    "    mu = np.zeros((K, p))\n",
    "    sig = np.zeros((K,p,p))\n",
    "    \n",
    "    #count number of samples from each mixture\n",
    "    cs = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        cs[k] = np.sum(Z == k)\n",
    "    #print(cs)\n",
    "    \n",
    "    #compute phi\n",
    "    phi = cs / N\n",
    "        \n",
    "    #compute mu\n",
    "    for i in range(N):\n",
    "        mu[Z[i]] += X[i]\n",
    "    mu /= cs.reshape(K,1)\n",
    "    \n",
    "    #compute sig\n",
    "    for i in range(N):\n",
    "        sig[Z[i]] += np.outer(X[i] - mu[Z[i]], X[i] - mu[Z[i]])\n",
    "    sig /= cs.reshape(K,1,1)\n",
    "    \n",
    "    return phi, mu, sig\n",
    "    \n",
    "    \n",
    "    \n",
    "phi, mu, sig = mle_mixture(X, Z, K)\n",
    "\n",
    "for i in range(K):\n",
    "    print('phi[{}] = {}'.format(i, phi[i]))\n",
    "    print('mu[{}] = {}'.format(i, mu[i]))\n",
    "    print('sig[{}] ='.format(i))\n",
    "    print(sig[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm for mixture of Gaussians\n",
    "\n",
    "Repeat E-Step and M-Step until convergence\n",
    "\n",
    "### E-Step\n",
    "\n",
    "Define $w_{ij} = P(z_i=j|x_i,\\phi,\\mu,\\sigma)$\n",
    "$$w_{ij} = \\frac{P(x_i|z_i=j) P(z_i=j)}{\\sum_{l=1}^K P(x_i|z_i=l) P(z_i=l)}$$\n",
    "\n",
    "### M-Step\n",
    "\n",
    "Apply MLE:\n",
    "\n",
    "$$\\phi_j = \\frac{1}{N} \\sum_{i=1}^N w_{ij}$$\n",
    "\n",
    "$$\\mu_j = \\frac{ \\sum_{i=1}^N w_{ij} x_i}{ \\sum_{i=1}^N w_{ij}} $$\n",
    "\n",
    "$$\\sigma_j = \\frac{ \\sum_{i=1}^N w_{ij} (x_i - \\mu_j)(x_i - \\mu_j)^T}{ \\sum_{i=1}^N w_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24456718 0.19667518 0.35196461 0.20679304]\n"
     ]
    }
   ],
   "source": [
    "class MultivariateNormal:\n",
    "    \n",
    "    def __init__(self, mu, cov):\n",
    "        self.mu = mu\n",
    "        self.cov = cov\n",
    "        self.icov = np.linalg.inv(cov)\n",
    "        #self.den = 1/np.sqrt(max(np.linalg.det(self.cov), 1e-8))\n",
    "        p = len(self.mu)\n",
    "        \n",
    "        cdet = np.linalg.det(self.cov)\n",
    "        if (cdet < 0):\n",
    "            print(self.cov)\n",
    "            print(self.icov)\n",
    "            print(cdet)\n",
    "            djdjdjhfh\n",
    "        \n",
    "        self.den = 1/np.sqrt((2*np.pi)**p * cdet)\n",
    "        \n",
    "    def pdf(self, x):\n",
    "        num = np.exp(-.5  * (x-self.mu)@self.icov@(x-self.mu))\n",
    "        return num/self.den\n",
    "        \n",
    "\n",
    "def em_gauss_mixture(X,k):\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    phi = np.ones(K)*1/K\n",
    "    \n",
    "    mup = np.random.choice(len(X), K, replace=False)\n",
    "    mu =  X[mup]\n",
    "    \n",
    "    Xc = np.mean(X,axis=0, keepdims=True)\n",
    "    sigi = 1/N * (Xc - X).T @ (Xc - X)\n",
    "    sig = np.repeat(sigi.reshape(1, p, p), repeats=K, axis=0)\n",
    "    \n",
    "    dens = [None] * K\n",
    "    for k in range(K):\n",
    "        dens[k] = MultivariateNormal(mu[k], sig[k])\n",
    "    \n",
    "    \n",
    "    for ite in range(1):\n",
    "    \n",
    "        \n",
    "        #Expectation step\n",
    "        w = np.empty((N, K))\n",
    "        for i in range(N):\n",
    "            \n",
    "            prs = np.array([\n",
    "               dens[j].pdf(X[i]) * phi[j] for j in range(K) \n",
    "            ])\n",
    "            prs_sum = np.sum(prs)\n",
    "            \n",
    "            for j in range(K):\n",
    "                w[i,j] = prs[j] / prs_sum\n",
    "        \n",
    "        \n",
    "        #Maximization step\n",
    "        phi = 1/N * np.sum(w, axis=0)\n",
    "        \n",
    "        for j in range(K):\n",
    "            sig[j] = 0\n",
    "            for i in range(N):\n",
    "                sig[j] += w[i,j]*np.outer(X[i] - mu[j], X[i] - mu[j])\n",
    "            sig[j] /= np.sum(w[:,j])\n",
    "        \n",
    "        \n",
    "        for j in range(K):\n",
    "            mu[j] = np.sum(w[:,j].reshape(N,1)*X, axis=0) / np.sum(w[:,j])\n",
    "        \n",
    "        for j in range(K):\n",
    "            dens[j] = MultivariateNormal(mu[j], sig[j])\n",
    "            \n",
    "\n",
    "        #print(phi)\n",
    "        \n",
    "            \n",
    "            \n",
    "    return phi, mu, sig \n",
    "    \n",
    "phi, mu, sig = em_gauss_mixture(X,K)\n",
    "\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen Inequality\n",
    "\n",
    "Let $f$ a convex function ($f''(x) \\geq 0$) and $X$ a random variable.  \n",
    "$$f(E[X]) \\leq E[f(X)]$$\n",
    "\n",
    "If $f$ is strictly convex ($f''(x) > 0$):\n",
    "$$f(E[X]) = E[f(X)] \\text{ iff } X=E[X]$$\n",
    "\n",
    "The opposite inequality holds for concave functions ($f''(x) \\leq 0$):\n",
    "$$f(E[X]) \\geq E[f(X)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm\n",
    "\n",
    "the log-likelihood of our model is:\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^N \\log \\sum_{z_i} P(x_i,z_i,\\theta)$$\n",
    "\n",
    "But the $z_i$ are not observed, and solving this problem with MLE is quite hard.  \n",
    "Another strategy is to build a lower-bound for $l(\\theta)$ (E-step) and optimize for that lower-bound (M-step).  \n",
    "\n",
    "Let's define for each $i$  some distribution $Q_i$ over the $z$'s ($\\sum_z Q_i(z) = 1, Q_i(z) \\geq 0$).\n",
    "\n",
    "Using Jensen Inequality, we find a lower bound for $l(\\theta)$:\n",
    "$$l(\\theta) \\geq \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$\n",
    "\n",
    "To get a tight lower bound, we need the Jensen step to hold for equality:\n",
    "$$\\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)} = \\text{constant}$$\n",
    "\n",
    "Solving this equality, we get:\n",
    "$$Q_i(z_i) = P(z_i|x_i;\\theta)$$\n",
    "\n",
    "EM algorithm:\n",
    "\n",
    "- E-step:\n",
    "$$\\text{Set } Q_i(z_i) = P(z_i|x_i;\\theta)$$\n",
    "\n",
    "- M-Step:\n",
    "$$\\theta \\leftarrow \\arg \\max_\\theta \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm and coordinate ascent\n",
    "\n",
    "Let's define:\n",
    "$$J(Q,\\theta) = \\sum_{i=1}^N \\sum_{z_i} Q_i(z_i) \\log \\frac{P(x_i,z_i;\\theta)}{Q_i(z_i)}$$\n",
    "Then we know that:\n",
    "$$l(\\theta) \\geq J(Q,\\theta)$$\n",
    "\n",
    "We can prove that the EM algorithm is equivalent to applying Coordinate ascent on $J(Q,\\theta)$:\n",
    "- The E-Step maximixes $J$ with respect to $Q$\n",
    "- The M-Step maximixes $J$ with respect to $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC for Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMCM (Markov Chain Monte Carlo) is a technique to draw samples from a posterior distribution, when it's density is intractable.  \n",
    "We can then use these samples to guess properties of the distribution.  \n",
    "\n",
    "Suppose we have random variables $U_1, U_2$, ..., $U_K$, and we wish to sample from the joint distribution.  \n",
    "If we can easily sample from the conditional distribution $P(U_j|U_1,U_2,$ ..., $U_{j-1}, U_{j+1},$ ..., $U_K)$, then we can use the Gibbs Sampling mehod, that samples a;ternatively from each of the conditionals distributions.  \n",
    "Gibbs Sampling produces a Markov Chain whose stationarry distribution is the true joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampler Algorithm\n",
    "\n",
    "1. Take some initial values $U_k^{(0)}$, $k=1,2,$ ..., $K$.\n",
    "\n",
    "2. Repeat for $t=1,2,\\text{...}:$\n",
    "\n",
    "$$\\text{For } k=1,2,\\text{...},K \\text{ generate } U_k^{(t)} \\text{ from } P(U_k^{(t)}|U_1^{(t)},\\text{...},U_{k-1}^{(t)}, U_{k+1}^{(t-1)}, \\text{...}, U_K^{(t-1)})$$\n",
    "\n",
    "3. Repeat step 2 until the joint distribution converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.99992574 4.99998352]\n",
      "[[0.99979984 0.89956632]\n",
      " [0.89956632 0.99938921]]\n"
     ]
    }
   ],
   "source": [
    "mus = np.array([5, 5])\n",
    "sigmas = np.array([[1, .9], [.9, 1]])\n",
    "\n",
    "def get_exp_cov(X):\n",
    "    e = np.mean(X, axis=0)\n",
    "    cov = (X - e.reshape(1, -1)).T @ (X - e.reshape(1, -1)) / len(X)\n",
    "    return e, cov\n",
    "\n",
    "X = np.random.multivariate_normal(mus, sigmas, size=5000000)\n",
    "e, cov = get_exp_cov(X)\n",
    "print(e)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19800, 2)\n",
      "[4.99931287 4.99819646]\n",
      "[[0.19076019 0.17244894]\n",
      " [0.17244894 0.19272046]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CondGauss:\n",
    "    \n",
    "    def __init__(self, mus, sigmas):\n",
    "        self.mus = mus\n",
    "        self.sigmas = sigmas\n",
    "        \n",
    "        self.sigma_xy = sigmas[0,0]-sigmas[1,0]/sigmas[1,1]*sigmas[1,0]\n",
    "        self.sigma_yx = sigmas[1,1]-sigmas[0,1]/sigmas[0,0]*sigmas[0,1]\n",
    "\n",
    "    def p_x_given_y(self, y):\n",
    "        mu = self.mus[0]+self.sigmas[1,0]/self.sigmas[0,0]*(y-self.mus[1])\n",
    "        return np.random.normal(mu, self.sigma_xy)\n",
    "\n",
    "    def p_y_given_x(self, x):\n",
    "        mu = self.mus[1]+self.sigmas[0,1]/self.sigmas[1,1]*(x-self.mus[0])\n",
    "        return np.random.normal(mu, self.sigma_yx)\n",
    "\n",
    "\n",
    "def gibbs_sampling(mus, sigmas, N=1000000):\n",
    "    samples = np.zeros((N, 2))\n",
    "    cond = CondGauss(mus, sigmas)\n",
    "    y = np.random.rand() * 10\n",
    "\n",
    "    for i in range(N):\n",
    "        x = cond.p_x_given_y(y)\n",
    "        y = cond.p_y_given_x(x)\n",
    "        samples[i] = [x,y]\n",
    "\n",
    "    return samples\n",
    "\n",
    "samples = gibbs_sampling(mus, sigmas)\n",
    "samples = samples[10000:]\n",
    "samples = samples[::50]\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "\n",
    "e, cov = get_exp_cov(samples)\n",
    "print(e)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling and LDA\n",
    "\n",
    "LDA in as algorithm to compute a distribution of topics for each documents, and a distribution of words for each topic.  \n",
    "It takes as input several documents as bag of words (number of instances of every word of the vocabulary for each document).  \n",
    "The number of a topics in an hyperparameter.  \n",
    "The algorithm can be used to distinguish topic between all documents. It can describes which words are more likely to appear for each topic.  \n",
    "Finally, given a new document, it can computes it's topic distribution. This can be helpful to categorize the text, and to find other texts of similar topics (recommander system).  \n",
    "\n",
    "Usually NLP preprocessing (stop words, lemming, stemming, filtering) is usually applied before building a bag of words from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aiw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "download dataset from kaggle:\n",
    "https://www.kaggle.com/therohk/million-headlines/data\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def load_data(max_docs = None):\n",
    "\n",
    "\n",
    "    # Loading data\n",
    "    print('Loading data...')\n",
    "\n",
    "\n",
    "    data = pd.read_csv('../../data/abcnews-date-text.csv',\n",
    "                       error_bad_lines=False)\n",
    "    data_text = data[['headline_text']]\n",
    "    data_text['index'] = data_text.index\n",
    "    docs = data_text\n",
    "    if max_docs is not None:\n",
    "        docs = docs[:max_docs]\n",
    "\n",
    "\n",
    "    # Preprocesing\n",
    "    print('Preprocessing data...')\n",
    "\n",
    "\n",
    "\n",
    "    docs = docs['headline_text'].map(preprocess)\n",
    "\n",
    "\n",
    "    # Building dictionary and bag of words\n",
    "    print('Building dictionarry and bag of words...')\n",
    "    dic = gensim.corpora.Dictionary(docs)\n",
    "    dic.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    bow = [dic.doc2bow(doc) for doc in docs]\n",
    "\n",
    "\n",
    "    # Build tf-idf mdel\n",
    "    print('Building tf-idf model...')\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    docs_tfidf = tfidf[bow]\n",
    "\n",
    "\n",
    "    print('Get {} documents'.format(len(docs)))\n",
    "    print('Get {} words'.format(len(dic)))\n",
    "    \n",
    "    return dic, bow, docs_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Building dictionarry and bag of words...\n",
      "Building tf-idf model...\n",
      "Get 1103663 documents\n",
      "Get 14173 words\n",
      "Training LDA with bag of words\n",
      "LDA results with bag of words:\n",
      "Topic: 0 \n",
      "Words: 0.026*\"australia\" + 0.022*\"test\" + 0.018*\"hospit\" + 0.016*\"hour\" + 0.015*\"leav\" + 0.013*\"driver\" + 0.011*\"prison\" + 0.011*\"releas\" + 0.011*\"china\" + 0.010*\"young\"\n",
      "Topic: 1 \n",
      "Words: 0.025*\"govern\" + 0.017*\"say\" + 0.016*\"chang\" + 0.015*\"nation\" + 0.014*\"countri\" + 0.013*\"live\" + 0.013*\"interview\" + 0.011*\"council\" + 0.010*\"call\" + 0.010*\"minist\"\n",
      "Topic: 2 \n",
      "Words: 0.021*\"final\" + 0.019*\"win\" + 0.016*\"leagu\" + 0.015*\"lose\" + 0.014*\"citi\" + 0.012*\"john\" + 0.012*\"announc\" + 0.010*\"unit\" + 0.010*\"celebr\" + 0.009*\"melbourn\"\n",
      "Topic: 3 \n",
      "Words: 0.016*\"donald\" + 0.016*\"adelaid\" + 0.015*\"turnbul\" + 0.013*\"feder\" + 0.011*\"centr\" + 0.011*\"vote\" + 0.010*\"game\" + 0.010*\"port\" + 0.009*\"refuge\" + 0.009*\"save\"\n",
      "Topic: 4 \n",
      "Words: 0.027*\"report\" + 0.021*\"famili\" + 0.017*\"water\" + 0.016*\"time\" + 0.015*\"concern\" + 0.014*\"abus\" + 0.014*\"busi\" + 0.013*\"guilti\" + 0.012*\"farm\" + 0.011*\"find\"\n",
      "Topic: 5 \n",
      "Words: 0.045*\"polic\" + 0.026*\"death\" + 0.026*\"attack\" + 0.023*\"kill\" + 0.021*\"crash\" + 0.019*\"die\" + 0.016*\"shoot\" + 0.015*\"rise\" + 0.015*\"woman\" + 0.014*\"arrest\"\n",
      "Topic: 6 \n",
      "Words: 0.034*\"court\" + 0.026*\"murder\" + 0.025*\"charg\" + 0.022*\"face\" + 0.018*\"miss\" + 0.017*\"accus\" + 0.016*\"child\" + 0.015*\"alleg\" + 0.014*\"peopl\" + 0.014*\"trial\"\n",
      "Topic: 7 \n",
      "Words: 0.027*\"world\" + 0.020*\"market\" + 0.019*\"women\" + 0.016*\"tasmania\" + 0.015*\"open\" + 0.014*\"share\" + 0.014*\"break\" + 0.014*\"life\" + 0.013*\"take\" + 0.013*\"student\"\n",
      "Topic: 8 \n",
      "Words: 0.042*\"australian\" + 0.022*\"south\" + 0.021*\"north\" + 0.016*\"elect\" + 0.016*\"australia\" + 0.014*\"west\" + 0.014*\"power\" + 0.014*\"help\" + 0.011*\"victoria\" + 0.011*\"flood\"\n",
      "Topic: 9 \n",
      "Words: 0.036*\"trump\" + 0.036*\"year\" + 0.022*\"hous\" + 0.021*\"canberra\" + 0.020*\"queensland\" + 0.012*\"elect\" + 0.011*\"bank\" + 0.011*\"say\" + 0.011*\"polit\" + 0.010*\"hobart\"\n",
      "Training LDA with TF-IDF\n",
      "LDA results with TF-IDF\n",
      "Topic: 0\n",
      "Words: 0.018*\"interview\" + 0.009*\"royal\" + 0.008*\"commiss\" + 0.007*\"michael\" + 0.006*\"energi\" + 0.006*\"septemb\" + 0.005*\"mount\" + 0.005*\"histori\" + 0.005*\"say\" + 0.005*\"young\"\n",
      "Topic: 1\n",
      "Words: 0.019*\"charg\" + 0.018*\"polic\" + 0.017*\"murder\" + 0.013*\"court\" + 0.012*\"woman\" + 0.010*\"jail\" + 0.009*\"assault\" + 0.009*\"death\" + 0.009*\"alleg\" + 0.009*\"accus\"\n",
      "Topic: 2\n",
      "Words: 0.016*\"crash\" + 0.015*\"north\" + 0.011*\"drum\" + 0.011*\"die\" + 0.011*\"south\" + 0.008*\"sport\" + 0.008*\"korea\" + 0.007*\"victoria\" + 0.007*\"west\" + 0.007*\"coast\"\n",
      "Topic: 3\n",
      "Words: 0.010*\"weather\" + 0.010*\"elect\" + 0.009*\"violenc\" + 0.007*\"decemb\" + 0.007*\"domest\" + 0.006*\"week\" + 0.006*\"action\" + 0.006*\"kid\" + 0.006*\"white\" + 0.005*\"burn\"\n",
      "Topic: 4\n",
      "Words: 0.008*\"abbott\" + 0.008*\"john\" + 0.008*\"coal\" + 0.008*\"david\" + 0.007*\"ash\" + 0.007*\"mother\" + 0.007*\"peter\" + 0.007*\"monday\" + 0.007*\"award\" + 0.006*\"zealand\"\n",
      "Topic: 5\n",
      "Words: 0.012*\"market\" + 0.011*\"share\" + 0.009*\"prison\" + 0.007*\"australian\" + 0.007*\"search\" + 0.007*\"wednesday\" + 0.007*\"wall\" + 0.006*\"august\" + 0.006*\"syria\" + 0.006*\"street\"\n",
      "Topic: 6\n",
      "Words: 0.019*\"trump\" + 0.011*\"final\" + 0.009*\"australia\" + 0.009*\"world\" + 0.009*\"leagu\" + 0.007*\"open\" + 0.006*\"cricket\" + 0.006*\"juli\" + 0.006*\"victorian\" + 0.006*\"test\"\n",
      "Topic: 7\n",
      "Words: 0.010*\"turnbul\" + 0.008*\"christma\" + 0.007*\"flood\" + 0.007*\"price\" + 0.006*\"rise\" + 0.006*\"malcolm\" + 0.006*\"farm\" + 0.005*\"kill\" + 0.005*\"damag\" + 0.004*\"cyclon\"\n",
      "Topic: 8\n",
      "Words: 0.027*\"countri\" + 0.027*\"rural\" + 0.024*\"hour\" + 0.017*\"news\" + 0.013*\"podcast\" + 0.012*\"donald\" + 0.008*\"friday\" + 0.008*\"asylum\" + 0.008*\"octob\" + 0.007*\"nation\"\n",
      "Topic: 9\n",
      "Words: 0.009*\"govern\" + 0.008*\"live\" + 0.008*\"health\" + 0.007*\"fund\" + 0.006*\"farmer\" + 0.005*\"plan\" + 0.005*\"council\" + 0.005*\"budget\" + 0.005*\"chang\" + 0.005*\"commun\"\n"
     ]
    }
   ],
   "source": [
    "dic, bow, docs_tfidf = load_data()\n",
    "\n",
    "print('Training LDA with bag of words')\n",
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=10, id2word=dic, passes=2, workers=2)\n",
    "\n",
    "print('LDA results with bag of words:')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "\n",
    "print('Training LDA with TF-IDF')\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(docs_tfidf, num_topics=10, id2word=dic, passes=2, workers=4)\n",
    "\n",
    "print('LDA results with TF-IDF')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}\\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Building dictionarry and bag of words...\n",
      "Building tf-idf model...\n",
      "Get 1000 documents\n",
      "Get 18 words\n",
      "Training LDA with bag of words\n",
      "LDA results with bag of words:\n",
      "Topic: 0 \n",
      "Words: 0.475*\"charg\" + 0.120*\"polic\" + 0.084*\"say\" + 0.057*\"iraq\" + 0.044*\"rain\" + 0.044*\"investig\" + 0.030*\"warn\" + 0.030*\"plan\" + 0.030*\"govt\" + 0.030*\"continu\"\n",
      "Topic: 1 \n",
      "Words: 0.413*\"warn\" + 0.404*\"claim\" + 0.044*\"fund\" + 0.031*\"iraq\" + 0.024*\"charg\" + 0.019*\"murder\" + 0.013*\"council\" + 0.013*\"court\" + 0.013*\"record\" + 0.003*\"rain\"\n",
      "Topic: 2 \n",
      "Words: 0.610*\"polic\" + 0.109*\"plan\" + 0.079*\"record\" + 0.070*\"iraq\" + 0.060*\"warn\" + 0.017*\"investig\" + 0.016*\"continu\" + 0.013*\"govt\" + 0.003*\"council\" + 0.003*\"rain\"\n",
      "Topic: 3 \n",
      "Words: 0.312*\"fund\" + 0.259*\"record\" + 0.249*\"murder\" + 0.067*\"claim\" + 0.033*\"death\" + 0.019*\"rain\" + 0.019*\"investig\" + 0.012*\"council\" + 0.011*\"polic\" + 0.002*\"plan\"\n",
      "Topic: 4 \n",
      "Words: 0.433*\"court\" + 0.289*\"council\" + 0.152*\"iraq\" + 0.046*\"murder\" + 0.026*\"charg\" + 0.018*\"water\" + 0.012*\"plan\" + 0.002*\"govt\" + 0.002*\"rain\" + 0.002*\"record\"\n",
      "Topic: 5 \n",
      "Words: 0.376*\"say\" + 0.323*\"investig\" + 0.070*\"claim\" + 0.066*\"continu\" + 0.065*\"iraq\" + 0.022*\"rain\" + 0.022*\"council\" + 0.017*\"polic\" + 0.015*\"warn\" + 0.003*\"govt\"\n",
      "Topic: 6 \n",
      "Words: 0.480*\"govt\" + 0.314*\"continu\" + 0.093*\"court\" + 0.022*\"say\" + 0.019*\"rain\" + 0.017*\"death\" + 0.017*\"water\" + 0.004*\"plan\" + 0.004*\"iraq\" + 0.004*\"council\"\n",
      "Topic: 7 \n",
      "Words: 0.530*\"plan\" + 0.270*\"water\" + 0.071*\"warn\" + 0.042*\"charg\" + 0.018*\"murder\" + 0.018*\"fund\" + 0.012*\"council\" + 0.010*\"death\" + 0.010*\"investig\" + 0.002*\"iraq\"\n",
      "Topic: 8 \n",
      "Words: 0.265*\"death\" + 0.257*\"iraq\" + 0.152*\"council\" + 0.133*\"govt\" + 0.060*\"continu\" + 0.052*\"claim\" + 0.039*\"murder\" + 0.012*\"polic\" + 0.010*\"charg\" + 0.002*\"plan\"\n",
      "Topic: 9 \n",
      "Words: 0.659*\"rain\" + 0.091*\"water\" + 0.073*\"govt\" + 0.052*\"council\" + 0.034*\"death\" + 0.024*\"iraq\" + 0.013*\"plan\" + 0.013*\"court\" + 0.013*\"record\" + 0.003*\"polic\"\n",
      "Training LDA with TF-IDF\n",
      "LDA results with TF-IDF\n",
      "Topic: 0 Word: 0.375*\"say\" + 0.318*\"investig\" + 0.084*\"polic\" + 0.060*\"govt\" + 0.045*\"warn\" + 0.036*\"court\" + 0.031*\"rain\" + 0.014*\"water\" + 0.011*\"claim\" + 0.003*\"plan\"\n",
      "Topic: 1 Word: 0.282*\"warn\" + 0.264*\"murder\" + 0.219*\"water\" + 0.074*\"charg\" + 0.055*\"court\" + 0.037*\"claim\" + 0.018*\"plan\" + 0.014*\"record\" + 0.011*\"council\" + 0.003*\"polic\"\n",
      "Topic: 2 Word: 0.428*\"govt\" + 0.324*\"death\" + 0.053*\"investig\" + 0.043*\"court\" + 0.034*\"polic\" + 0.021*\"water\" + 0.020*\"rain\" + 0.014*\"plan\" + 0.013*\"iraq\" + 0.013*\"fund\"\n",
      "Topic: 3 Word: 0.522*\"iraq\" + 0.295*\"continu\" + 0.055*\"warn\" + 0.051*\"say\" + 0.028*\"charg\" + 0.011*\"fund\" + 0.009*\"death\" + 0.008*\"plan\" + 0.002*\"council\" + 0.002*\"court\"\n",
      "Topic: 4 Word: 0.497*\"record\" + 0.125*\"polic\" + 0.063*\"charg\" + 0.060*\"plan\" + 0.059*\"claim\" + 0.059*\"iraq\" + 0.045*\"warn\" + 0.031*\"fund\" + 0.017*\"say\" + 0.014*\"continu\"\n",
      "Topic: 5 Word: 0.386*\"plan\" + 0.279*\"court\" + 0.180*\"claim\" + 0.052*\"water\" + 0.027*\"death\" + 0.023*\"warn\" + 0.017*\"murder\" + 0.011*\"record\" + 0.006*\"charg\" + 0.005*\"rain\"\n",
      "Topic: 6 Word: 0.497*\"polic\" + 0.164*\"claim\" + 0.127*\"govt\" + 0.078*\"murder\" + 0.048*\"council\" + 0.018*\"investig\" + 0.012*\"death\" + 0.011*\"charg\" + 0.011*\"plan\" + 0.010*\"continu\"\n",
      "Topic: 7 Word: 0.475*\"council\" + 0.318*\"fund\" + 0.074*\"charg\" + 0.036*\"rain\" + 0.019*\"warn\" + 0.019*\"death\" + 0.011*\"investig\" + 0.009*\"plan\" + 0.008*\"iraq\" + 0.008*\"murder\"\n",
      "Topic: 8 Word: 0.594*\"rain\" + 0.177*\"charg\" + 0.114*\"water\" + 0.032*\"claim\" + 0.014*\"record\" + 0.012*\"continu\" + 0.011*\"murder\" + 0.010*\"polic\" + 0.010*\"plan\" + 0.003*\"iraq\"\n",
      "Topic: 9 Word: 0.477*\"warn\" + 0.068*\"plan\" + 0.068*\"investig\" + 0.068*\"fund\" + 0.053*\"murder\" + 0.052*\"charg\" + 0.051*\"iraq\" + 0.015*\"claim\" + 0.015*\"court\" + 0.015*\"council\"\n"
     ]
    }
   ],
   "source": [
    "dic, bow, docs_tfidf = load_data(max_docs=1000)\n",
    "\n",
    "print('Training LDA with bag of words')\n",
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=10, id2word=dic, passes=2, workers=2)\n",
    "\n",
    "print('LDA results with bag of words:')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "\n",
    "print('Training LDA with TF-IDF')\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(docs_tfidf, num_topics=10, id2word=dic, passes=2, workers=4)\n",
    "\n",
    "print('LDA results with TF-IDF')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define:\n",
    "- $D$ the corpus of documents\n",
    "- $K$ the number of topics\n",
    "- $V$ the vocabulary size\n",
    "- $N$ the number of documents\n",
    "\n",
    "We define $\\phi_k$ the distribution of words for the topic $k$:\n",
    "$$\\phi_k \\sim \\text{Dir}(\\beta)$$\n",
    "It's follows a dirichlet distribution with hyperparameter $\\beta$.\n",
    "\n",
    "We define $\\theta_d$ the distribution of topics for the document $d$:\n",
    "$$\\theta_d \\sim \\text{Dir}(\\alpha)$$\n",
    "It's follows a dirichlet distribution with hyperparameter $\\alpha$.\n",
    "\n",
    "For each word $w_i$ in document $d$:\n",
    "$$z_i \\sim \\text{Categorical}(\\theta_d)$$\n",
    "$$w_i \\sim \\text{Categorical}(\\phi_{z_i})$$\n",
    "\n",
    "Let's define the matrix $\\phi \\in \\mathbb{R}^{K*V}$, with $\\phi_{ij}$ the probability of word $j$ for the distribution of the topic $i$.  \n",
    "Let's define the matrix $\\theta \\in \\mathbb{R}^{N*K}$, with $\\theta_{ij}$ the probability of topic $j$ for the distribution of the document $i$.  \n",
    "\n",
    "The model gives us the following joint distribution:\n",
    "$$p(w, z, \\phi, \\theta | \\alpha, \\beta) = p(\\phi|\\beta) p(\\theta|\\beta) p(z|\\theta) p(w|\\phi_z)$$\n",
    "\n",
    "We are trying to learn the posterior distribution:\n",
    "$$p(z, \\phi, \\theta | w, \\alpha, \\beta) = \\frac{p(w, z, \\phi, \\theta | \\alpha, \\beta)}{p(w|\\alpha,\\beta)}$$\n",
    "Unfortunatately this problem is intractable, the normalization factor cannot be computed.  \n",
    "We are using Gibbs sampling to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define :\n",
    "- $TA$, the topics assignment list, where $TA_{ij}$ is the topic assigned to the word $j$ in document $i$.\n",
    "- $WT \\in \\mathbb{N}^{K*V}$, where $WT_{ij}$ is the number of times the word $j$ is assigned to the topic $i$.\n",
    "- $DT \\in \\mathbb{N}^{N*K}$, where $DT_{ij}$ is the number of words assigned to the topic $j$ in the document $i$.\n",
    "- $T_k = \\sum_{j=1}^V WT_{kj}$ : total number of words assigned to topic $k$\n",
    "- $W_k = \\sum_{j=1}^K DT_{kj}$ : total number of words in document k\n",
    "\n",
    "The goal of the algorithm is to determine $TA$, that is the $z_i$ for every words in every documents. Both matrices $WT$ and $DT$ are directly computed from $TA$.  \n",
    "\n",
    "From $TA$, we can obtain $\\phi$ and $\\theta$:\n",
    "\n",
    "$$\\phi_{ij} = \\frac{WT_{ij} + \\beta}{T_i + V\\beta}$$\n",
    "$$\\theta_{ij} = \\frac{DT_{ij} + \\alpha}{W_i + K\\alpha}$$\n",
    "\n",
    "We can sample from the conditional distribution of $z$:\n",
    "\n",
    "$$p(z_i=k|z^{-i},w_i) \\propto (DT^{-i}_{dk} + \\alpha) \\frac{WT^{-i}_{kw_i} + \\beta}{T_k + V\\beta}$$\n",
    "\n",
    "with $z_i$ topic of word $w_i$ in document $d$. $z^{-i}$ refers to the topic when leaving word $w_i$ out of all calculations.  \n",
    "\n",
    "The algorithm iterate and sample over all $p(z_i=k|z^{-i},w_i)$, updating each time the current value of $z_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 1, 3, 4]\n",
      "[5, 6, 7, 0, 8, 2, 4]\n",
      "[1, 9, 10, 2, 11, 4]\n",
      "[12, 10, 13, 14]\n",
      "[15, 16, 17, 10]\n",
      "[18, 2, 11]\n",
      "[18, 19, 20, 21, 17, 22, 23, 24, 18]\n",
      "[25, 18, 26]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"eat turkey on turkey day holiday\",\n",
    "    \"i like to eat cake on holiday\",\n",
    "    \"turkey trot race on thanksgiving holiday\",\n",
    "    \"snail race the turtle\",\n",
    "    \"time travel space race\",\n",
    "    \"movie on thanksgiving\",\n",
    "    \"movie at air and space museum is cool movie\",\n",
    "    \"aspiring movie star\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def prepr(docs):\n",
    "    docs = [x.split(' ') for x in docs]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_dic(docs):\n",
    "    docs = [list(dict.fromkeys(x)) for  x in docs]\n",
    "    dic = {}\n",
    "    for d in docs:\n",
    "        for w in d:\n",
    "            if w not in dic:\n",
    "                dic[w] = len(dic)\n",
    "\n",
    "    return dic\n",
    "\n",
    "def build_bow(docs, dic):\n",
    "    res = []\n",
    "    for d in docs:\n",
    "        res.append([dic[w] for w in d])\n",
    "    return res\n",
    "    \n",
    "\n",
    "docs = prepr(docs)\n",
    "dic = build_dic(docs)\n",
    "bow = build_bow(docs, dic)\n",
    "for l in bow: print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Words: 0.076923*\"turkey\"; 0.076923*\"on\"; 0.051282*\"time\"; 0.051282*\"thanksgiving\"; 0.051282*\"space\"; \n",
      "Topic 1\n",
      "Words: 0.081081*\"race\"; 0.081081*\"on\"; 0.081081*\"holiday\"; 0.054054*\"turtle\"; 0.054054*\"thanksgiving\"; \n",
      "Topic 2\n",
      "Words: 0.085106*\"movie\"; 0.063830*\"eat\"; 0.042553*\"turkey\"; 0.042553*\"trot\"; 0.042553*\"travel\"; \n"
     ]
    }
   ],
   "source": [
    "def LDA(bow, dic, K, alpha, beta, niters=10000):\n",
    "    N = len(bow)\n",
    "    V = len(dic)\n",
    "    tk = np.zeros(K, dtype=np.int)\n",
    "    wk = np.zeros(N, dtype=np.int)\n",
    "    WT = np.zeros((K, V), dtype=np.int)\n",
    "    DT = np.zeros((N, K), dtype=np.int)\n",
    "    \n",
    "    # Initialization: random assignment of topic to words of docs\n",
    "    TA = list()\n",
    "    for i in range(N):\n",
    "        wk[i] = len(bow[i])\n",
    "        TA.append(np.random.randint(0,K, size=wk[i]))\n",
    "        \n",
    "        for j in range(wk[i]):\n",
    "            word = bow[i][j]\n",
    "            topic = TA[i][j]\n",
    "            WT[topic,word] += 1\n",
    "            DT[i, topic] += 1\n",
    "            tk[topic] += 1\n",
    "\n",
    "    \n",
    "    for it in range(niters):\n",
    "        \n",
    "        for d in range(N):\n",
    "            for i in range(wk[d]):\n",
    "                \n",
    "                wi = bow[d][i]\n",
    "                \n",
    "                zi = TA[d][i]\n",
    "                DT[d,zi]-=1\n",
    "                WT[zi,wi]-=1\n",
    "                tk[zi]-=1\n",
    "                \n",
    "                zips = np.empty(K)\n",
    "                for k in range(K):\n",
    "                    zips[k] = ((WT[k,wi] + beta) / (tk[k] + V * beta)) * (\n",
    "                        (DT[d,k] + alpha) / 1# (wk[d] + K * alpha)\n",
    "                    )\n",
    "\n",
    "                zips = zips/np.sum(zips)\n",
    "                zi = np.random.choice(K, p=zips)\n",
    "                TA[d][i] = zi\n",
    "                DT[d,zi]+=1\n",
    "                WT[zi,wi]+=1\n",
    "                tk[zi]+=1\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute probas    \n",
    "    phi = (WT + beta) / (tk.reshape(K,1) + V * beta)\n",
    "    theta = (DT + alpha) / (wk.reshape(N,1) + K * alpha)\n",
    "    return phi, theta\n",
    "    \n",
    "   \n",
    "phi, theta = LDA(bow, dic, 3, 1, 1)\n",
    "\n",
    "for k in range(len(phi)):\n",
    "    msg = 'Topic {}\\nWords: '.format(k)\n",
    "    words = list(dic.keys())\n",
    "    props = phi[k].tolist()\n",
    "    \n",
    "    props, words = zip(*sorted(zip(props, words), reverse=True))\n",
    "    for i in range(5):\n",
    "        msg += '{:.6f}*\"{}\"; '.format(props[i], words[i])\n",
    "    print(msg)\n",
    "    \n",
    "#print(phi)\n",
    "#print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap is used to estimate the prediction error of the model.  \n",
    "It can also be used to improve the estimate itself, it's called bagging.  \n",
    "\n",
    "For the regression problem, where we fit the dataset $Z = {(x_1,y_1), (x_2,y_2), \\text{...}, (x_N,y_N)}$, we can fit $B$ same classifiers, but each on a different bootstrap sample of $Z$: $Z^{*b}, b=1,2,\\text{...},B$\n",
    "\n",
    "The bagging estimate is given by:\n",
    "$$\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B f^{*b}(x)$$\n",
    "\n",
    "The bagging estimate has a lower variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "err_train1: 21.871593283838422\n",
      "err_test1: 23.812245465080714\n",
      "err_train2: 21.884793608962777\n",
      "err_test2: 24.166391718828116\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BaggingRegressor:\n",
    "    \n",
    "    def __init__(self, clf, B):\n",
    "        self.clf= clf\n",
    "        self.B = B\n",
    "        \n",
    "        self.clfs = [deepcopy(self.clf) for _ in range(self.B)] \n",
    "    \n",
    "    def bootstrap_sample(self, X, y):\n",
    "        p = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X[p], y[p]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for clf in self.clfs:\n",
    "            bX, by = self.bootstrap_sample(X, y)\n",
    "            clf.fit(bX, by)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([clf.predict(X) for clf in self.clfs])\n",
    "        preds = np.average(preds, axis=0)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "X, y = load_boston().data, load_boston().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "    \n",
    "clf1 = LinearRegression()\n",
    "clf1.fit(X_train, y_train)\n",
    "print('err_train1:', np.mean((y_train - clf1.predict(X_train))**2))\n",
    "print('err_test1:', np.mean((y_test - clf1.predict(X_test))**2))\n",
    "\n",
    "mod = LinearRegression()\n",
    "clf2 = BaggingRegressor(mod, 100)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('err_train2:', np.mean((y_train - clf2.predict(X_train))**2))\n",
    "print('err_test2:', np.mean((y_test - clf2.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification problem, the model is defined by:\n",
    "\n",
    "$$\\hat{G}_\\text{bag}(x) = \\arg \\max_{k} \\hat{f}_\\text{bag}(x)$$\n",
    "\n",
    "If $\\hat{f}^{*b}(x)$ returns a one-hot vector, then the model acts as if it's voting between all bootstrap classifiers and returns the prediction with the most votes.  \n",
    "If $\\hat{f}^{*b}(x)$ returns a vector of probability for each class, then the model averages over all probabilities, and returns an estimate with lower variance than the voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "acc_train_1: 1.0\n",
      "acc_test_1: 0.9666666666666667\n",
      "acc_train_2: 1.0\n",
      "acc_test_2: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class BaggingPredictor:\n",
    "    \n",
    "    def __init__(self, clf, B):\n",
    "        self.clf= clf\n",
    "        self.B = B\n",
    "        \n",
    "        self.clfs = [deepcopy(self.clf) for _ in range(self.B)] \n",
    "    \n",
    "    def bootstrap_sample(self, X, y):\n",
    "        p = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X[p], y[p]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for clf in self.clfs:\n",
    "            bX, by = self.bootstrap_sample(X, y)\n",
    "            clf.fit(bX, by)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([clf.predict_proba(X) for clf in self.clfs])\n",
    "        preds = np.average(preds, axis=0)\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return preds\n",
    "\n",
    "\n",
    "X, y = load_iris().data, load_iris().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=15)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('acc_train_1:', np.mean(y_train == clf1.predict(X_train)))\n",
    "print('acc_test_1:', np.mean(y_test == clf1.predict(X_test)))\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=15)\n",
    "clf2 = BaggingPredictor(model, 100)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('acc_train_2:', np.mean(y_train == clf2.predict(X_train)))\n",
    "print('acc_test_2:', np.mean(y_test == clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Averaging and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian model averagin have a set of candiadates models $\\mathcal{M}_m, m=1,\\text{...},M$ for the training set $Z$.  \n",
    "These model may be:\n",
    "- identical models with same hyperparameters\n",
    "- different models\n",
    "- identical models trained on subsets of the data\n",
    "\n",
    "Suppose $C$ quantity of interest, for example a prediction $f(x)$. The posterior distribution of $C$ is:\n",
    "\n",
    "$$P(C|Z) = \\sum_{m=1}^M P(C|\\mathcal{M}_m, Z)P(\\mathcal{M}_m|Z)$$\n",
    "\n",
    "with posterior mean:\n",
    "\n",
    "$$E(C|Z) = \\sum_{m=1}^M E(C|\\mathcal{M}_m, Z)P(\\mathcal{M}_m|Z)$$\n",
    "\n",
    "The Bayesian prediction is a weighted average of model predictions.  \n",
    "Committe methods use an unweighted average.  \n",
    "\n",
    "The BIC criterion can be used to estimate the posterior P$(\\mathcal{M}_m|Z)$  \n",
    "\n",
    "Or we can also simply choose the weights that minimizes the mean squared error:\n",
    "\n",
    "$$\\hat{w} = \\arg \\min_{w} E_\\mathcal{P}[Y - \\sum_{m=1}^Mw_m \\hat{f}_m(x)]^2$$\n",
    "\n",
    "This can be fit using linear regression, but the population data is not available, so we use the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_train1: 21.871593283838422\n",
      "err_test1: 23.812245465080714\n",
      "err_train2: 22.55981207147202\n",
      "err_test2: 21.299777022650886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class ModelAvg:\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for m in self.models:\n",
    "            m.fit(X, y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([m.predict(X) for m in self.models])\n",
    "        preds = np.average(preds, axis=0)\n",
    "        return preds\n",
    "        \n",
    "\n",
    "X, y = load_boston().data, load_boston().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "    \n",
    "clf1 = LinearRegression()\n",
    "clf1.fit(X_train, y_train)\n",
    "print('err_train1:', np.mean((y_train - clf1.predict(X_train))**2))\n",
    "print('err_test1:', np.mean((y_test - clf1.predict(X_test))**2))\n",
    "\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(0.5),\n",
    "    Lasso(0.5),\n",
    "    SVR(kernel='linear')\n",
    "]\n",
    "clf2 = ModelAvg(models)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('err_train2:', np.mean((y_train - clf2.predict(X_train))**2))\n",
    "print('err_test2:', np.mean((y_test - clf2.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_train1: 21.871593283838422\n",
      "err_test1: 23.812245465080714\n",
      "[ 1.18274999 -0.11477123 -0.07159821]\n",
      "err_train2: 21.932206683793588\n",
      "err_test2: 24.173238244924555\n"
     ]
    }
   ],
   "source": [
    "class WeigthedModelAvg:\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for m in self.models:\n",
    "            m.fit(X, y)\n",
    "            \n",
    "        X2 = np.array([m.predict(X) for m in self.models]).T\n",
    "        self.beta = np.linalg.inv(X2.T @ X2) @ X2.T @ y\n",
    "        print(self.beta)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([m.predict(X) for m in self.models]).T\n",
    "        preds = preds @ self.beta\n",
    "        return preds\n",
    "        \n",
    "\n",
    "X, y = load_boston().data, load_boston().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "    \n",
    "clf1 = LinearRegression()\n",
    "clf1.fit(X_train, y_train)\n",
    "print('err_train1:', np.mean((y_train - clf1.predict(X_train))**2))\n",
    "print('err_test1:', np.mean((y_test - clf1.predict(X_test))**2))\n",
    "\n",
    "models = [\n",
    "    #LinearRegression(),\n",
    "    Ridge(0.5),\n",
    "    Lasso(0.5),\n",
    "    SVR(kernel='linear')\n",
    "]\n",
    "clf2 = WeigthedModelAvg(models)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('err_train2:', np.mean((y_train - clf2.predict(X_train))**2))\n",
    "print('err_test2:', np.mean((y_test - clf2.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of this approach is that it gives more weights to more complex models.  \n",
    "The problem can be solved using stacking:  \n",
    "Let $\\hat{f}^{-i}_m(x)$ the $m$-th model trained on the dataset without the $i$-th element.\n",
    "\n",
    "$$\\hat{w}^\\text{st} = \\arg \\min_{w} \\sum_{i=1}^N [ y_i - \\sum_{m=1}^Mw_m \\hat{f}^{-i}_m(x_i)]^2$$\n",
    "\n",
    "The stacking weights can be found similarly using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Search: Bumping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw bootstrap samples, and fit $B$ models $f^{*b}(x)$.  \n",
    "Instead of averaing over them, whe choose the one that minimizes a specific criterion on the training set. It could be for example the MSE:\n",
    "\n",
    "$$\\hat{b} = \\arg \\min_b \\sum_{i=1}^N [y_i - \\hat{f}^{*b}(x_i)]^2$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
