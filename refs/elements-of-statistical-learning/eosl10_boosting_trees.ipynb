{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost.M1\n",
    "\n",
    "The idea of boosting is to combine many weak classifiers intro a strong one.  \n",
    "A weak classifier is one slight better than random guessing.\n",
    "\n",
    "Let's define the error rate:\n",
    "\n",
    "$$\\bar{\\text{err}} = \\frac{1}{N} \\sum_{i=1}^N I(y_i \\neq G(x_i))$$\n",
    "Adaboost combine $M$ weak classifiers:\n",
    "$$G(x) = \\text{sign} \\left( \\sum_{m=1}^M \\alpha_m G_m(x) \\right) $$\n",
    "\n",
    "$\\alpha$ is the contribution vector of the classifiers, they are learned, such that a better model as an higher $\\alpha_m$.  \n",
    "\n",
    "All classifiers are trained one by one, but with weighted examples $w_i$. At first all examples have the same weight, then at each iteration the weight of misclassified examples increase, and the others decrease.  \n",
    "\n",
    "Algorithm $10.1$ page $339$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "train acc: 0.9102296450939458\n",
      "test acc: 0.8916666666666667\n",
      "train acc: 0.9617258176757133\n",
      "test acc: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from copy import deepcopy\n",
    "\n",
    "X, y = load_digits().data, load_digits().target\n",
    "y = (y < 5).astype(np.int32)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == logreg.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == logreg.predict(X_test)))\n",
    "\n",
    "\n",
    "class AdaboostM1:\n",
    "    \n",
    "    def __init__(self, model, M):\n",
    "        self.model = model\n",
    "        self.M = M\n",
    "        self.mods = []\n",
    "        self.alpha = np.empty(M)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N = len(X)\n",
    "        w = np.ones(N) / N\n",
    "        \n",
    "        \n",
    "        for m in range(self.M):\n",
    "            clf = deepcopy(self.model)\n",
    "            clf.fit(X, y, w)\n",
    "            self.mods.append(clf)\n",
    "            preds = clf.predict(X)\n",
    "            err = np.sum(w * (preds != y)) / np.sum(w)\n",
    "            self.alpha[m] = np.log((1 - err) / err)\n",
    "            w = w * np.exp(self.alpha[m] * (preds != y))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(len(X))\n",
    "        for m in range(self.M):\n",
    "            preds += self.alpha[m] * self.mods[m].predict(X)\n",
    "        preds = np.round(preds / np.sum(self.alpha)).astype(np.int32)\n",
    "        return preds\n",
    "            \n",
    "        \n",
    " \n",
    "mod = DecisionTreeClassifier(max_depth=1)\n",
    "clf = AdaboostM1(mod, 500)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train acc:', np.mean(y_train == clf.predict(X_train)))\n",
    "print('test acc:', np.mean(y_test == clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Fits an Additive Model\n",
    "\n",
    "Boosting is just a special case of additive models:\n",
    "    \n",
    "$$f(x) = \\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$$\n",
    "\n",
    "$b(x;\\gamma_m)$ are simple functions of argument $x$ and parameters $\\gamma_m$. For boosting, each basis function is a weak classifier.\n",
    "\n",
    "These models are trained by fitting $\\beta$ and $\\gamma$ minimizing a loss function over the dataset:\n",
    "\n",
    "$$\\min_{\\{\\beta_m, \\gamma_m\\}_1^M} \\sum_{i=1}^N L\\left(y_i, \\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\right)$$\n",
    "\n",
    "for any loss function $L(y, f(x))$ such as squared-error or negative log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Stagewise Additive Modeling\n",
    "\n",
    "This algorithm find an approximate solution by solving a simpler problem.   It starts with an empty model, and add a new basic function one at a time,  fitting it without modyfing the parameters of the previous ones.\n",
    "\n",
    "The problem is a lot simpler to optimize:\n",
    "\n",
    "$$\\min_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta_m b(x_i;\\gamma_m))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Loss and Adaboost\n",
    "\n",
    "AdaBoost.M1 is equivalent to forward stagewise additive modeling using the exponential loss:\n",
    "\n",
    "$$L(y, f(x)) = \\exp (-yf(x))$$\n",
    "\n",
    "The problem is:\n",
    "\n",
    "$$(\\beta_m, G_m) = \\min_{\\beta, G} \\sum_{i=1}^N \\exp [-y_i(f_{m-1}(x_i) + \\beta G(x_i))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Exponential Loss ?\n",
    "\n",
    "The principal attraction is computational: additive modeling with computational loss leads to a simple modular reweighting algorithm.\n",
    "\n",
    "$$f^*(x) = \\arg \\min_{f(x)} E_{y|x} \\exp(-yf(x)) = \\frac{1}{2} \\log \\frac{P(y=1|x)}{P(y=-1|x)}$$\n",
    "\n",
    "Thus AdaBoost estimates one-half of the log-odds, that justifies using the  sign operator.  \n",
    "\n",
    "Another loss is the deviance loss:\n",
    "\n",
    "$$l(y, f(x)) = \\log (1 + e^{-2yf(x)})$$\n",
    "\n",
    "At the population level, using either criterion leads to the same solution, but this is not true for finite datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Robustness\n",
    "\n",
    "## Robust Loss functions for classification\n",
    "\n",
    "deviance and exponential loss are both monotone decreasing functions of the margin yf(x).  \n",
    "With $G(x) = \\text{sign}(f(x))$, observations with positive margin are classified corretly, and those with negative magin are misclassified.  \n",
    "Any loss criterion should penalize negative margin more heavily than positive ones.  \n",
    "\n",
    "The difference between deviance and exponential loss is how much they penalize negative margins. The penalty for deviance increase linearly, where the one for exponential loss increase exponentially. In noisy settings, with misclassifications in the training data, the deviance gives better results.  \n",
    "\n",
    "Mean Squared error increases quadratically when $yf(x) > 1$, therefore increasing error for correctly classified examples with increasing certainty. Thus MSE is a terrible choice of loss function.  \n",
    "\n",
    "The problem generalize to K-class classification:\n",
    "$$G(x) = \\arg \\max_{k} p_k(x)$$\n",
    "with $p_k(x)$ the probability that $x$ belongs to class $k$:\n",
    "$$p_k(x) = \\frac{e^{f_k(x)}}{\\sum_{l=1}^K e^{f_l(x)}}$$\n",
    "\n",
    "We can use the K-class multinomial deviance loss function:\n",
    "\n",
    "$$L(y, p(x)) = \\sum_{k=1}^K I(y = k) \\log p_k(x)$$\n",
    "\n",
    "## Robust Loss functions for regression\n",
    "\n",
    "For regression, both the squared error: $L(y, f(x)) = (y - f(x))^2$ and absolute loss $L(y, f(x)) = |y - f(x)|$ leads to the same populations results, but vary for finite datasets.  \n",
    "Squared error loss places much empahish on obersation with large residuals, which if far less robust for outliers. Absolute loss performs much better in these situations.  \n",
    "\n",
    "Another solution to resist outliers is the Huber loss:\n",
    "$$\n",
    "L(y, f(x)) = \n",
    "\\begin{cases}\n",
    "    (y - f(x))^2 & \\text{if } |y - f(x)| \\leq \\delta \\\\\n",
    "    2 \\delta |y - f(x)| - \\delta^2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Trees\n",
    "\n",
    "A tree can be expressed as:\n",
    "\n",
    "$$T(x;\\theta) = \\sum_{j=1}^J \\gamma_j I(x \\in R_j)$$\n",
    "\n",
    "The parameters are found by minimizing the empirical risk:\n",
    "\n",
    "$$\\theta = \\arg \\min_\\theta \\sum_{j=1}^J \\sum_{x_i \\in R_j} L(y_i, \\gamma_j)$$  \n",
    "\n",
    "The boosted tree model is a sum of such trees:\n",
    "$$f_M(x) = \\sum_{m=1}^M T(x;\\theta_m)$$\n",
    "\n",
    "With a forward stagewise procedure, one must solve at each step:\n",
    "$$\\hat{\\theta_m} = \\arg \\min_{\\theta_m} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i, \\theta_m))$$\n",
    "For MSE, we simply need to fit a new regression tree with the residual errors.  \n",
    "\n",
    "For binary classificaton with exponential loss, we get the following criterion for each tree:\n",
    "$$\\hat{\\theta_m} = \\arg \\min_{\\theta} \\sum_{i=1}^N w_i^{(m)} \\exp (-y_i T(x_i;\\theta_m))$$\n",
    "This criterion can be implemented by updating the criterion of splitting for the classical tree growing algorithms.  \n",
    "\n",
    "Using other loss such as the absolute error, the Huber Loss, or the deviance gives most robust models, but there is no simple algorithms to optimize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization via Gradient Boosting\n",
    "\n",
    "Let's define the loss as:\n",
    "$$L(f) = \\sum_{i=1}^N L(y_i, f(x_i))$$\n",
    "\n",
    "The goal is to minimize $L(f)$ with respect fo $f$, with $f$ a sum of trees.  \n",
    "\n",
    "Let's say the parameters of $f$ are the values of $f$ at each point in the training set:\n",
    "$$f = \\{ f(x_1), f(x_2), \\text{...}, f(x_N) \\}^T$$\n",
    "\n",
    "Numerical optimisation solve $f$ using a sum of components vectors, or sum of steps:\n",
    "$$f_M = \\sum_{m=0}^M h_m, \\space h_m \\in \\mathbb{R}^N$$  \n",
    "\n",
    "Using steepest descent, we define $h_m = -\\rho_m g_m$ with $\\rho_m \\in \\mathbb{R}$ the step size, and $g_m \\in \\mathbb{R}^N$ the gradient of $L$.  \n",
    "\n",
    "$$g_{im} = \\frac{\\partial L(y_i, f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}$$\n",
    "$$\\rho_m = \\arg \\min_{\\rho} L(f_{m-1} 0 \\rho g_m)$$\n",
    "The current solution is updated:\n",
    "$$f_m = f_{m-1} - \\rho_m g_m$$  \n",
    "\n",
    "The process is repeated $M$ times, this is a greedy strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "This process is great to minize loss on the training data, but our goal is generalization.  \n",
    "A solution is to build a tree $T(x;\\theta_m)$ at each iteration, as close as possible to the negative gradient. Using MSE, we get the criterion:\n",
    "$$\\hat{\\theta} = \\arg \\min_\\theta \\sum_{i=1}^N (-g_{im} - T(x_i;\\theta))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression:\n",
    "\n",
    "1. Initialize:\n",
    "\n",
    "$$f_0(x) = \\arg \\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma)$$\n",
    "\n",
    "2. For $m=1$ to $M$:\n",
    "\n",
    "    $$r_{im} = - \\frac{\\partial L(y_i, f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}$$\n",
    "    \n",
    "    Fit a regression tree to targets $r_{im}$, and update $f_m(x)$:\n",
    "    \n",
    "    $$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m} \\gamma_{jm} I(x \\in R_{jm})$$\n",
    "    \n",
    "3. Output $\\hat{f}(x) = f_M(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other losses, we plug in different loss functions $L$.  \n",
    "For K-classes classification, we need to build $K$ trees at eachh iteration.  \n",
    "Two hyperparemeters are the number of iterations $M$, and the size of each tree $J_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right-Sized Trees for Boosting\n",
    "\n",
    "Each time a new tree is built using the usual procedure, starting by building a very large tree, then pruning it. This procedure suppose the tree built is the last one, which is a poor assumption for non-final trees. It results in tree way too large in each iteration.  \n",
    "One solution is to restrict all tress to the size $J$, an hyperparameter to be fixed.  \n",
    "\n",
    "The interation level is limited by $J$. With $J=2$, only main effects are possible, $J=3$ allow only two-variable interactions, and so on.  \n",
    "The interaction level is unknow, but low in general. By experience $4 \\leq J \\leq 8$ works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Another hyperpameter to be fixed is $M$. Each iteration recudes the risk on the training set, but may lead to overfitting. We can find the optimital $M^*$ by monitoring the risk on a validation set.  \n",
    "\n",
    "Another regularization technique is Shrinkage, that scale the contribution of each tree by a factor $v \\in [0,1]$:\n",
    "$$f_m(x) = f_{m-1}(x) + v \\sum_{j=1}^{J_m} \\gamma_{jm} I(x \\in R_{jm})$$  \n",
    "Smaller values of $v$ cause more shrinkage. In practice, set $v$ very small $(< 0.1)$ and chose $M$ as above works well.  \n",
    "\n",
    "We can also use subsampling, similar to bagging. At each iteration, we sample a fraction of the training dataset and perform the iteration on this sample. It usually produces a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative importance of Predictor Variables\n",
    "\n",
    "For a single tree $T$, a measure of releveance of feature $X_l$ is:\n",
    "$$\\mathcal{I}_l^2(T) = \\sum_{t=1}^{J-1} \\hat{i}^2 I(v(t) = l)$$ \n",
    "\n",
    "With $i_t^2$ the improvement in squared error fit over that for a constant fit to the world region, where the split variable is $l$.  \n",
    "For tree boosting, we simply average over all the trees:\n",
    "$$\\mathcal{I}_l^2 = \\frac{1}{M} \\sum_{i=1}^M \\mathcal{I}_l^2(T_m)$$  \n",
    "All values are relative, we can set the higher to $100$, and scale all other accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots\n",
    "\n",
    "We can plot the dependance of a subset of variables $X_S$, by a marginal average over the other variables $X_C$:\n",
    "$$f_S(X_S) = E_{X_C} f(X_s,X_C)$$\n",
    "We can thuse realise several partial dependace plots, using several sets $S$.  \n",
    "They can be estimated by:\n",
    "$$\\bar{f_S(X_S)} = \\frac{1}{N} \\sum_{i=1}^N f(X_S, x_{iC})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting (Regression)\n",
    "\n",
    "Least Absolute Error Tree Bosting Regression algorithm:\n",
    "\n",
    "1. set $F_0(x) = \\text{median} \\{ y_i \\}_1^N$\n",
    "\n",
    "2. For $m=1$ to $M$:\n",
    "    $$\\hat{y}_i = \\text{sign}(y_i - F_{m-1}(x_i))$$\n",
    "    $$\\{ R_{jm} \\}_1^J \\text{ tree with J terminal nodes trained on } \\{\\hat{y}_i, x_i \\}_1^N$$\n",
    "    $$\\gamma_{jm} = \\text{median} \\{ y_i - F_{m-1}(x_i) : x_i \\in R_{jm} \\}$$\n",
    "    $$F_m(x) = F_{m-1}(x) + \\sum_{j=1}^J \\gamma_{jm} 1(x \\in R_{jm})$$\n",
    "    \n",
    "3. Output $\\hat{F}(x) = F_M(x)$  \n",
    "\n",
    "An alternate approach is to build tree $T(x)$ minizing LAE loss:\n",
    "$$T_m(x) = \\arg \\min_{T} \\sum_{i=1}^N |y_i - F_{m-1}(x_i) - T(x)|$$\n",
    "$$F_m(x) = F_{m-1}(x) = T_m(x)$$  \n",
    "\n",
    "The first solution is much faster because it uses the sqared error loss to build the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Paper\n",
    "\n",
    "[Greedy Function Approximation: A Gradient boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error: 23.811758966794812\n",
      "test_error: 33.320464682729074\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "class DTNode:\n",
    "    \n",
    "    def __init__(self, X, y, val):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.val = val\n",
    "        self.cut = None\n",
    "        self.subs = None\n",
    "        \n",
    "    def pred(self, x):\n",
    "        if self.cut is None:\n",
    "            return self.val\n",
    "        elif x[self.cut[0]] <= self.cut[1]:\n",
    "            return self.subs[0].pred(x)\n",
    "        else:\n",
    "            return self.subs[1].pred(x)\n",
    "        \n",
    "    def split(self, j, s, eval_fn):\n",
    "        if self.cut is not None:\n",
    "            raise Exception('already cut')\n",
    "        \n",
    "        leftp = self.X[:,j] <= s\n",
    "        rightp = self.X[:,j] > s\n",
    "        \n",
    "        X_left, y_left = self.X[leftp], self.y[leftp]\n",
    "        X_right, y_right = self.X[rightp], self.y[rightp]\n",
    "        left = DTNode(X_left, y_left, eval_fn(X_left, y_left))\n",
    "        right = DTNode(X_right, y_right, eval_fn(X_right, y_right))\n",
    "        self.cut = (j, s)\n",
    "        self.subs = (left, right)\n",
    "        \n",
    "    def update_vals(self, X, y, eval_fn):\n",
    "        if self.cut is None:\n",
    "            self.val = eval_fn(X, y)\n",
    "            return\n",
    "        \n",
    "        p1 = X[:,self.cut[0]] <= self.cut[1]\n",
    "        p2 = X[:,self.cut[0]] > self.cut[1]\n",
    "        self.subs[0].update_vals(X[p1], y[p1], eval_fn)\n",
    "        self.subs[1].update_vals(X[p2], y[p2], eval_fn)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def get_best_cut(node, j, val_fn, err_fn, min_leaf_size):\n",
    "    X = node.X\n",
    "    y = node.y\n",
    "    best_s = None\n",
    "    best_err = float('inf')\n",
    "    \n",
    "    \n",
    "    for i in range(len(X) - 1):\n",
    "        \n",
    "        s = (X[i,j] + X[i+1,j])/2\n",
    "        X_left = X[X[:,j] <= s]\n",
    "        X_right = X[X[:,j] > s]\n",
    "        y_left = y[X[:,j] <= s]\n",
    "        y_right = y[X[:,j] > s]\n",
    "        if len(y_left) < min_leaf_size or len(y_right) < min_leaf_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        preds_left = np.ones(len(y_left)) * val_fn(X_left, y_left)\n",
    "        preds_right = np.ones(len(y_right)) * val_fn(X_right, y_right)\n",
    "        err = err_fn(y_left, preds_left) + err_fn(y_right, preds_right)\n",
    "        \n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_s = s\n",
    "        \n",
    "    return best_s, best_err\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "def split_tree(node, val_fn, err_fn, max_size, size = None):\n",
    "    if size is None:\n",
    "        size = [1]\n",
    "    if size[0] >= max_size:\n",
    "        return\n",
    "    \n",
    "    best_j = None\n",
    "    best_s = None\n",
    "    best_err = float('inf')\n",
    "    \n",
    "    for j in range(node.X.shape[1]):\n",
    "            \n",
    "        s, err = get_best_cut(node, j, val_fn, err_fn, min_leaf_size=3)\n",
    "        if err < best_err:\n",
    "            best_s = s\n",
    "            best_j = j\n",
    "            best_err = err\n",
    "    \n",
    "    if best_j is None:\n",
    "        return\n",
    "    \n",
    "    node.split(best_j, best_s, val_fn)\n",
    "    size[0] += 1\n",
    "    split_tree(node.subs[0], val_fn, err_fn, max_size, size)\n",
    "    split_tree(node.subs[1], val_fn, err_fn, max_size, size)\n",
    "    \n",
    "    \n",
    "    \n",
    "def build_tree(X, y, val_fn, err_fn, max_size):\n",
    "    root = DTNode(X, y, val_fn(X, y))\n",
    "    split_tree(root, val_fn, err_fn, max_size)\n",
    "    return root\n",
    "\n",
    "def val_avg(X, y):\n",
    "    return np.mean(y)\n",
    "\n",
    "def err_mse(y, preds):\n",
    "    return np.sum((y - preds)**2)\n",
    "    \n",
    "    \n",
    "class TreeRegressor:\n",
    "    \n",
    "    def __init__(self, max_size=4):\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.root = build_tree(X, y, val_avg, err_mse,\n",
    "                              self.max_size)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.root.pred(X[i])\n",
    "        return y\n",
    "    \n",
    "    \n",
    "        \n",
    "X, y = load_boston().data, load_boston().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "clf = TreeRegressor(max_size=4)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train_error:', np.mean((y_train - clf.predict(X_train))**2))\n",
    "print('test_error:', np.mean((y_test - clf.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error: 16.5096349009901\n",
      "test_error: 18.787406556372538\n"
     ]
    }
   ],
   "source": [
    "class ConstModel:\n",
    "    def __init__(self, val):\n",
    "        self.root = DTNode(None, None, val)\n",
    "\n",
    "def update_median(_, resi):\n",
    "    return np.median(resi)\n",
    "        \n",
    "class LADTreeBost:\n",
    "    \n",
    "    def __init__(self, J, M):\n",
    "        self.J = J\n",
    "        self.M = M\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.mods = []\n",
    "        self.mods.append(ConstModel(np.median(y)))\n",
    "        resid = y - np.median(y)\n",
    "        \n",
    "        for m in range(self.M):\n",
    "            yhat = np.sign(resid)\n",
    "            tree = TreeRegressor(max_size=self.J)\n",
    "            tree.fit(X,yhat)\n",
    "            tree.root.update_vals(X, resid, update_median)\n",
    "            resid -= tree.predict(X) \n",
    "            self.mods.append(tree)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.get_pred(X[i])\n",
    "        return y\n",
    "    \n",
    "    def get_pred(self, x):\n",
    "        y = 0\n",
    "        for m in self.mods:\n",
    "            y += m.root.pred(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "X, y = load_boston().data, load_boston().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=15)\n",
    "\n",
    "\n",
    "clf = LADTreeBost(J=4, M=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train_error:', np.mean((y_train - clf.predict(X_train))**2))\n",
    "print('test_error:', np.mean((y_test - clf.predict(X_test))**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
