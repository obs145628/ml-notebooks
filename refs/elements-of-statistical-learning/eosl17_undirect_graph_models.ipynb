{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../pyutils')\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In an undirected graph model, each vertex represent a random variable, and the absence of edge between two vertices means that the 2 randoms variables are conditionaly independent, given the other. The graph gives a visual way of understanding the joint distribution of the entire set of random variables.  \n",
    "This graph is also called a Markov Random Field.  \n",
    "Sparge graphs have a small number of edges, and are convenient for interpretation.  \n",
    "\n",
    "Each edge is parametrized by it's value (or potiential) that encode the strength of the conditional dependence between the random variables.  \n",
    "Challenges of grahical models are:\n",
    "- model selection (graph structure)\n",
    "- estimation of the edges parameters from data (learning)\n",
    "- compute marginal random variables propabilities and expectations. (inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Graphs and Their Properties\n",
    "\n",
    "Let a graph $\\mathcal{G}$ a pair $(V,E)$ with $V$ a set of vertices and $E$ a set of edges.  \n",
    "Two vertices $X$ and $Y$ are adjacent is there is an edge between them: $X \\sim Y$.  \n",
    "A path $X_1,\\text{...},X_n$ is a set of joined vertices; $X_{i-1} \\sim X_i$ for $i=2,\\text{...},n$.  \n",
    "A complete graph is a graph with every pair of vertices joined by an edge.  \n",
    "A subgraph $U \\in V$ is a subject of vertices with their edges.  \n",
    "\n",
    "In a markov graph, the absence of an edge implies that the random variables are conditionally independant given the other variables:\n",
    "$$\\text{No edge joining $X$ and $Y$} \\iff X \\perp Y | \\text{rest}$$\n",
    "\n",
    "$A,B,C$ subgraphs. $C$ separate $A$ and $B$ if every path between $A$ and $B$ intersects $C$.\n",
    "$$\\text{$C$ separates $A$ and $B$} \\implies A \\perp B | C$$\n",
    "\n",
    "A clique is a complete subgraph. A clique is said maximal is no other vertices can be added to it and still yield a clique.  \n",
    "\n",
    "A probability density function $f$ over $\\mathcal{G}$ can be represented as:\n",
    "$$f(x) = \\frac{1}{Z} \\sum_{C \\in \\mathcal{C}} \\psi_C(x_C)$$\n",
    "with $\\mathcal{C}$ set of maximal cliques, and $\\psi_C$ clique potentials. These are not really density functions, but capture dependence in $X_C$ by scoring certains $x_C$ higher than others.  \n",
    "$Z$ is the normilazing constant, also called the partition function:\n",
    "$$Z = \\sum_{x \\in \\mathcal{X}} \\prod_{C \\in \\mathcal{C}} \\Psi_C (x_C)$$\n",
    "\n",
    "This chapter focus on pairwise Markov Graph. There is a potential function for each edge, and at-most second-order interactions are represented. Thewyneed fewer parameters, and are easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undirected Graphical Models for Continuous Variables\n",
    "\n",
    "We assume the observations have a multivariate Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$. Since Gaussian distribution represents at most second-order relationships, it encodes a pairwise Markov Graph.  \n",
    "\n",
    "Th Gaussian distribution has the property that all conditionals distributions are also Gaussian.  \n",
    "Let $\\Theta = \\Sigma^{-1}$. If $\\Theta_{ij}=0$, then variables $i$ and $j$ are conditionally independant given the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the Parameters when the Graph Structure is Known\n",
    "\n",
    "Given some observations of $X$, let estimate the parameters of the joint distribution ($\\mu$ and $\\Sigma$). We suppose that the graph is complete.  \n",
    "Let's define the empirical covariance matrix $S$:\n",
    "$$S = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x}) (x_i - \\bar{x})^T$$\n",
    "with $\\bar{x}$ the sample mean vector.  \n",
    "\n",
    "The log-likelihood of the data can be written as:\n",
    "$$l(\\Theta) = \\log \\det \\theta - \\text{trace}(S\\theta)$$\n",
    "The maximum likelihood estimate of $\\Sigma$ is $S$.  \n",
    "\n",
    "Now is some edges are missing, we are trying to maximize $l(\\Theta)$ under the constraint that somes entries of $\\Theta$ are 0.  \n",
    "We add Lagrange constants for all missing edges:\n",
    "$$l_C(\\Theta) = \\log \\det \\theta - \\text{trace}(S\\theta) - \\sum_{(j,k) \\notin E} \\gamma_{jk}\\theta_{jk}$$\n",
    "This can be maximized with the followigng equation:\n",
    "$$\\Theta^{-1} - S - \\Gamma = 0$$\n",
    "\n",
    "with $\\Gamma$ matrix of Lagrange parameters.  \n",
    "\n",
    "We can patition the matrices into 2 parts: part 1 the first $p-1$ rows and columns, and part 2 the $pth$ row and column.  \n",
    "The equation can be rewrtieen as:\n",
    "$$W_{11}\\beta - s_{12} - \\gamma_{12} = 0$$\n",
    "We can remove all non-zeros elements from $\\gamma_12$, corresponding to edges constrained to be $0$, because they carry no information. We can also reduce the same way $\\beta$ and $W_11$, giving us the new equation:\n",
    "$$W^*_{11}\\beta^* - s^*_{12} = 0$$\n",
    "with solution:\n",
    "$$\\hat{\\beta}^* = W_{11}^{*-1} s^*_{12}$$\n",
    "the solution is padded with zeros to give $\\hat{\\beta}$\n",
    "\n",
    "Algorithm $17.1$ page 634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10\n",
      "[[10.    1.    1.31  4.  ]\n",
      " [ 1.   10.    2.    0.87]\n",
      " [ 1.31  2.   10.    3.  ]\n",
      " [ 4.    0.87  3.   10.  ]]\n",
      "[[ 0.12 -0.01 -0.   -0.05]\n",
      " [-0.01  0.1  -0.02 -0.  ]\n",
      " [ 0.   -0.02  0.11 -0.03]\n",
      " [-0.05  0.   -0.03  0.13]]\n"
     ]
    }
   ],
   "source": [
    "def part_mats(W, S, j):\n",
    "    W_11 = np.delete(W, j, axis=0)\n",
    "    W_11 = np.delete(W_11, j, axis=1)\n",
    "    s_12 = np.delete(S, j, axis=0)[:,j]\n",
    "    return W_11, s_12\n",
    "\n",
    "def regroup_mat(W, w_12, j):\n",
    "    w_12 = np.insert(w_12, j, W[j,j])\n",
    "    W[:,j] = w_12\n",
    "    return W\n",
    "    \n",
    "\n",
    "def edit_mats(G, j, W_11, s_12):\n",
    "    N = len(G)\n",
    "    suppr = [i for i in range(N) if G[i,j] == 0]\n",
    "    suppr = [x if x < j else x - 1 for x in suppr]\n",
    "    s_12 = np.delete(s_12, suppr)\n",
    "    W_11 = np.delete(W_11, suppr, axis=0)\n",
    "    W_11 = np.delete(W_11, suppr, axis=1)\n",
    "    return W_11, s_12\n",
    "    \n",
    "def extend_beta(G, j, betar):\n",
    "    N = len(G)\n",
    "    suppr = [i for i in range(N) if G[i,j] == 0]\n",
    "    suppr = [x if x < j else x - 1 for x in suppr]\n",
    "    beta = np.insert(betar, suppr, 0)\n",
    "    return beta\n",
    "        \n",
    "def estim_gauss(S, G, max_iters = 100, tol=1e-16):\n",
    "    \n",
    "    N = len(S)\n",
    "    W = S.copy()\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        \n",
    "        W_old = W.copy() \n",
    "        \n",
    "        for j in range(N):\n",
    "            W_11, s_12 = part_mats(W, S, j)    \n",
    "            #print(W_11)\n",
    "            #print(s_12)\n",
    "\n",
    "            W_11r, s_12r = edit_mats(G, j, W_11, s_12)\n",
    "\n",
    "            #print(W_11r)\n",
    "            #print(s_12r)\n",
    "\n",
    "\n",
    "            betar = np.linalg.inv(W_11r) @ s_12r\n",
    "            beta = extend_beta(G, j, betar)\n",
    "            #print(betar)\n",
    "            #print(beta)\n",
    "\n",
    "            w_12 = W_11 @ beta\n",
    "            #print(w_12)\n",
    "\n",
    "            W = regroup_mat(W, w_12, j)\n",
    "            #W[:N-1, -1] = w_12\n",
    "\n",
    "            #print(W)\n",
    "    \n",
    "        if np.linalg.norm(W - W_old) < tol:\n",
    "            break\n",
    "    \n",
    "    print('Iterations:', it)\n",
    "    return W\n",
    "\n",
    "S = np.array([\n",
    "    [10, 1, 5, 4],\n",
    "    [1., 10, 2, 6],\n",
    "    [5, 2, 10, 3],\n",
    "    [4, 6, 3, 10]\n",
    "])\n",
    "\n",
    "G = np.array([\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [1, 0, 1, 1]\n",
    "])\n",
    "\n",
    "W = estim_gauss(S, G)\n",
    "print(np.around(W,2))\n",
    "print(np.around(np.linalg.inv(W),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the Graph Structure\n",
    "\n",
    "Sparse inverse covariance estimation with the graphical lasso - Friedman, J., Hastie, T. and Tibshirani, R. (2008) -[PDF](file:///home/aiw/docs/eosl-refs.pdf)\n",
    "\n",
    "We can use the lasso regularization to estimate $\\Sigma$ in a way that tries to insert zeroes in $\\Theta$, giving us the graph strcture.  \n",
    "Let's maximizing the penalized log-likelihood:\n",
    "$$\\log \\det \\Theta - \\text{trace}(S\\Theta) - \\lambda ||\\Theta||_1$$\n",
    "\n",
    "The gradient equation is:\n",
    "$$\\Theta^{-1} - S - \\lambda \\text{sign}(\\Theta) = 0$$\n",
    "\n",
    "Similary to the algorithm above, we reach the equation:\n",
    "$$W_{11}\\beta - s_{12} + \\lambda \\text{sign}(\\beta) = 0$$\n",
    "\n",
    "This problem is similar to linear regression with lasso, and can be solved using the pathwise coordinate descent method.  \n",
    "Let $V = W_{11}$, the update has the form:\n",
    "$$\\hat{\\beta}_j \\leftarrow \\frac{1}{V_jj} S(s_{12j} - \\sum_{k \\neq j} V_{kj} \\hat{\\beta}_k, \\lambda)$$\n",
    "with $S$ the soft-threshold operator:\n",
    "$$S(x,t) = \\text{sign}(x)(|x| - t)_+$$\n",
    "\n",
    "Algorithm $17.2$ page $636$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undirected Graphical Models for Discrete Variables\n",
    "\n",
    "Pairwise Markov Networks with binary variables are veary common. They are called Ising models, or Botlzmann machines.  \n",
    "Each vertices are referred to as nodes or units. The values at each node can either be obversed (visible) or unobserved (hidden).  \n",
    "\n",
    "We consider first the case where all $p$ nodes are visible with edge pairs $(j,k) \\in E$. Their joint probability is given by:\n",
    "$$p(X,\\Theta) = \\exp \\left[ \\sum_{(j,k) \\in E} \\theta_{jk} X_j X_k - \\Phi(\\Theta) \\right]$$\n",
    "\n",
    "with $\\Phi(\\Theta)$ the log of the partition function:\n",
    "$$\\Phi(\\Theta) = \\log \\sum_{x \\in \\mathcal{X}} \\left[ \\exp \\sum_{(j,k) \\in E} \\theta_{jk} x_j x_k \\right]$$\n",
    "\n",
    "The model requires a constant node $X_0=1$.  \n",
    "The Ising model implies a logistic form for each node conditional on the others:\n",
    "$$P(X_j=1| X_{-j} = x_{-j}) = \\left( 1 + \\exp ( -\\theta_{j0} - \\sum_{(j,k) \\in E} \\theta_{jk} x_k) \\right) ^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimations of the Parameters when the Graph Structure is known\n",
    "\n",
    "Given N observations $x_i$, we can estimate the parameters by maximizing the log-likelihood:\n",
    "$$l(\\Theta) = \\sum_{i=1}^N \\log P_\\Theta(X = x_i)$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\Theta) & = \\sum_{i=1}^N \\log P_\\Theta(X = x_i) \\\\\n",
    "& = \\sum_{i=1}^N \\left( \\sum_{(j,k) \\in E} \\theta_{jk} x_{ij} x_{ik} - \\Phi(\\Theta) \\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Setting the gradient to $0$ gives:\n",
    "$$\\hat{E}(X_j,X_k) - E_\\Theta(X_j,X_k) = 0$$\n",
    "with $\\hat{E}(X_j,X_k)$ the expecation taken with respect to the empirical distribution of the data:\n",
    "$$\\hat{E}(X_j, X_k) = \\frac{1}{N} \\sum_{i=1}^N x_{ij}x_{ik}$$\n",
    "\n",
    "We can find the maximum likelihood estimates using gradient search or Netwon methods, but computing $E_\\Theta(X_j,X_k)$ involves the enumeration of $p(X,\\Theta)$ over the $|\\mathcal{X}|=2^p$ values of $X$, and it not feasible for large $p$ (over 30).\n",
    "\n",
    "When $p$ is large, the gradient is approximated using other methods, like mean field approximation or Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Nodes\n",
    "\n",
    "Let's suppose we have a subset of visible variables $X_V$, and the remaining are the hidden $X_H$. The log-likelihood become:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\Theta) & = \\sum_{i=1}^N \\log P_\\Theta(X_V = x_{iV}) \\\\\n",
    "& = \\sum_{i=1}^N \\left( \\log \\sum_{x_h \\in \\mathcal{X}_H} \\exp \\sum_{(j,k) \\in E} \\theta_{jk} x_{ij} x_{ik} - \\Phi(\\Theta) \\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The gradient becames:\n",
    "$$\\frac{d l(\\Theta)}{d\\theta_{jk}} = \\hat{E}_V E_\\Theta(X_k,X_K|X_V) - E\\Theta(X_j,X_k)$$\n",
    "\n",
    "It can be computed using Gibbs sampling, but the method can be very slow, even for moderate-sized models. We can add more restrictions to make those computations manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the Graph Structure\n",
    "\n",
    "As for continuous random variables, we can use lasso to remove edges.  \n",
    "\n",
    "One solution is to use a penalized log-likelihood, but the gradient computation for dense graphs is not manageable.  \n",
    "\n",
    "Another solution fit an $L_1$ penalized logistic regression model to each node as a functon of the other nodes, and symmetrize the edge parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines\n",
    "\n",
    "An RBM consts of one layer of visible units, and one layer of hidden units , with no connections within a layer.  \n",
    "The restricted form simplifies the Gibbs Sampling to compute the gradient of the log-likelihhod.  \n",
    "Using the contrastive-divergence algorithm, they can be trained rapidly.  \n",
    "\n",
    "RBM can learn to extract interesting features from data.  \n",
    "We can stack several RBM togethers and train the whole joint density model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
